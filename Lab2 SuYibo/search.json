[
  {
    "objectID": "practice.html",
    "href": "practice.html",
    "title": "Practice2-analysis_dataset",
    "section": "",
    "text": "Practice2-analysis_dataset\n\n\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pathlib import Path\nimport pingouin as pg\nfrom lets_plot import *\n\nLetsPlot.setup_html(no_js=True)\n\n\n# Create a dictionary with the data in\ndata = {\n    \"Copenhagen\": [14.1, 14.1, 13.7, 12.9, 12.3, 11.7, 10.8, 10.6, 9.8, 5.3],\n    \"Dniprop\": [11.0, 12.6, 12.1, 11.2, 11.3, 10.5, 9.5, 10.3, 9.0, 8.7],\n    \"Minsk\": [12.8, 12.3, 12.6, 12.3, 11.8, 9.9, 9.9, 8.4, 8.3, 6.9],\n}\n\ndf = pd.DataFrame.from_dict(data)\ndf.head()\n\n\n\n\n\n\n\n\nCopenhagen\nDniprop\nMinsk\n\n\n\n\n0\n14.1\n11.0\n12.8\n\n\n1\n14.1\n12.6\n12.3\n\n\n2\n13.7\n12.1\n12.6\n\n\n3\n12.9\n11.2\n12.3\n\n\n4\n12.3\n11.3\n11.8\n\n\n\n\n\n\n\n\n# Plot the data\nfig, ax = plt.subplots()\ndf.plot(ax=ax)\nax.set_title(\"Average contribution to public goods game: without punishment\")\nax.set_ylabel(\"Average contribution\")\nax.set_xlabel(\"Round\")\n\nText(0.5, 0, 'Round')\n\n\n\n\n\n\n\n\n\n\ndata_np = pd.read_excel(\n    \"D:\\360MoveData\\Users\\75903\\Desktop\\SuYibo-Homework\\Practice2 SuYibo/Public-goods-experimental-data.xlsx\",\n    usecols=\"A:Q\",\n    header=1,\n    index_col=\"Period\",\n)\ndata_n = data_np.iloc[:10, :].copy()\ndata_p = data_np.iloc[14:24, :].copy()\n\n\ntest_data = {\n    \"City A\": [14.1, 14.1, 13.7],\n    \"City B\": [11.0, 12.6, 12.1],\n}\n\n# Original dataframe\ntest_df = pd.DataFrame.from_dict(test_data)\n# A copy of the dataframe\ntest_copy = test_df.copy()\n# A pointer to the dataframe\ntest_pointer = test_df\n\n\ntest_pointer.iloc[1, 1] = 99\n\n\nprint(\"test_df=\")\nprint(f\"{test_df}\\n\")\nprint(\"test_copy=\")\nprint(f\"{test_copy}\\n\")\n\ntest_df=\n   City A  City B\n0    14.1    11.0\n1    14.1    99.0\n2    13.7    12.1\n\ntest_copy=\n   City A  City B\n0    14.1    11.0\n1    14.1    12.6\n2    13.7    12.1\n\n\n\n\ndata_n.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 10 entries, 1 to 10\nData columns (total 16 columns):\n #   Column           Non-Null Count  Dtype \n---  ------           --------------  ----- \n 0   Copenhagen       10 non-null     object\n 1   Dnipropetrovs’k  10 non-null     object\n 2   Minsk            10 non-null     object\n 3   St. Gallen       10 non-null     object\n 4   Muscat           10 non-null     object\n 5   Samara           10 non-null     object\n 6   Zurich           10 non-null     object\n 7   Boston           10 non-null     object\n 8   Bonn             10 non-null     object\n 9   Chengdu          10 non-null     object\n 10  Seoul            10 non-null     object\n 11  Riyadh           10 non-null     object\n 12  Nottingham       10 non-null     object\n 13  Athens           10 non-null     object\n 14  Istanbul         10 non-null     object\n 15  Melbourne        10 non-null     object\ndtypes: object(16)\nmemory usage: 1.3+ KB\n\n\n\ndata_n = data_n.astype(\"double\")\ndata_p = data_p.astype(\"double\")\n\n\nmean_n_c = data_n.mean(axis=1)\nmean_p_c = data_p.agg(np.mean, axis=1)\n\n\nfig, ax = plt.subplots()\nmean_n_c.plot(ax=ax, label=\"Without punishment\")\nmean_p_c.plot(ax=ax, label=\"With punishment\")\nax.set_title(\"Average contribution to public goods game\")\nax.set_ylabel(\"Average contribution\")\nax.legend()\n\n\n\n\n\n\n\n\n\npartial_names_list = [\"F. Kennedy\", \"Lennon\", \"Maynard Keynes\", \"Wayne\"]\n[\"John \" + name for name in partial_names_list]\n\n['John F. Kennedy', 'John Lennon', 'John Maynard Keynes', 'John Wayne']\n\n\n\n# Create new dataframe with bars in\ncompare_grps = pd.DataFrame(\n    [mean_n_c.loc[[1, 10]], mean_p_c.loc[[1, 10]]],\n    index=[\"Without punishment\", \"With punishment\"],\n)\n# Rename columns to have \"round\" in them\ncompare_grps.columns = [\"Round \" + str(i) for i in compare_grps.columns]\n# flip cols and index round ready for plotting (.T is transpose)\ncompare_grps = compare_grps.T\n# Make a bar chart\ncompare_grps.plot.bar(rot=0)\n\n\n\n\n\n\n\n\n\nn_c = data_n.agg([\"std\", \"var\", \"mean\"], 1)\nn_c\n\n\n\n\n\n\n\n\nstd\nvar\nmean\n\n\nPeriod\n\n\n\n\n\n\n\n1\n2.020724\n4.083325\n10.578313\n\n\n2\n2.238129\n5.009220\n10.628398\n\n\n3\n2.329569\n5.426891\n10.407079\n\n\n4\n2.068213\n4.277504\n9.813033\n\n\n5\n2.108329\n4.445049\n9.305433\n\n\n6\n2.240881\n5.021549\n8.454844\n\n\n7\n2.136614\n4.565117\n7.837568\n\n\n8\n2.349442\n5.519880\n7.376388\n\n\n9\n2.413845\n5.826645\n6.392985\n\n\n10\n2.187126\n4.783520\n4.383769\n\n\n\n\n\n\n\n\np_c = data_p.agg([\"std\", \"var\", \"mean\"], 1)\n\n\nfig, ax = plt.subplots()\nn_c[\"mean\"].plot(ax=ax, label=\"mean\")\n# mean + 2 sd\n(n_c[\"mean\"] + 2 * n_c[\"std\"]).plot(ax=ax, ylim=(0, None), color=\"red\", label=\"±2 s.d.\")\n# mean - 2 sd\n(n_c[\"mean\"] - 2 * n_c[\"std\"]).plot(ax=ax, ylim=(0, None), color=\"red\", label=\"\")\nfor i in range(len(data_n.columns)):\n    ax.scatter(x=data_n.index, y=data_n.iloc[:, i], color=\"k\", alpha=0.3)\nax.legend()\nax.set_ylabel(\"Average contribution\")\nax.set_title(\"Contribution to public goods game without punishment\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\np_c[\"mean\"].plot(ax=ax, label=\"mean\")\n# mean + 2 sd\n(p_c[\"mean\"] + 2 * p_c[\"std\"]).plot(ax=ax, ylim=(0, None), color=\"red\", label=\"±2 s.d.\")\n# mean - 2 sd\n(p_c[\"mean\"] - 2 * p_c[\"std\"]).plot(ax=ax, ylim=(0, None), color=\"red\", label=\"\")\nfor i in range(len(data_p.columns)):\n    ax.scatter(x=data_p.index, y=data_p.iloc[:, i], color=\"k\", alpha=0.3)\nax.legend()\nax.set_ylabel(\"Average contribution\")\nax.set_title(\"Contribution to public goods game without punishment\")\nplt.show()\n\n\n\n\n\n\n\n\n\ndata_p.apply(lambda x: x.max() - x.min(), axis=1)\n\nPeriod\n1     10.199675\n2     12.185065\n3     12.689935\n4     12.625000\n5     12.140375\n6     12.827541\n7     13.098931\n8     13.482621\n9     13.496754\n10    11.307360\ndtype: float64\n\n\n\n# A lambda function accepting three inputs, a,b and c and calculating the sum of the squares\ntest_function = lambda a, b, c: a**2 + b**2 + c**2\n\n# Now we apply the function by handing over (in parenthesis) the following inputs: a=3, b=4 and c=5\ntest_function(3, 4, 5)\n\n50\n\n\n\nrange_function = lambda x: x.max() - x.min()\nrange_p = data_p.apply(range_function, axis=1)\nrange_n = data_n.apply(range_function, axis=1)\n\n\nfig, ax = plt.subplots()\nrange_p.plot(ax=ax, label=\"With punishment\")\nrange_n.plot(ax=ax, label=\"Without punishment\")\nax.set_ylim(0, None)\nax.legend()\nax.set_title(\"Range of contributions to the public goods game\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfuncs_to_apply = [range_function, \"max\", \"min\", \"std\", \"mean\"]\nsumm_p = data_p.apply(funcs_to_apply, axis=1).rename(columns={\"&lt;lambda&gt;\": \"range\"})\nsumm_n = data_n.apply(funcs_to_apply, axis=1).rename(columns={\"&lt;lambda&gt;\": \"range\"})\n\n\nsumm_n.loc[[1, 10], :].round(2)\n\n\n\n\n\n\n\n\nrange\nmax\nmin\nstd\nmean\n\n\nPeriod\n\n\n\n\n\n\n\n\n\n1\n6.14\n14.10\n7.96\n2.02\n10.58\n\n\n10\n7.38\n8.68\n1.30\n2.19\n4.38\n\n\n\n\n\n\n\n\nsumm_p.loc[[1, 10], :].round(2)\n\n\n\n\n\n\n\n\nrange\nmax\nmin\nstd\nmean\n\n\nPeriod\n\n\n\n\n\n\n\n\n\n1\n10.20\n16.02\n5.82\n3.21\n10.64\n\n\n10\n11.31\n17.51\n6.20\n3.90\n12.87\n\n\n\n\n\n\n\n\npg.ttest(x=data_n.iloc[0, :], y=data_p.iloc[0, :])\n\n\n\n\n\n\n\n\nT\ndof\nalternative\np-val\nCI95%\ncohen-d\nBF10\npower\n\n\n\n\nT-test\n-0.063782\n30\ntwo-sided\n0.949567\n[-2.0, 1.87]\n0.02255\n0.337\n0.050437\n\n\n\n\n\n\n\n\npg.ttest(x=data_n.iloc[0, :], y=data_p.iloc[0, :], paired=True)\n\n\n\n\n\n\n\n\nT\ndof\nalternative\np-val\nCI95%\ncohen-d\nBF10\npower\n\n\n\n\nT-test\n-0.149959\n15\ntwo-sided\n0.882795\n[-0.92, 0.8]\n0.02255\n0.258\n0.05082\n\n\n\n\n\n\n\nSource: Create a dictionary with the data in\n\n\nPractice3-Empirical\n\n\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pathlib import Path\nimport pingouin as pg\nfrom lets_plot import *\n\nLetsPlot.setup_html(no_js=True)\n\n\n### You don't need to use these settings yourself\n### — they are just here to make the book look nicer!\n# Set the plot style for prettier charts:\n#plt.style.use(\n#    \"https://raw.githubusercontent.com/aeturrell/core_python/main/plot_style.txt\"\n#)\n\n\n#Python Walkthrough 1.1\ndf = pd.read_csv(\n    \"https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.csv\",\n    skiprows=1,\n    na_values=\"***\",\n)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nYear\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nJ-D\nD-N\nDJF\nMAM\nJJA\nSON\n\n\n\n\n0\n1880\n-0.39\n-0.54\n-0.24\n-0.31\n-0.05\n-0.18\n-0.23\n-0.27\n-0.26\n-0.31\n-0.46\n-0.43\n-0.31\nNaN\nNaN\n-0.20\n-0.23\n-0.34\n\n\n1\n1881\n-0.31\n-0.26\n-0.07\n-0.03\n0.03\n-0.34\n0.08\n-0.06\n-0.29\n-0.45\n-0.38\n-0.24\n-0.19\n-0.21\n-0.33\n-0.02\n-0.11\n-0.37\n\n\n2\n1882\n0.25\n0.20\n0.01\n-0.31\n-0.24\n-0.29\n-0.28\n-0.17\n-0.26\n-0.53\n-0.34\n-0.69\n-0.22\n-0.18\n0.07\n-0.18\n-0.25\n-0.38\n\n\n3\n1883\n-0.58\n-0.66\n-0.15\n-0.30\n-0.26\n-0.11\n-0.06\n-0.23\n-0.34\n-0.16\n-0.45\n-0.15\n-0.29\n-0.33\n-0.64\n-0.24\n-0.13\n-0.32\n\n\n4\n1884\n-0.17\n-0.11\n-0.64\n-0.59\n-0.36\n-0.41\n-0.41\n-0.51\n-0.45\n-0.45\n-0.58\n-0.47\n-0.43\n-0.40\n-0.14\n-0.53\n-0.45\n-0.49\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 145 entries, 0 to 144\nData columns (total 19 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   Year    145 non-null    int64  \n 1   Jan     145 non-null    float64\n 2   Feb     145 non-null    float64\n 3   Mar     145 non-null    float64\n 4   Apr     145 non-null    float64\n 5   May     145 non-null    float64\n 6   Jun     145 non-null    float64\n 7   Jul     145 non-null    float64\n 8   Aug     145 non-null    float64\n 9   Sep     145 non-null    float64\n 10  Oct     145 non-null    float64\n 11  Nov     144 non-null    float64\n 12  Dec     144 non-null    float64\n 13  J-D     144 non-null    float64\n 14  D-N     143 non-null    float64\n 15  DJF     144 non-null    float64\n 16  MAM     145 non-null    float64\n 17  JJA     145 non-null    float64\n 18  SON     144 non-null    float64\ndtypes: float64(18), int64(1)\nmemory usage: 21.6 KB\n\n\n\n#Python Walkthrough 1.2\n\ndf = df.set_index(\"Year\")\ndf.head()\ndf.tail()\n\n\n\n\n\n\n\n\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nJ-D\nD-N\nDJF\nMAM\nJJA\nSON\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2020\n1.59\n1.70\n1.67\n1.40\n1.27\n1.14\n1.10\n1.12\n1.19\n1.21\n1.59\n1.19\n1.35\n1.36\n1.56\n1.44\n1.12\n1.33\n\n\n2021\n1.25\n0.96\n1.21\n1.13\n1.05\n1.21\n1.07\n1.03\n1.05\n1.30\n1.29\n1.17\n1.14\n1.14\n1.13\n1.13\n1.10\n1.21\n\n\n2022\n1.25\n1.17\n1.41\n1.09\n1.02\n1.13\n1.06\n1.17\n1.15\n1.31\n1.10\n1.06\n1.16\n1.17\n1.19\n1.17\n1.12\n1.19\n\n\n2023\n1.30\n1.30\n1.64\n1.02\n1.13\n1.19\n1.45\n1.57\n1.67\n1.88\n1.98\n1.85\n1.50\n1.43\n1.22\n1.26\n1.40\n1.84\n\n\n2024\n1.68\n1.93\n1.78\n1.80\n1.45\n1.54\n1.42\n1.42\n1.58\n1.72\nNaN\nNaN\nNaN\nNaN\n1.82\n1.67\n1.46\nNaN\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\ndf[\"Jan\"].plot(ax=ax)\nax.set_ylabel(\"y label\")\nax.set_xlabel(\"x label\")\nax.set_title(\"title\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\nax.plot(df.index, df[\"Jan\"])\nax.set_ylabel(\"y label\")\nax.set_xlabel(\"x label\")\nax.set_title(\"title\")\nplt.show()\n\n\n\n\n\n\n\n\n\nmonth = \"Jan\"\nfig, ax = plt.subplots()\nax.axhline(0, color=\"orange\")\nax.annotate(\"1951—1980 average\", xy=(0.66, -0.2), xycoords=(\"figure fraction\", \"data\"))\ndf[month].plot(ax=ax)\nax.set_title(\n    f\"Average temperature anomaly in {month} \\n in the northern hemisphere (1880—{df.index.max()})\"\n)\nax.set_ylabel(\"Annual temperature anomalies\")\n\nText(0, 0.5, 'Annual temperature anomalies')\n\n\n\n\n\n\n\n\n\n\n#Python Walkthrough 1.3\nmonth = \"J-D\"\nfig, ax = plt.subplots()\nax.axhline(0, color=\"orange\")\nax.annotate(\"1951—1980 average\", xy=(0.68, -0.2), xycoords=(\"figure fraction\", \"data\"))\ndf[month].plot(ax=ax)\nax.set_title(\n    f\"Average annual temperature anomaly in \\n in the northern hemisphere (1880—{df.index.max()})\"\n)\nax.set_ylabel(\"Annual temperature anomalies\")\n\nText(0, 0.5, 'Annual temperature anomalies')\n\n\n\n\n\n\n\n\n\n\n#Python Walkthrough 1.4\ndf[\"Period\"] = pd.cut(\n    df.index,\n    bins=[1921, 1950, 1980, 2010],\n    labels=[\"1921—1950\", \"1951—1980\", \"1981—2010\"],\n    ordered=True,\n)\n\n\ndf[\"Period\"].tail(20)\n\nYear\n2005    1981—2010\n2006    1981—2010\n2007    1981—2010\n2008    1981—2010\n2009    1981—2010\n2010    1981—2010\n2011          NaN\n2012          NaN\n2013          NaN\n2014          NaN\n2015          NaN\n2016          NaN\n2017          NaN\n2018          NaN\n2019          NaN\n2020          NaN\n2021          NaN\n2022          NaN\n2023          NaN\n2024          NaN\nName: Period, dtype: category\nCategories (3, object): ['1921—1950' &lt; '1951—1980' &lt; '1981—2010']\n\n\n\nlist_of_months = [\"Jun\", \"Jul\", \"Aug\"]\ndf[list_of_months].stack().head()\n\nYear     \n1880  Jun   -0.18\n      Jul   -0.23\n      Aug   -0.27\n1881  Jun   -0.34\n      Jul    0.08\ndtype: float64\n\n\n\nfig, axes = plt.subplots(ncols=3, figsize=(9, 4), sharex=True, sharey=True)\nfor ax, period in zip(axes, df[\"Period\"].dropna().unique()):\n    df.loc[df[\"Period\"] == period, list_of_months].stack().hist(ax=ax)\n    ax.set_title(period)\nplt.suptitle(\"Histogram of temperature anomalies\")\naxes[1].set_xlabel(\"Summer temperature distribution\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n#Python Walkthrough 1.5\n# Create a variable that has years 1951 to 1980, and months Jan to Dec (inclusive)\ntemp_all_months = df.loc[(df.index &gt;= 1951) & (df.index &lt;= 1980), \"Jan\":\"Dec\"]\n# Put all the data in stacked format and give the new columns sensible names\ntemp_all_months = (\n    temp_all_months.stack()\n    .reset_index()\n    .rename(columns={\"level_1\": \"month\", 0: \"values\"})\n)\n# Take a look at this data:\ntemp_all_months\n\n\n\n\n\n\n\n\nYear\nmonth\nvalues\n\n\n\n\n0\n1951\nJan\n-0.36\n\n\n1\n1951\nFeb\n-0.51\n\n\n2\n1951\nMar\n-0.18\n\n\n3\n1951\nApr\n0.06\n\n\n4\n1951\nMay\n0.17\n\n\n...\n...\n...\n...\n\n\n355\n1980\nAug\n0.10\n\n\n356\n1980\nSep\n0.10\n\n\n357\n1980\nOct\n0.12\n\n\n358\n1980\nNov\n0.21\n\n\n359\n1980\nDec\n0.09\n\n\n\n\n360 rows × 3 columns\n\n\n\n\nquantiles = [0.3, 0.7]\nlist_of_percentiles = np.quantile(temp_all_months[\"values\"], q=quantiles)\n\nprint(f\"The cold threshold of {quantiles[0]*100}% is {list_of_percentiles[0]}\")\nprint(f\"The hot threshold of {quantiles[1]*100}% is {list_of_percentiles[1]}\")\n\nThe cold threshold of 30.0% is -0.1\nThe hot threshold of 70.0% is 0.1\n\n\n\n#Python Walkthrough 1.6\n# Create a variable that has years 1981 to 2010, and months Jan to Dec (inclusive)\ntemp_all_months = df.loc[(df.index &gt;= 1981) & (df.index &lt;= 2010), \"Jan\":\"Dec\"]\n# Put all the data in stacked format and give the new columns sensible names\ntemp_all_months = (\n    temp_all_months.stack()\n    .reset_index()\n    .rename(columns={\"level_1\": \"month\", 0: \"values\"})\n)\n# Take a look at the start of this data data:\ntemp_all_months.head()\n\n\n\n\n\n\n\n\nYear\nmonth\nvalues\n\n\n\n\n0\n1981\nJan\n0.80\n\n\n1\n1981\nFeb\n0.62\n\n\n2\n1981\nMar\n0.68\n\n\n3\n1981\nApr\n0.39\n\n\n4\n1981\nMay\n0.18\n\n\n\n\n\n\n\n\nentries_less_than_q30 = temp_all_months[\"values\"] &lt; list_of_percentiles[0]\nproportion_under_q30 = entries_less_than_q30.mean()\nprint(\n    f\"The proportion under {list_of_percentiles[0]} is {proportion_under_q30*100:.2f}%\"\n)\n\nThe proportion under -0.1 is 1.94%\n\n\n\nproportion_over_q70 = (temp_all_months[\"values\"] &gt; list_of_percentiles[1]).mean()\nprint(f\"The proportion over {list_of_percentiles[1]} is {proportion_over_q70*100:.2f}%\")\n\nThe proportion over 0.1 is 84.72%\n\n\n\n#Python Walkthrough 1.7\ntemp_all_months = (\n    df.loc[:, \"DJF\":\"SON\"]\n    .stack()\n    .reset_index()\n    .rename(columns={\"level_1\": \"season\", 0: \"values\"})\n)\ntemp_all_months[\"Period\"] = pd.cut(\n    temp_all_months[\"Year\"],\n    bins=[1921, 1950, 1980, 2010],\n    labels=[\"1921—1950\", \"1951—1980\", \"1981—2010\"],\n    ordered=True,\n)\n# Take a look at a cut of the data using `.iloc`, which provides position\ntemp_all_months.iloc[-135:-125]\n\n\n\n\n\n\n\n\nYear\nseason\nvalues\nPeriod\n\n\n\n\n443\n1991\nDJF\n0.51\n1981—2010\n\n\n444\n1991\nMAM\n0.45\n1981—2010\n\n\n445\n1991\nJJA\n0.42\n1981—2010\n\n\n446\n1991\nSON\n0.32\n1981—2010\n\n\n447\n1992\nDJF\n0.44\n1981—2010\n\n\n448\n1992\nMAM\n0.30\n1981—2010\n\n\n449\n1992\nJJA\n-0.04\n1981—2010\n\n\n450\n1992\nSON\n-0.15\n1981—2010\n\n\n451\n1993\nDJF\n0.37\n1981—2010\n\n\n452\n1993\nMAM\n0.31\n1981—2010\n\n\n\n\n\n\n\n\ngrp_mean_var = temp_all_months.groupby([\"season\", \"Period\"])[\"values\"].agg(\n    [np.mean, np.var]\n)\ngrp_mean_var\n\n\n\n\n\n\n\n\n\nmean\nvar\n\n\nseason\nPeriod\n\n\n\n\n\n\nDJF\n1921—1950\n-0.026207\n0.057303\n\n\n1951—1980\n-0.002333\n0.050494\n\n\n1981—2010\n0.524333\n0.079646\n\n\nJJA\n1921—1950\n-0.052414\n0.021290\n\n\n1951—1980\n-0.000333\n0.014631\n\n\n1981—2010\n0.400333\n0.067727\n\n\nMAM\n1921—1950\n-0.041724\n0.031236\n\n\n1951—1980\n0.000333\n0.025245\n\n\n1981—2010\n0.510333\n0.076238\n\n\nSON\n1921—1950\n0.083103\n0.027751\n\n\n1951—1980\n-0.001000\n0.026258\n\n\n1981—2010\n0.429333\n0.111731\n\n\n\n\n\n\n\n\nmin_year = 1880\n(\n    ggplot(temp_all_months, aes(x=\"Year\", y=\"values\", color=\"season\"))\n    + geom_abline(slope=0, color=\"black\", size=1)\n    + geom_line(size=1)\n    + labs(\n        title=f\"Average annual temperature anomaly in \\n in the northern hemisphere ({min_year}—{temp_all_months['Year'].max()})\",\n        y=\"Annual temperature anomalies\",\n    )\n    + scale_x_continuous(format=\"d\")\n    + geom_text(\n        x=min_year, y=0.1, label=\"1951—1980 average\", hjust=\"left\", color=\"black\"\n    )\n)\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n      \n      \n        \n          \n            \n              \n              \n            \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n            \n              \n                \n                  \n                    1951—1980 average\n                  \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                1880\n              \n            \n          \n          \n            \n            \n            \n              \n                1900\n              \n            \n          \n          \n            \n            \n            \n              \n                1920\n              \n            \n          \n          \n            \n            \n            \n              \n                1940\n              \n            \n          \n          \n            \n            \n            \n              \n                1960\n              \n            \n          \n          \n            \n            \n            \n              \n                1980\n              \n            \n          \n          \n            \n            \n            \n              \n                2000\n              \n            \n          \n          \n            \n            \n            \n              \n                2020\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                -1.0\n              \n            \n          \n          \n            \n              \n                -0.5\n              \n            \n          \n          \n            \n              \n                0.0\n              \n            \n          \n          \n            \n              \n                0.5\n              \n            \n          \n          \n            \n              \n                1.0\n              \n            \n          \n          \n            \n              \n                1.5\n              \n            \n          \n        \n      \n    \n    \n      \n        Average annual temperature anomaly in \n      \n      \n         in the northern hemisphere (1880—2024)\n      \n    \n    \n      \n        Annual temperature anomalies\n      \n    \n    \n      \n        Year\n      \n    \n    \n      \n      \n      \n        \n          \n            season\n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                MAM\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                JJA\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                SON\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                DJF\n              \n            \n          \n        \n      \n    \n    \n    \n  \n  \n  \n\n\n\n\n#Python Walkthrough 1.8\ndf_co2 = pd.read_csv(\"D:\\360MoveData\\Users\\75903\\Desktop\\SuYibo-Homework\\Practice1 SuYibo/CO2-data.csv\")\ndf_co2.head()\n\n\n\n\n\n\n\n\nYear\nMonth\nMonthly average\nInterpolated\nTrend\n\n\n\n\n0\n1958\n3\n315.71\n315.71\n314.62\n\n\n1\n1958\n4\n317.45\n317.45\n315.29\n\n\n2\n1958\n5\n317.50\n317.50\n314.71\n\n\n3\n1958\n6\n-99.99\n317.10\n314.85\n\n\n4\n1958\n7\n315.86\n315.86\n314.98\n\n\n\n\n\n\n\n\ndf_co2_june = df_co2.loc[df_co2[\"Month\"] == 6]\ndf_co2_june.head()\n\n\n\n\n\n\n\n\nYear\nMonth\nMonthly average\nInterpolated\nTrend\n\n\n\n\n3\n1958\n6\n-99.99\n317.10\n314.85\n\n\n15\n1959\n6\n318.15\n318.15\n315.92\n\n\n27\n1960\n6\n319.59\n319.59\n317.36\n\n\n39\n1961\n6\n319.77\n319.77\n317.48\n\n\n51\n1962\n6\n320.55\n320.55\n318.27\n\n\n\n\n\n\n\n\ndf_temp_co2 = pd.merge(df_co2_june, df, on=\"Year\")\ndf_temp_co2[[\"Year\", \"Jun\", \"Trend\"]].head()\n\n\n\n\n\n\n\n\nYear\nJun\nTrend\n\n\n\n\n0\n1958\n0.05\n314.85\n\n\n1\n1959\n0.14\n315.92\n\n\n2\n1960\n0.18\n317.36\n\n\n3\n1961\n0.18\n317.48\n\n\n4\n1962\n-0.13\n318.27\n\n\n\n\n\n\n\n\n(\n    ggplot(df_temp_co2, aes(x=\"Jun\", y=\"Trend\"))\n    + geom_point(color=\"black\", size=3)\n    + labs(\n        title=\"Scatterplot of temperature anomalies vs carbon dioxide emissions\",\n        y=\"Carbon dioxide levels (trend, mole fraction)\",\n        x=\"Temperature anomaly (degrees Celsius)\",\n    )\n)\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                -0.2\n              \n            \n          \n          \n            \n            \n            \n              \n                0.0\n              \n            \n          \n          \n            \n            \n            \n              \n                0.2\n              \n            \n          \n          \n            \n            \n            \n              \n                0.4\n              \n            \n          \n          \n            \n            \n            \n              \n                0.6\n              \n            \n          \n          \n            \n            \n            \n              \n                0.8\n              \n            \n          \n          \n            \n            \n            \n              \n                1.0\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                320\n              \n            \n          \n          \n            \n              \n                340\n              \n            \n          \n          \n            \n              \n                360\n              \n            \n          \n          \n            \n              \n                380\n              \n            \n          \n          \n            \n              \n                400\n              \n            \n          \n        \n      \n    \n    \n      \n        Scatterplot of temperature anomalies vs carbon dioxide emissions\n      \n    \n    \n      \n        Carbon dioxide levels (trend, mole fraction)\n      \n    \n    \n      \n        Temperature anomaly (degrees Celsius)\n      \n    \n    \n    \n  \n  \n  \n\n\n\n\ndf_temp_co2[[\"Jun\", \"Trend\"]].corr(method=\"pearson\")\n\n\n\n\n\n\n\n\nJun\nTrend\n\n\n\n\nJun\n1.00000\n0.91495\n\n\nTrend\n0.91495\n1.00000\n\n\n\n\n\n\n\n\n(\n    ggplot(df_temp_co2, aes(x=\"Year\", y=\"Jun\"))\n    + geom_line(size=1)\n    + labs(\n        title=\"June temperature anomalies\",\n    )\n    + scale_x_continuous(format=\"d\")\n)\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                1960\n              \n            \n          \n          \n            \n            \n            \n              \n                1970\n              \n            \n          \n          \n            \n            \n            \n              \n                1980\n              \n            \n          \n          \n            \n            \n            \n              \n                1990\n              \n            \n          \n          \n            \n            \n            \n              \n                2000\n              \n            \n          \n          \n            \n            \n            \n              \n                2010\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                -0.2\n              \n            \n          \n          \n            \n              \n                0.0\n              \n            \n          \n          \n            \n              \n                0.2\n              \n            \n          \n          \n            \n              \n                0.4\n              \n            \n          \n          \n            \n              \n                0.6\n              \n            \n          \n          \n            \n              \n                0.8\n              \n            \n          \n          \n            \n              \n                1.0\n              \n            \n          \n        \n      \n    \n    \n      \n        June temperature anomalies\n      \n    \n    \n      \n        Jun\n      \n    \n    \n      \n        Year\n      \n    \n    \n    \n  \n  \n  \n\n\n\n\nbase_plot = ggplot(df_temp_co2) + scale_x_continuous(format=\"d\")\nplot_p = (\n    base_plot\n    + geom_line(aes(x=\"Year\", y=\"Jun\"), size=1)\n    + labs(title=\"June temperature anomalies\")\n)\nplot_q = (\n    base_plot\n    + geom_line(aes(x=\"Year\", y=\"Trend\"), size=1)\n    + labs(title=\"Carbon dioxide emissions\")\n)\ngggrid([plot_p, plot_q], ncol=2)\n\n\n  \n    \n    \n  \n  \n    \n    \n      \n      \n      \n        \n          \n            \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n            \n          \n          \n            \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n            \n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n          \n          \n            \n              \n              \n            \n          \n        \n        \n          \n            \n              \n              \n              \n                \n                  1960\n                \n              \n            \n            \n              \n              \n              \n                \n                  1970\n                \n              \n            \n            \n              \n              \n              \n                \n                  1980\n                \n              \n            \n            \n              \n              \n              \n                \n                  1990\n                \n              \n            \n            \n              \n              \n              \n                \n                  2000\n                \n              \n            \n            \n              \n              \n              \n                \n                  2010\n                \n              \n            \n            \n            \n          \n          \n            \n              \n                \n                  -0.2\n                \n              \n            \n            \n              \n                \n                  0.0\n                \n              \n            \n            \n              \n                \n                  0.2\n                \n              \n            \n            \n              \n                \n                  0.4\n                \n              \n            \n            \n              \n                \n                  0.6\n                \n              \n            \n            \n              \n                \n                  0.8\n                \n              \n            \n            \n              \n                \n                  1.0\n                \n              \n            \n          \n        \n      \n      \n        \n          June temperature anomalies\n        \n      \n      \n        \n          Jun\n        \n      \n      \n        \n          Year\n        \n      \n      \n      \n    \n    \n    \n  \n  \n    \n    \n      \n      \n      \n        \n          \n            \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n            \n          \n          \n            \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n            \n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n          \n          \n            \n              \n              \n            \n          \n        \n        \n          \n            \n              \n              \n              \n                \n                  1960\n                \n              \n            \n            \n              \n              \n              \n                \n                  1970\n                \n              \n            \n            \n              \n              \n              \n                \n                  1980\n                \n              \n            \n            \n              \n              \n              \n                \n                  1990\n                \n              \n            \n            \n              \n              \n              \n                \n                  2000\n                \n              \n            \n            \n              \n              \n              \n                \n                  2010\n                \n              \n            \n            \n            \n          \n          \n            \n              \n                \n                  320\n                \n              \n            \n            \n              \n                \n                  340\n                \n              \n            \n            \n              \n                \n                  360\n                \n              \n            \n            \n              \n                \n                  380\n                \n              \n            \n            \n              \n                \n                  400\n                \n              \n            \n          \n        \n      \n      \n        \n          Carbon dioxide emissions\n        \n      \n      \n        \n          Trend\n        \n      \n      \n        \n          Year\n        \n      \n      \n      \n    \n    \n    \n  \n\n\n\nSource: You don't need to use these settings yourself\n\n\nPractice3-Practice3.1\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nimport textwrap\n\n\npd.read_csv(\n    \"https://vincentarelbundock.github.io/Rdatasets/csv/dplyr/storms.csv\", nrows=10\n)\n\n\n\n\n\n\n\n\nrownames\nname\nyear\nmonth\nday\nhour\nlat\nlong\nstatus\ncategory\nwind\npressure\ntropicalstorm_force_diameter\nhurricane_force_diameter\n\n\n\n\n0\n1\nAmy\n1975\n6\n27\n0\n27.5\n-79.0\ntropical depression\nNaN\n25\n1013\nNaN\nNaN\n\n\n1\n2\nAmy\n1975\n6\n27\n6\n28.5\n-79.0\ntropical depression\nNaN\n25\n1013\nNaN\nNaN\n\n\n2\n3\nAmy\n1975\n6\n27\n12\n29.5\n-79.0\ntropical depression\nNaN\n25\n1013\nNaN\nNaN\n\n\n3\n4\nAmy\n1975\n6\n27\n18\n30.5\n-79.0\ntropical depression\nNaN\n25\n1013\nNaN\nNaN\n\n\n4\n5\nAmy\n1975\n6\n28\n0\n31.5\n-78.8\ntropical depression\nNaN\n25\n1012\nNaN\nNaN\n\n\n5\n6\nAmy\n1975\n6\n28\n6\n32.4\n-78.7\ntropical depression\nNaN\n25\n1012\nNaN\nNaN\n\n\n6\n7\nAmy\n1975\n6\n28\n12\n33.3\n-78.0\ntropical depression\nNaN\n25\n1011\nNaN\nNaN\n\n\n7\n8\nAmy\n1975\n6\n28\n18\n34.0\n-77.0\ntropical depression\nNaN\n30\n1006\nNaN\nNaN\n\n\n8\n9\nAmy\n1975\n6\n29\n0\n34.4\n-75.8\ntropical storm\nNaN\n35\n1004\nNaN\nNaN\n\n\n9\n10\nAmy\n1975\n6\n29\n6\n34.0\n-74.8\ntropical storm\nNaN\n40\n1002\nNaN\nNaN\n\n\n\n\n\n\n\n\n#\nimport requests\n\nurl = \"https://api.ons.gov.uk/timeseries/JP9Z/dataset/UNEM/data\"\n\nresponse = requests.get(url)\n\nprint(f\"请求状态码: {response.status_code}\")\nprint(f\"返回的原始内容: {response.text}\")\n#The interface API is no longer available, so I can't get the data at all!\n\n请求状态码: 404\n返回的原始内容: This API is being decommissioned as part of a suite of work to improve the digital products and services we offer. It is planned to be fully retired on 25/11/2024. If you have any queries please contact us at apiservice@ons.gov.uk.\n\n\n\nFor the above, I made changes to the code. Using the FRED API key, query series_id=“LMJVTTUVGBM647S”, which is the series number of the UK job vacancies (in thousands).167ccd5ae95c579ae64d6b1295c563feThis is Jie Xu’s FRED API KEY\n\nimport requests\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# 替换为你的 FRED API key\napi_key = \"167ccd5ae95c579ae64d6b1295c563fe\"\nurl = f\"https://api.stlouisfed.org/fred/series/observations?series_id=LMJVTTUVGBM647S&api_key={api_key}&file_type=json\"\n\nresponse = requests.get(url)\ndata = response.json()['observations']\n\n# 提取日期和职位空缺数量，过滤掉非数字数据\ndates = [item['date'] for item in data]\nvalues = []\nfor item in data:\n    try:\n        values.append(float(item['value']))\n    except ValueError:\n        values.append(None)  # 使用 None 表示无效数据\n\n# 创建 DataFrame 并去除无效数据\ndf = pd.DataFrame({'Date': dates, 'Vacancies': values})\ndf.dropna(inplace=True)\n\n# 绘制折线图\nplt.figure(figsize=(10, 5))\nplt.plot(df['Date'], df['Vacancies'], label='UK Job Vacancies (Service Industries)', color='blue')\nplt.xlabel('Date')\nplt.ylabel('Vacancies (Thousands)')\nplt.title('UK Job Vacancies (Service Industries)')\nplt.xticks(rotation=45)\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport pandas_datareader.data as web\n\ndf_u = web.DataReader(\"LRHUTTTTGBM156S\", \"fred\")\n\ndf_u.plot(title=\"UK unemployment (percent)\", legend=False, ylim=(2, 6), lw=3.0)\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport requests\n\nurl = \"https://databank.worldbank.org/source/millennium-development-goals/Series/EN.ATM.CO2E.KT\"\nhtml = requests.get(url).content\ndf = pd.read_html(html)[0]\nprint(df.columns)\n\nIndex(['Indicator', 'Rule', 'Weighted Indicator', '%'], dtype='object')\n\n\nFirst, I went to Getting Data-Coding for Economists based on the code, but could not get the data. Then, I went to https://datacatalog.worldbank.org/indicator/b66c366b-bdce-eb11-bacc-000d3a596ff0/CO2-emissions–metric-tons-per-capita. this site to get the Data and the page keeps reporting an error: An error occurs. This could be due to\nOur network is unusually busy The server is currently unavailable Please try again later or contact data@worldbank.org\nError ID: 9bcb1bcd7bd91946aaddafba37719f72. So I can only use excel sheet.\n\nimport pandas as pd\n\n# 读取Excel文件，假设数据在第一个工作表\ndf = pd.read_excel('data\\EN.ATM.CO2E.PC.xlsx')\n\n# 重命名列（如果需要），确保列名与你的要求一致，这里假设原始列名分别为'Country Name'、'Year'、'EN.ATM.CO2E.KT'\ndf.rename(columns={'Country Name': 'country', 'Year': 'year', 'EN.ATM.CO2E.KT': 'EN.ATM.CO2E.PC'}, inplace=True)\n\n# 选择需要的列并按照人均二氧化碳排放量排序\nselected_data = df[['country', 'year', 'EN.ATM.CO2E.PC']].sort_values('EN.ATM.CO2E.PC')\n\n# 显示处理后的数据表格\nprint(selected_data)\n\n                   country  year  EN.ATM.CO2E.PC\n0                    India  2017        1.704927\n1     East Asia\\n& Pacific  2017        5.960076\n2  Europe &\\nCentral\\nAsia  2017        6.746232\n3                    China  2017        7.226160\n4           United\\nStates  2017       14.823245\n\n\n\nimport seaborn as sns\n\nfig, ax = plt.subplots()\nsns.barplot(x=\"country\", y=\"EN.ATM.CO2E.PC\", data=df.reset_index(), ax=ax)\nax.set_title(r\"CO$_2$ (metric tons per capita)\", loc=\"right\")\nplt.suptitle(\"The USA leads the world on per-capita emissions\", y=1.01)\nfor key, spine in ax.spines.items():\n    spine.set_visible(False)\nax.set_ylabel(\"\")\nax.set_xlabel(\"\")\nax.yaxis.tick_right()\nplt.show()\n\n\n\n\n\n\n\n\nFor this link: https://stats.oecd.org/SDMX-JSON/data/PDB_LV/GBR+FRA+CAN+ITA+DEU+JPN+USA.T_GDPEMP.CPC/all?startTime=2010 can no longer be found.404 - File or directory not found. The resource you are looking for may have been deleted, renamed, or is temporarily unavailable.\n\nurl = \"http://aeturrell.com/research\"\npage = requests.get(url)\npage.text[:300]\n\n'&lt;!DOCTYPE html&gt;\\n&lt;html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\" xml:lang=\"en\"&gt;&lt;head&gt;\\n\\n&lt;meta charset=\"utf-8\"&gt;\\n&lt;meta name=\"generator\" content=\"quarto-1.5.56\"&gt;\\n\\n&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, user-scalable=yes\"&gt;\\n\\n&lt;meta name=\"author\" content=\"Arthur Turrell\"&gt;\\n'\n\n\n\nsoup = BeautifulSoup(page.text, \"html.parser\")\nprint(soup.prettify()[60000:60500])\n\n       &lt;/div&gt;\n          &lt;div class=\"project-category\"&gt;\n           &lt;a href=\"#category=gender pay gap\"&gt;\n            gender pay gap\n           &lt;/a&gt;\n          &lt;/div&gt;\n          &lt;div class=\"project-category\"&gt;\n           &lt;a href=\"#category=labour\"&gt;\n            labour\n           &lt;/a&gt;\n          &lt;/div&gt;\n          &lt;div class=\"project-category\"&gt;\n           &lt;a href=\"#category=text analysis\"&gt;\n            text analysis\n           &lt;/a&gt;\n          &lt;/div&gt;\n         &lt;/div&gt;\n         &lt;div class=\"project-details-listing\n\n\n\n# Get all paragraphs\nall_paras = soup.find_all(\"p\")\n# Just show one of the paras\nall_paras[1]\n\n&lt;p&gt;Botta, Federico, Robin Lovelace, Laura Gilbert, and Arthur Turrell. \"Packaging code and data for reproducible research: A case study of journey time statistics.\" &lt;i&gt;Environment and Planning B: Urban Analytics and City Science&lt;/i&gt; (2024): 23998083241267331. doi: &lt;a href=\"https://doi.org/10.1177/23998083241267331\"&gt;&lt;code&gt;10.1177/23998083241267331&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;\n\n\n\nall_paras[1].text\n\n'Botta, Federico, Robin Lovelace, Laura Gilbert, and Arthur Turrell. \"Packaging code and data for reproducible research: A case study of journey time statistics.\" Environment and Planning B: Urban Analytics and City Science (2024): 23998083241267331. doi: 10.1177/23998083241267331'\n\n\n\nprojects = soup.find_all(\"div\", class_=\"project-content listing-pub-info\")\nprojects = [x.text.strip() for x in projects]\nprojects[:4]\n\n['Botta, Federico, Robin Lovelace, Laura Gilbert, and Arthur Turrell. \"Packaging code and data for reproducible research: A case study of journey time statistics.\" Environment and Planning B: Urban Analytics and City Science (2024): 23998083241267331. doi: 10.1177/23998083241267331',\n 'Kalamara, Eleni, Arthur Turrell, Chris Redl, George Kapetanios, and Sujit Kapadia. \"Making text count: economic forecasting using newspaper text.\" Journal of Applied Econometrics 37, no. 5 (2022): 896-919. doi: 10.1002/jae.2907',\n 'Turrell, A., Speigner, B., Copple, D., Djumalieva, J. and Thurgood, J., 2021. Is the UK’s productivity puzzle mostly driven by occupational mismatch? An analysis using big data on job vacancies. Labour Economics, 71, p.102013. doi: 10.1016/j.labeco.2021.102013',\n 'Haldane, Andrew G., and Arthur E. Turrell. \"Drawing on different disciplines: macroeconomic agent-based models.\" Journal of Evolutionary Economics 29 (2019): 39-66. doi: 10.1007/s00191-018-0557-5']\n\n\n\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef scraper(url):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # 检查请求是否成功，若不成功则抛出异常\n        soup = BeautifulSoup(response.text, 'html.parser')\n        return soup.title.string if soup.title else \"No title found\"\n    except requests.RequestException as e:\n        print(f\"Error fetching page {url}: {e}\")\n        return None\n\nstart, stop = 0, 50\nroot_url = \"www.codingforeconomists.com/page=\"\ninfo_on_pages = []\nfor i in range(start, stop):\n    url = root_url + str(i)\n    info = scraper(url)\n    info_on_pages.append(info)\n\nprint(info_on_pages)\n\nError fetching page www.codingforeconomists.com/page=0: Invalid URL 'www.codingforeconomists.com/page=0': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=0?\nError fetching page www.codingforeconomists.com/page=1: Invalid URL 'www.codingforeconomists.com/page=1': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=1?\nError fetching page www.codingforeconomists.com/page=2: Invalid URL 'www.codingforeconomists.com/page=2': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=2?\nError fetching page www.codingforeconomists.com/page=3: Invalid URL 'www.codingforeconomists.com/page=3': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=3?\nError fetching page www.codingforeconomists.com/page=4: Invalid URL 'www.codingforeconomists.com/page=4': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=4?\nError fetching page www.codingforeconomists.com/page=5: Invalid URL 'www.codingforeconomists.com/page=5': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=5?\nError fetching page www.codingforeconomists.com/page=6: Invalid URL 'www.codingforeconomists.com/page=6': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=6?\nError fetching page www.codingforeconomists.com/page=7: Invalid URL 'www.codingforeconomists.com/page=7': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=7?\nError fetching page www.codingforeconomists.com/page=8: Invalid URL 'www.codingforeconomists.com/page=8': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=8?\nError fetching page www.codingforeconomists.com/page=9: Invalid URL 'www.codingforeconomists.com/page=9': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=9?\nError fetching page www.codingforeconomists.com/page=10: Invalid URL 'www.codingforeconomists.com/page=10': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=10?\nError fetching page www.codingforeconomists.com/page=11: Invalid URL 'www.codingforeconomists.com/page=11': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=11?\nError fetching page www.codingforeconomists.com/page=12: Invalid URL 'www.codingforeconomists.com/page=12': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=12?\nError fetching page www.codingforeconomists.com/page=13: Invalid URL 'www.codingforeconomists.com/page=13': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=13?\nError fetching page www.codingforeconomists.com/page=14: Invalid URL 'www.codingforeconomists.com/page=14': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=14?\nError fetching page www.codingforeconomists.com/page=15: Invalid URL 'www.codingforeconomists.com/page=15': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=15?\nError fetching page www.codingforeconomists.com/page=16: Invalid URL 'www.codingforeconomists.com/page=16': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=16?\nError fetching page www.codingforeconomists.com/page=17: Invalid URL 'www.codingforeconomists.com/page=17': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=17?\nError fetching page www.codingforeconomists.com/page=18: Invalid URL 'www.codingforeconomists.com/page=18': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=18?\nError fetching page www.codingforeconomists.com/page=19: Invalid URL 'www.codingforeconomists.com/page=19': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=19?\nError fetching page www.codingforeconomists.com/page=20: Invalid URL 'www.codingforeconomists.com/page=20': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=20?\nError fetching page www.codingforeconomists.com/page=21: Invalid URL 'www.codingforeconomists.com/page=21': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=21?\nError fetching page www.codingforeconomists.com/page=22: Invalid URL 'www.codingforeconomists.com/page=22': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=22?\nError fetching page www.codingforeconomists.com/page=23: Invalid URL 'www.codingforeconomists.com/page=23': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=23?\nError fetching page www.codingforeconomists.com/page=24: Invalid URL 'www.codingforeconomists.com/page=24': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=24?\nError fetching page www.codingforeconomists.com/page=25: Invalid URL 'www.codingforeconomists.com/page=25': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=25?\nError fetching page www.codingforeconomists.com/page=26: Invalid URL 'www.codingforeconomists.com/page=26': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=26?\nError fetching page www.codingforeconomists.com/page=27: Invalid URL 'www.codingforeconomists.com/page=27': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=27?\nError fetching page www.codingforeconomists.com/page=28: Invalid URL 'www.codingforeconomists.com/page=28': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=28?\nError fetching page www.codingforeconomists.com/page=29: Invalid URL 'www.codingforeconomists.com/page=29': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=29?\nError fetching page www.codingforeconomists.com/page=30: Invalid URL 'www.codingforeconomists.com/page=30': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=30?\nError fetching page www.codingforeconomists.com/page=31: Invalid URL 'www.codingforeconomists.com/page=31': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=31?\nError fetching page www.codingforeconomists.com/page=32: Invalid URL 'www.codingforeconomists.com/page=32': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=32?\nError fetching page www.codingforeconomists.com/page=33: Invalid URL 'www.codingforeconomists.com/page=33': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=33?\nError fetching page www.codingforeconomists.com/page=34: Invalid URL 'www.codingforeconomists.com/page=34': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=34?\nError fetching page www.codingforeconomists.com/page=35: Invalid URL 'www.codingforeconomists.com/page=35': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=35?\nError fetching page www.codingforeconomists.com/page=36: Invalid URL 'www.codingforeconomists.com/page=36': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=36?\nError fetching page www.codingforeconomists.com/page=37: Invalid URL 'www.codingforeconomists.com/page=37': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=37?\nError fetching page www.codingforeconomists.com/page=38: Invalid URL 'www.codingforeconomists.com/page=38': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=38?\nError fetching page www.codingforeconomists.com/page=39: Invalid URL 'www.codingforeconomists.com/page=39': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=39?\nError fetching page www.codingforeconomists.com/page=40: Invalid URL 'www.codingforeconomists.com/page=40': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=40?\nError fetching page www.codingforeconomists.com/page=41: Invalid URL 'www.codingforeconomists.com/page=41': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=41?\nError fetching page www.codingforeconomists.com/page=42: Invalid URL 'www.codingforeconomists.com/page=42': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=42?\nError fetching page www.codingforeconomists.com/page=43: Invalid URL 'www.codingforeconomists.com/page=43': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=43?\nError fetching page www.codingforeconomists.com/page=44: Invalid URL 'www.codingforeconomists.com/page=44': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=44?\nError fetching page www.codingforeconomists.com/page=45: Invalid URL 'www.codingforeconomists.com/page=45': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=45?\nError fetching page www.codingforeconomists.com/page=46: Invalid URL 'www.codingforeconomists.com/page=46': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=46?\nError fetching page www.codingforeconomists.com/page=47: Invalid URL 'www.codingforeconomists.com/page=47': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=47?\nError fetching page www.codingforeconomists.com/page=48: Invalid URL 'www.codingforeconomists.com/page=48': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=48?\nError fetching page www.codingforeconomists.com/page=49: Invalid URL 'www.codingforeconomists.com/page=49': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=49?\n[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n\n\n\ndf_list = pd.read_html(\n    \"https://simple.wikipedia.org/wiki/FIFA_World_Cup\", match=\"Sweden\"\n)\n# Retrieve first and only entry from list of dataframes\ndf = df_list[0]\ndf.head()\n\n\n\n\n\n\n\n\nYears\nHosts\nWinners\nScore\nRunner's-up\nThird place\nScore.1\nFourth place\n\n\n\n\n0\n1930 Details\nUruguay\nUruguay\n4 - 2\nArgentina\nUnited States\n[note 1]\nYugoslavia\n\n\n1\n1934 Details\nItaly\nItaly\n2 - 1\nCzechoslovakia\nGermany\n3 - 2\nAustria\n\n\n2\n1938 Details\nFrance\nItaly\n4 - 2\nHungary\nBrazil\n4 - 2\nSweden\n\n\n3\n1950 Details\nBrazil\nUruguay\n2 - 1\nBrazil\nSweden\n[note 2]\nSpain\n\n\n4\n1954 Details\nSwitzerland\nWest Germany\n3 - 2\nHungary\nAustria\n3 - 1\nUruguay\n\n\n\n\n\n\n\npdf part\n\nimport PyPDF2\nfrom pathlib import Path\n\ndef read_pdf_text(file_path):\n    text = \"\"\n    with open(file_path, 'rb') as file:\n        reader = PyPDF2.PdfReader(file)\n        for page in reader.pages:\n            text += page.extract_text()\n    return text\n\npdf_file_path = Path(\"D:\\360MoveData\\Users\\75903\\Desktop\\SuYibo-Homework\\data\\pdf_with_table.pdf\")\npdf_text = read_pdf_text(pdf_file_path)\nprint(pdf_text[:220])\n\n3 \n 2 Quantifying Fuel -Saving Opportunit ies from Specific Driving \nBehavior Changes  \n2.1 Savings from Improving Individual Driving  Profiles  \n2.1.1  Drive Profile Subsample from Real -World Travel Survey  \nThe interi\n\n\n\nimport tabula\nimport os\n\n# 设置 PDF 文件路径\npdf_path = os.path.join('data', 'pdf_with_table.pdf')\n\n# 使用 tabula 读取 PDF 中的表格数据\ntables = tabula.read_pdf(pdf_path, pages='all')\n\n# 打印第一个表格的数据（你可以根据实际需求修改处理方式）\nif len(tables) &gt; 0:\n    print(tables[0])\n\n  Unnamed: 0 Unnamed: 1 Unnamed: 2 Unnamed: 3 Percent Fuel Savings Unnamed: 4\n0      Cycle         KI   Distance        NaN                  NaN        NaN\n1       Name     (1/km)       (mi)   Improved  Decreased Eliminate  Decreased\n2        NaN        NaN        NaN      Speed          Accel Stops       Idle\n3     2012_2       3.30        1.3       5.9%           9.5% 29.2%      17.4%\n4     2145_1       0.68       11.2       2.4%            0.1% 9.5%       2.7%\n5     4234_1       0.59       58.7       8.5%            1.3% 8.5%       3.3%\n6     2032_2       0.17       57.8      21.7%            0.3% 2.7%       1.2%\n7     4171_1       0.07      173.9      58.1%            1.6% 2.1%       0.5%\n\n\nSource: 替换为你的 FRED API key\n\n\nPractice3-Practice3.2\n\n2\n\nfrom bs4 import BeautifulSoup\nimport requests\nimport pandas as pd\nfrom time import sleep\n# Downloading imdb top 250 movie's data\nurl = 'http://www.imdb.com/chart/top'\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.text, \"html.parser\")\nmovies = soup.select('li.ipc-metadata-list-summary-item')\n\n\ncrew = [a.attrs.get('title') for a in soup.select('td.titleColumn a')]\nratings = [b.attrs.get('data-value')\n        for b in soup.select('td.posterColumn span[name=ir]')]\n\n\n\n# movie information\nlist = []\n \n# Iterating over movies to extract\n# each movie's details\nfor index in range(0, len(movies)):\n     \n    # Separating movie into: 'place',\n    # 'title', 'year'\n    movie_string = movies[index].get_text()\n    movie = (' '.join(movie_string.split()).replace('.', ''))\n    movie_title = movie[len(str(index))+1:-7]\n    year = re.search('\\((.*?)\\)', movie_string).group(1)\n    place = movie[:len(str(index))-(len(movie))]\n    data = {\"place\": place,\n            \"movie_title\": movie_title,\n            \"rating\": ratings[index],\n            \"year\": year,\n            \"star_cast\": crew[index],\n            }\n    list.append(data)\n\n\nfor movie in list:\n    print(movie['place'], '-', movie['movie_title'], '('+movie['year'] +\n          ') -', 'Starring:', movie['star_cast'], movie['rating'])\n\n\n#saving the list as dataframe\n#then converting into .csv file\ndf = pd.DataFrame(list)\ndf.to_csv('D:\\360MoveData\\Users\\75903\\Desktop\\SuYibo-Homework\\data\\imdb_top_250_movies.csv',index=False)\n\nSource: Downloading imdb top 250 movie's data\n\n\nPractice3-Practice3.3\n\n\n\n\n\nimport requests\n \n# 定义请求的 URL 和 headers\nurl = \"https://movie.douban.com/top250\"\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n}\n \n# 发送 GET 请求\nresponse = requests.get(url, headers=headers)\nresponse.encoding = 'utf-8'  # 设置编码方式\nhtml_content = response.text  # 获取网页的 HTML 内容\nprint(\"网页内容加载成功！\")\n\n网页内容加载成功！\n\n\n\nfrom bs4 import BeautifulSoup\n \n# 使用 Beautiful Soup 解析 HTML\nsoup = BeautifulSoup(html_content, 'html.parser')\n \n# 提取电影名称、描述、评分和评价人数\nmovies = []\nfor item in soup.find_all('div', class_='item'):\n    title = item.find('span', class_='title').get_text()  # 电影名称\n    description = item.find('span', class_='inq')  # 电影描述\n    rating = item.find('span', class_='rating_num').get_text()  # 评分\n    votes = item.find('div', class_='star').find_all('span')[3].get_text()  # 评价人数\n    \n    # 如果没有描述，将其置为空字符串\n    if description:\n        description = description.get_text()\n    else:\n        description = ''\n    \n    movie = {\n        \"title\": title,\n        \"description\": description,\n        \"rating\": rating,\n        \"votes\": votes.replace('人评价', '').strip()\n    }\n    movies.append(movie)\n \nprint(\"数据提取成功！\")\n\n数据提取成功！\n\n\n\nimport csv\n \n# 将数据保存到 CSV 文件\nwith open('D:\\360MoveData\\Users\\75903\\Desktop\\SuYibo-Homework\\data\\douban_top250.csv', 'w', newline='', encoding='utf-8') as csvfile:\n    fieldnames = ['title', 'description', 'rating', 'votes']\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n \n    writer.writeheader()  # 写入表头\n    for movie in movies:\n        writer.writerow(movie)  # 写入每一行数据\n \nprint(\"数据已成功保存到 D:\\360MoveData\\Users\\75903\\Desktop\\SuYibo-Homework\\data\\douban_top250.csv\")\n\n数据已成功保存到 data\\douban_top250.csv\n\n\n\nimport pandas as pd\n\ndf = pd.read_csv('D:\\360MoveData\\Users\\75903\\Desktop\\SuYibo-Homework\\data\\douban_top250.csv')\n# show\nprint(df)\n\n        title                      description  rating    votes\n0      肖申克的救赎                          希望让人自由。     9.7  3086743\n1        霸王别姬                            风华绝代。     9.6  2278421\n2        阿甘正传                        一部美国近现代史。     9.5  2297780\n3       泰坦尼克号                       失去的才是永恒的。      9.5  2338435\n4        千与千寻                  最好的宫崎骏，最好的久石让。      9.4  2386813\n5        美丽人生                           最美的谎言。     9.5  1404250\n6     这个杀手不太冷                  怪蜀黍和小萝莉不得不说的故事。     9.4  2434726\n7        星际穿越            爱是一种力量，让我们超越时空感知它的存在。     9.4  2010081\n8        盗梦空间                  诺兰给了我们一场无法盗取的梦。     9.4  2197475\n9       楚门的世界            如果再也不能见到你，祝你早安，午安，晚安。     9.4  1868113\n10     辛德勒的名单                  拯救一个人，就是拯救整个世界。     9.5  1187176\n11    忠犬八公的故事                    永远都不能忘记你所爱的人。     9.4  1469596\n12      海上钢琴师        每个人都要走一条自己坚定了的路，就算是粉身碎骨。      9.3  1780067\n13    三傻大闹宝莱坞                   英俊版憨豆，高情商版谢耳朵。     9.2  1967345\n14     放牛班的春天              天籁一般的童声，是最接近上帝的存在。      9.3  1395330\n15     机器人总动员                         小瓦力，大人生。     9.3  1400496\n16      疯狂动物城  迪士尼给我们营造的乌托邦就是这样，永远善良勇敢，永远出乎意料。     9.2  2096015\n17        无间道                   香港电影史上永不过时的杰作。     9.3  1467474\n18       控方证人                       比利·怀德满分作品。     9.6   640164\n19  大话西游之大圣娶亲                            一生所爱。     9.2  1621517\n20         熔炉     我们一路奋战不是为了改变世界，而是为了不让世界改变我们。     9.3   986933\n21         教父            千万不要记恨你的对手，这样会让你失去理智。     9.3  1037883\n22       触不可及                       满满温情的高雅喜剧。     9.3  1208935\n23      寻梦环游记              死亡不是真的逝去，遗忘才是永恒的消亡。     9.1  1823049\n24     当幸福来敲门                          平民励志片。      9.2  1606700\n\n\nSource: 定义请求的 URL 和 headers\n\n\nPractice4-Practice4\n\n\nfrom bs4 import BeautifulSoup\nimport re  \nimport urllib.request, urllib.error  # certain URL\nimport xlwt  # excel operation\n \n \ndef main():\n    baseurl = \"https://movie.douban.com/top250?start=\"\n    datalist = getdata(baseurl)\n    savepath = \".\\\\douban_top250.csv\"\n    savedata(datalist, savepath)\n \n \n# compile返回的是匹配到的模式对象\nfindLink = re.compile(r'&lt;a href=\"(.*?)\"&gt;')  # detail\nfindImgSrc = re.compile(r'&lt;img.*src=\"(.*?)\"', re.S)  # re.S  message of picture\nfindTitle = re.compile(r'&lt;span class=\"title\"&gt;(.*)&lt;/span&gt;')  # name \nfindRating = re.compile(r'&lt;span class=\"rating_num\" property=\"v:average\"&gt;(.*)&lt;/span&gt;')  # score\nfindJudge = re.compile(r'&lt;span&gt;(\\d*)人评价&lt;/span&gt;')  # number\nfindInq = re.compile(r'&lt;span class=\"inq\"&gt;(.*)&lt;/span&gt;')  # about\nfindBd = re.compile(r'&lt;p class=\"\"&gt;(.*?)&lt;/p&gt;', re.S)  # actor..\n \n \n##获取网页数据\ndef getdata(baseurl):\n    datalist = []\n    for i in range(0, 10):\n        url = baseurl + str(i * 25)  ##move on next page\n        html = geturl(url)\n        soup = BeautifulSoup(html, \"html.parser\")  #  BeautifulSoup soup，html\n        for item in soup.find_all(\"div\", class_='item'):  ##find_all \n            data = []  # save HTML \n            item = str(item)  ##trans\n            link = re.findall(findLink, item)[0]  \n            data.append(link)\n \n            imgSrc = re.findall(findImgSrc, item)[0]\n            data.append(imgSrc)\n \n            titles = re.findall(findTitle, item)  ##en zh transla\n            if (len(titles) == 2):\n                onetitle = titles[0]\n                data.append(onetitle)\n                twotitle = titles[1].replace(\"/\", \"\")  # can\n                data.append(twotitle)\n            else:\n                data.append(titles)\n                data.append(\" \")  ##value\n \n            rating = re.findall(findRating, item)[0]  # add score\n            data.append(rating)\n \n            judgeNum = re.findall(findJudge, item)[0]  # add number\n            data.append(judgeNum)\n \n            inq = re.findall(findInq, item)  # add abut\n            if len(inq) != 0:\n                inq = inq[0].replace(\"。\", \"\")\n                data.append(inq)\n            else:\n                data.append(\" \")\n \n            bd = re.findall(findBd, item)[0]\n            bd = re.sub('&lt;br(\\s+)?/&gt;(\\s+)?', \" \", bd)\n            bd = re.sub('/', \" \", bd)\n            data.append(bd.strip())  # cancel\n            datalist.append(data)\n    return datalist\n \n \n##保存数据\ndef savedata(datalist, savepath):\n    workbook = xlwt.Workbook(encoding=\"utf-8\", style_compression=0)  ##style_compression=0\n    worksheet = workbook.add_sheet(\"douban_top250\", cell_overwrite_ok=True)  # cell_overwrite_ok=True\n    column = (\"电影详情链接\", \"图片链接\", \"影片中文名\", \"影片外国名\", \"评分\", \"评价数\", \"概况\", \"相关信息\")  ##execl\n    for i in range(0, 8):\n        worksheet.write(0, i, column[i])  # 将column[i] save [0]\n    for i in range(0, 250):\n        data = datalist[i]\n        for j in range(0, 8):\n            worksheet.write(i + 1, j, data[j])\n    workbook.save(savepath)\n \n \n##爬取网页\ndef geturl(url):\n    head = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n                      \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36\"\n    }\n    req = urllib.request.Request(url, headers=head)\n    try:  ## check error\n        response = urllib.request.urlopen(req)\n        html = response.read().decode(\"utf-8\")\n    except urllib.error.URLError as e:\n        if hasattr(e, \"code\"):  \n            print(e.code)\n        if hasattr(e, \"reason\"):\n            print(e.reason)\n    return html\n \n \nif __name__ == '__main__':\n    main()\n    print(\"爬取成功！！！\")\n\n爬取成功！！！\n\n\n\nfrom bs4 import BeautifulSoup\nimport re\nimport urllib.request, urllib.error  # for URL requests\nimport csv  # for saving as CSV\n\n\ndef main():\n    baseurl = \"https://movie.douban.com/top250?start=\"\n    datalist = getdata(baseurl)\n    savepath = \"./douban_top250.csv\"\n    savedata(datalist, savepath)\n\n\n# Regular expressions to extract information\nfindLink = re.compile(r'&lt;a href=\"(.*?)\"&gt;')  # detail link\nfindImgSrc = re.compile(r'&lt;img.*src=\"(.*?)\"', re.S)  # image link\nfindTitle = re.compile(r'&lt;span class=\"title\"&gt;(.*)&lt;/span&gt;')  # movie title\nfindRating = re.compile(r'&lt;span class=\"rating_num\" property=\"v:average\"&gt;(.*)&lt;/span&gt;')  # rating\nfindJudge = re.compile(r'&lt;span&gt;(\\d*)人评价&lt;/span&gt;')  # number of reviews\nfindInq = re.compile(r'&lt;span class=\"inq\"&gt;(.*)&lt;/span&gt;')  # summary\nfindBd = re.compile(r'&lt;p class=\"\"&gt;(.*?)&lt;/p&gt;', re.S)  # additional info\n\n\n# Function to get data from the website\ndef getdata(baseurl):\n    datalist = []\n    for i in range(0, 10):\n        url = baseurl + str(i * 25)  # Go to the next page\n        html = geturl(url)\n        soup = BeautifulSoup(html, \"html.parser\")\n        for item in soup.find_all(\"div\", class_='item'):  # Extract movie items\n            data = []  # Save movie data\n            item = str(item)  # Convert to string for regex\n            link = re.findall(findLink, item)[0]  # Detail link\n            data.append(link)\n\n            imgSrc = re.findall(findImgSrc, item)[0]  # Image link\n            data.append(imgSrc)\n\n            titles = re.findall(findTitle, item)  # Titles (CN and foreign)\n            if len(titles) == 2:\n                data.append(titles[0])  # Chinese title\n                data.append(titles[1].replace(\"/\", \"\").strip())  # Foreign title\n            else:\n                data.append(titles[0])  # Only Chinese title\n                data.append(\" \")  # Empty for foreign title\n\n            rating = re.findall(findRating, item)[0]  # Rating\n            data.append(rating)\n\n            judgeNum = re.findall(findJudge, item)[0]  # Number of reviews\n            data.append(judgeNum)\n\n            inq = re.findall(findInq, item)  # Summary\n            if len(inq) != 0:\n                data.append(inq[0].replace(\"。\", \"\"))\n            else:\n                data.append(\" \")\n\n            bd = re.findall(findBd, item)[0]  # Additional info\n            bd = re.sub('&lt;br(\\s+)?/&gt;(\\s+)?', \" \", bd)  # Replace line breaks\n            bd = re.sub('/', \" \", bd)  # Replace slashes\n            data.append(bd.strip())\n\n            datalist.append(data)\n    return datalist\n\n\n# Function to save data to a CSV file\ndef savedata(datalist, savepath):\n    headers = [\"电影详情链接\", \"图片链接\", \"影片中文名\", \"影片外国名\", \"评分\", \"评价数\", \"概况\", \"相关信息\"]\n    with open(savepath, mode='w', encoding='utf-8', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(headers)  # Write headers\n        for data in datalist:\n            writer.writerow(data)  # Write each movie's data\n\n\n# Function to get HTML content from a URL\ndef geturl(url):\n    head = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n                      \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36\"\n    }\n    req = urllib.request.Request(url, headers=head)\n    try:\n        response = urllib.request.urlopen(req)\n        html = response.read().decode(\"utf-8\")\n    except urllib.error.URLError as e:\n        if hasattr(e, \"code\"):\n            print(e.code)\n        if hasattr(e, \"reason\"):\n            print(e.reason)\n        return \"\"\n    return html\n\n\nif __name__ == '__main__':\n    main()\n    print(\"爬取成功并保存为CSV文件！\")\n\n爬取成功并保存为CSV文件！\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load datasets\ndouban_file_path = 'douban_top250.csv'  \nimdb_file_path = 'IMDB_Top250.csv'      \n\ndouban_data = pd.read_csv(douban_file_path, encoding='utf-8', on_bad_lines='skip')\nimdb_data = pd.read_csv(imdb_file_path, encoding='utf-8', on_bad_lines='skip')\n\n# Renaming columns for clarity and merging compatibility\ndouban_data.rename(columns={\n    '影片中文名': 'Title',\n    '评分': 'Douban_Score',\n    '评价数': 'Douban_Reviews',\n    '相关信息': 'Douban_Info'\n}, inplace=True)\n\nimdb_data.rename(columns={\n    'Name': 'Title',\n    'Year': 'Release_Year',\n    'IMDB Ranking': 'IMDB_Score',\n    'Genre': 'IMDB_Genre',\n    'Director': 'IMDB_Director'\n}, inplace=True)\n\n# Calculate average scores for both platforms\ndouban_avg_score = douban_data['Douban_Score'].mean()\nimdb_avg_score = imdb_data['IMDB_Score'].mean()\n\n# Find overlapping movies by title\noverlap_movies = pd.merge(douban_data, imdb_data, on='Title')\n\n# Visualize average scores\nplt.figure(figsize=(8, 5))\nplt.bar(['Douban', 'IMDb'], [douban_avg_score, imdb_avg_score], alpha=0.7)\nplt.title('Average Scores: Douban vs IMDb')\nplt.ylabel('Average Score')\nplt.show()\n\n# Analyze release year distribution\nplt.figure(figsize=(10, 5))\ndouban_data['Douban_Info'] = douban_data['Douban_Info'].astype(str)\ndouban_years = douban_data['Douban_Info'].str.extract(r'(\\d{4})').dropna()\ndouban_years = douban_years[0].astype(int).value_counts().sort_index()\n\nimdb_years = imdb_data['Release_Year'].value_counts().sort_index()\n\ndouban_years.plot(kind='bar', alpha=0.7, label='Douban', figsize=(10, 5))\nimdb_years.plot(kind='bar', alpha=0.7, label='IMDb', color='orange')\nplt.title('Release Year Distribution')\nplt.xlabel('Year')\nplt.ylabel('Number of Movies')\nplt.legend()\nplt.show()\n\n# Analyze genre distribution\nimdb_genres = imdb_data['IMDB_Genre'].str.split(',').explode().str.strip().value_counts()\nplt.figure(figsize=(10, 5))\nimdb_genres.head(10).plot(kind='bar', alpha=0.7, color='orange')\nplt.title('Top 10 IMDb Genres')\nplt.xlabel('Genre')\nplt.ylabel('Count')\nplt.show()\n\n# Top directors by movie count\ndouban_directors = douban_data['Douban_Info'].str.extract(r'导演: (.+?) ').dropna()\ndouban_top_directors = douban_directors[0].value_counts().head(10)\n\nimdb_top_directors = imdb_data['IMDB_Director'].value_counts().head(10)\n\nplt.figure(figsize=(10, 5))\ndouban_top_directors.plot(kind='bar', alpha=0.7, label='Douban', color='blue')\nplt.title('Top 10 Douban Directors')\nplt.xlabel('Director')\nplt.ylabel('Movie Count')\nplt.show()\n\nplt.figure(figsize=(10, 5))\nimdb_top_directors.plot(kind='bar', alpha=0.7, label='IMDb', color='orange')\nplt.title('Top 10 IMDb Directors')\nplt.xlabel('Director')\nplt.ylabel('Movie Count')\nplt.show()\n\n# Save overlapping movies to a CSV file\noverlap_movies.to_csv('overlap_movies.csv', index=False)\n\n# Print results\nprint(f\"豆瓣平均评分: {douban_avg_score}\")\nprint(f\"IMDb平均评分: {imdb_avg_score}\")\nprint(f\"重叠电影数量: {len(overlap_movies)}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n豆瓣平均评分: 8.9396\nIMDb平均评分: 8.254\n重叠电影数量: 0\n\n\nSource: compile返回的是匹配到的模式对象\n\n\nPractice4-recreate_graph\n\n\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pathlib import Path\nimport pingouin as pg\nfrom lets_plot import *\n\nLetsPlot.setup_html(no_js=True)\n\n\n\n\nread_file = pd.read_excel (\"D:\\360MoveData\\Users\\75903\\Desktop\\SuYibo-Homework\\Practice2 SuYibo/Public-goods-experimental-data.xlsx\") \n\nread_file.to_csv (\"Public-goods-experimental-data.csv\", \n                index = None, \n                header=True) \n    \n\ndf = pd.DataFrame(pd.read_csv(\"Public-goods-experimental-data.csv\")) \n\n\ndf.head()\n\n\n\n\n\n\n\n\nContributions without punishment (Figure 3 in Herrmann, Thoni, and Gachter (2008))\nUnnamed: 1\nUnnamed: 2\nUnnamed: 3\nUnnamed: 4\nUnnamed: 5\nUnnamed: 6\nUnnamed: 7\nUnnamed: 8\nUnnamed: 9\nUnnamed: 10\nUnnamed: 11\nUnnamed: 12\nUnnamed: 13\nUnnamed: 14\nUnnamed: 15\nUnnamed: 16\n\n\n\n\n0\nPeriod\nCopenhagen\nDnipropetrovs’k\nMinsk\nSt. Gallen\nMuscat\nSamara\nZurich\nBoston\nBonn\nChengdu\nSeoul\nRiyadh\nNottingham\nAthens\nIstanbul\nMelbourne\n\n\n1\n1\n14.102941176470589\n10.954545272727273\n12.7941\n13.6875\n9.53846\n10.8421\n11.0833\n12.9643\n10.85\n10\n8.249999904761905\n7.958333416666666\n10.928571428571429\n8.136363818181819\n8.9375\n8.225\n\n\n2\n2\n14.132352941176471\n12.636363454545455\n12.3382\n12.80206675\n10.9808\n11.5\n12.1667\n12.7143\n10.8\n9.9625\n9.142857142857142\n7.729166916666666\n10.535714285714286\n6.2727272727272725\n9.015625\n7.325\n\n\n3\n3\n13.720588235294118\n12.068181636363636\n12.5882\n12.354183500000001\n11.5192\n11.7237\n10.7813\n12.7143\n11.516666666666666\n10.225\n9.892857238095237\n7.1875\n9.125000142857143\n6.409090636363637\n8.4375\n6.25\n\n\n4\n4\n12.897058823529411\n11.181818\n12.2647\n10.60418325\n10.3077\n11.3092\n10.6354\n11.3571\n10.7\n10.0125\n9.666666666666666\n7.875000166666666\n8.982142857142858\n6.3181815454545465\n6.921875125\n5.975\n\n\n\n\n\n\n\nSource: recreate_graph.ipynb"
  },
  {
    "objectID": "labs/Practice/recreate_graph.html",
    "href": "labs/Practice/recreate_graph.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pathlib import Path\nimport pingouin as pg\nfrom lets_plot import *\n\nLetsPlot.setup_html(no_js=True)\n\n\n\n\nread_file = pd.read_excel (\"D:\\360MoveData\\Users\\75903\\Desktop\\SuYibo-Homework\\Practice2 SuYibo/Public-goods-experimental-data.xlsx\") \n\nread_file.to_csv (\"Public-goods-experimental-data.csv\", \n                index = None, \n                header=True) \n    \n\ndf = pd.DataFrame(pd.read_csv(\"Public-goods-experimental-data.csv\")) \n\n\ndf.head()\n\n\n\n\n\n\n\n\nContributions without punishment (Figure 3 in Herrmann, Thoni, and Gachter (2008))\nUnnamed: 1\nUnnamed: 2\nUnnamed: 3\nUnnamed: 4\nUnnamed: 5\nUnnamed: 6\nUnnamed: 7\nUnnamed: 8\nUnnamed: 9\nUnnamed: 10\nUnnamed: 11\nUnnamed: 12\nUnnamed: 13\nUnnamed: 14\nUnnamed: 15\nUnnamed: 16\n\n\n\n\n0\nPeriod\nCopenhagen\nDnipropetrovs’k\nMinsk\nSt. Gallen\nMuscat\nSamara\nZurich\nBoston\nBonn\nChengdu\nSeoul\nRiyadh\nNottingham\nAthens\nIstanbul\nMelbourne\n\n\n1\n1\n14.102941176470589\n10.954545272727273\n12.7941\n13.6875\n9.53846\n10.8421\n11.0833\n12.9643\n10.85\n10\n8.249999904761905\n7.958333416666666\n10.928571428571429\n8.136363818181819\n8.9375\n8.225\n\n\n2\n2\n14.132352941176471\n12.636363454545455\n12.3382\n12.80206675\n10.9808\n11.5\n12.1667\n12.7143\n10.8\n9.9625\n9.142857142857142\n7.729166916666666\n10.535714285714286\n6.2727272727272725\n9.015625\n7.325\n\n\n3\n3\n13.720588235294118\n12.068181636363636\n12.5882\n12.354183500000001\n11.5192\n11.7237\n10.7813\n12.7143\n11.516666666666666\n10.225\n9.892857238095237\n7.1875\n9.125000142857143\n6.409090636363637\n8.4375\n6.25\n\n\n4\n4\n12.897058823529411\n11.181818\n12.2647\n10.60418325\n10.3077\n11.3092\n10.6354\n11.3571\n10.7\n10.0125\n9.666666666666666\n7.875000166666666\n8.982142857142858\n6.3181815454545465\n6.921875125\n5.975"
  },
  {
    "objectID": "labs/Practice/Practice4.html",
    "href": "labs/Practice/Practice4.html",
    "title": "",
    "section": "",
    "text": "from bs4 import BeautifulSoup\nimport re  \nimport urllib.request, urllib.error  # certain URL\nimport xlwt  # excel operation\n \n \ndef main():\n    baseurl = \"https://movie.douban.com/top250?start=\"\n    datalist = getdata(baseurl)\n    savepath = \".\\\\douban_top250.csv\"\n    savedata(datalist, savepath)\n \n \n# compile返回的是匹配到的模式对象\nfindLink = re.compile(r'&lt;a href=\"(.*?)\"&gt;')  # detail\nfindImgSrc = re.compile(r'&lt;img.*src=\"(.*?)\"', re.S)  # re.S  message of picture\nfindTitle = re.compile(r'&lt;span class=\"title\"&gt;(.*)&lt;/span&gt;')  # name \nfindRating = re.compile(r'&lt;span class=\"rating_num\" property=\"v:average\"&gt;(.*)&lt;/span&gt;')  # score\nfindJudge = re.compile(r'&lt;span&gt;(\\d*)人评价&lt;/span&gt;')  # number\nfindInq = re.compile(r'&lt;span class=\"inq\"&gt;(.*)&lt;/span&gt;')  # about\nfindBd = re.compile(r'&lt;p class=\"\"&gt;(.*?)&lt;/p&gt;', re.S)  # actor..\n \n \n##获取网页数据\ndef getdata(baseurl):\n    datalist = []\n    for i in range(0, 10):\n        url = baseurl + str(i * 25)  ##move on next page\n        html = geturl(url)\n        soup = BeautifulSoup(html, \"html.parser\")  #  BeautifulSoup soup，html\n        for item in soup.find_all(\"div\", class_='item'):  ##find_all \n            data = []  # save HTML \n            item = str(item)  ##trans\n            link = re.findall(findLink, item)[0]  \n            data.append(link)\n \n            imgSrc = re.findall(findImgSrc, item)[0]\n            data.append(imgSrc)\n \n            titles = re.findall(findTitle, item)  ##en zh transla\n            if (len(titles) == 2):\n                onetitle = titles[0]\n                data.append(onetitle)\n                twotitle = titles[1].replace(\"/\", \"\")  # can\n                data.append(twotitle)\n            else:\n                data.append(titles)\n                data.append(\" \")  ##value\n \n            rating = re.findall(findRating, item)[0]  # add score\n            data.append(rating)\n \n            judgeNum = re.findall(findJudge, item)[0]  # add number\n            data.append(judgeNum)\n \n            inq = re.findall(findInq, item)  # add abut\n            if len(inq) != 0:\n                inq = inq[0].replace(\"。\", \"\")\n                data.append(inq)\n            else:\n                data.append(\" \")\n \n            bd = re.findall(findBd, item)[0]\n            bd = re.sub('&lt;br(\\s+)?/&gt;(\\s+)?', \" \", bd)\n            bd = re.sub('/', \" \", bd)\n            data.append(bd.strip())  # cancel\n            datalist.append(data)\n    return datalist\n \n \n##保存数据\ndef savedata(datalist, savepath):\n    workbook = xlwt.Workbook(encoding=\"utf-8\", style_compression=0)  ##style_compression=0\n    worksheet = workbook.add_sheet(\"douban_top250\", cell_overwrite_ok=True)  # cell_overwrite_ok=True\n    column = (\"电影详情链接\", \"图片链接\", \"影片中文名\", \"影片外国名\", \"评分\", \"评价数\", \"概况\", \"相关信息\")  ##execl\n    for i in range(0, 8):\n        worksheet.write(0, i, column[i])  # 将column[i] save [0]\n    for i in range(0, 250):\n        data = datalist[i]\n        for j in range(0, 8):\n            worksheet.write(i + 1, j, data[j])\n    workbook.save(savepath)\n \n \n##爬取网页\ndef geturl(url):\n    head = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n                      \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36\"\n    }\n    req = urllib.request.Request(url, headers=head)\n    try:  ## check error\n        response = urllib.request.urlopen(req)\n        html = response.read().decode(\"utf-8\")\n    except urllib.error.URLError as e:\n        if hasattr(e, \"code\"):  \n            print(e.code)\n        if hasattr(e, \"reason\"):\n            print(e.reason)\n    return html\n \n \nif __name__ == '__main__':\n    main()\n    print(\"爬取成功！！！\")\n\n爬取成功！！！\n\n\n\nfrom bs4 import BeautifulSoup\nimport re\nimport urllib.request, urllib.error  # for URL requests\nimport csv  # for saving as CSV\n\n\ndef main():\n    baseurl = \"https://movie.douban.com/top250?start=\"\n    datalist = getdata(baseurl)\n    savepath = \"./douban_top250.csv\"\n    savedata(datalist, savepath)\n\n\n# Regular expressions to extract information\nfindLink = re.compile(r'&lt;a href=\"(.*?)\"&gt;')  # detail link\nfindImgSrc = re.compile(r'&lt;img.*src=\"(.*?)\"', re.S)  # image link\nfindTitle = re.compile(r'&lt;span class=\"title\"&gt;(.*)&lt;/span&gt;')  # movie title\nfindRating = re.compile(r'&lt;span class=\"rating_num\" property=\"v:average\"&gt;(.*)&lt;/span&gt;')  # rating\nfindJudge = re.compile(r'&lt;span&gt;(\\d*)人评价&lt;/span&gt;')  # number of reviews\nfindInq = re.compile(r'&lt;span class=\"inq\"&gt;(.*)&lt;/span&gt;')  # summary\nfindBd = re.compile(r'&lt;p class=\"\"&gt;(.*?)&lt;/p&gt;', re.S)  # additional info\n\n\n# Function to get data from the website\ndef getdata(baseurl):\n    datalist = []\n    for i in range(0, 10):\n        url = baseurl + str(i * 25)  # Go to the next page\n        html = geturl(url)\n        soup = BeautifulSoup(html, \"html.parser\")\n        for item in soup.find_all(\"div\", class_='item'):  # Extract movie items\n            data = []  # Save movie data\n            item = str(item)  # Convert to string for regex\n            link = re.findall(findLink, item)[0]  # Detail link\n            data.append(link)\n\n            imgSrc = re.findall(findImgSrc, item)[0]  # Image link\n            data.append(imgSrc)\n\n            titles = re.findall(findTitle, item)  # Titles (CN and foreign)\n            if len(titles) == 2:\n                data.append(titles[0])  # Chinese title\n                data.append(titles[1].replace(\"/\", \"\").strip())  # Foreign title\n            else:\n                data.append(titles[0])  # Only Chinese title\n                data.append(\" \")  # Empty for foreign title\n\n            rating = re.findall(findRating, item)[0]  # Rating\n            data.append(rating)\n\n            judgeNum = re.findall(findJudge, item)[0]  # Number of reviews\n            data.append(judgeNum)\n\n            inq = re.findall(findInq, item)  # Summary\n            if len(inq) != 0:\n                data.append(inq[0].replace(\"。\", \"\"))\n            else:\n                data.append(\" \")\n\n            bd = re.findall(findBd, item)[0]  # Additional info\n            bd = re.sub('&lt;br(\\s+)?/&gt;(\\s+)?', \" \", bd)  # Replace line breaks\n            bd = re.sub('/', \" \", bd)  # Replace slashes\n            data.append(bd.strip())\n\n            datalist.append(data)\n    return datalist\n\n\n# Function to save data to a CSV file\ndef savedata(datalist, savepath):\n    headers = [\"电影详情链接\", \"图片链接\", \"影片中文名\", \"影片外国名\", \"评分\", \"评价数\", \"概况\", \"相关信息\"]\n    with open(savepath, mode='w', encoding='utf-8', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(headers)  # Write headers\n        for data in datalist:\n            writer.writerow(data)  # Write each movie's data\n\n\n# Function to get HTML content from a URL\ndef geturl(url):\n    head = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n                      \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36\"\n    }\n    req = urllib.request.Request(url, headers=head)\n    try:\n        response = urllib.request.urlopen(req)\n        html = response.read().decode(\"utf-8\")\n    except urllib.error.URLError as e:\n        if hasattr(e, \"code\"):\n            print(e.code)\n        if hasattr(e, \"reason\"):\n            print(e.reason)\n        return \"\"\n    return html\n\n\nif __name__ == '__main__':\n    main()\n    print(\"爬取成功并保存为CSV文件！\")\n\n爬取成功并保存为CSV文件！\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load datasets\ndouban_file_path = 'douban_top250.csv'  \nimdb_file_path = 'IMDB_Top250.csv'      \n\ndouban_data = pd.read_csv(douban_file_path, encoding='utf-8', on_bad_lines='skip')\nimdb_data = pd.read_csv(imdb_file_path, encoding='utf-8', on_bad_lines='skip')\n\n# Renaming columns for clarity and merging compatibility\ndouban_data.rename(columns={\n    '影片中文名': 'Title',\n    '评分': 'Douban_Score',\n    '评价数': 'Douban_Reviews',\n    '相关信息': 'Douban_Info'\n}, inplace=True)\n\nimdb_data.rename(columns={\n    'Name': 'Title',\n    'Year': 'Release_Year',\n    'IMDB Ranking': 'IMDB_Score',\n    'Genre': 'IMDB_Genre',\n    'Director': 'IMDB_Director'\n}, inplace=True)\n\n# Calculate average scores for both platforms\ndouban_avg_score = douban_data['Douban_Score'].mean()\nimdb_avg_score = imdb_data['IMDB_Score'].mean()\n\n# Find overlapping movies by title\noverlap_movies = pd.merge(douban_data, imdb_data, on='Title')\n\n# Visualize average scores\nplt.figure(figsize=(8, 5))\nplt.bar(['Douban', 'IMDb'], [douban_avg_score, imdb_avg_score], alpha=0.7)\nplt.title('Average Scores: Douban vs IMDb')\nplt.ylabel('Average Score')\nplt.show()\n\n# Analyze release year distribution\nplt.figure(figsize=(10, 5))\ndouban_data['Douban_Info'] = douban_data['Douban_Info'].astype(str)\ndouban_years = douban_data['Douban_Info'].str.extract(r'(\\d{4})').dropna()\ndouban_years = douban_years[0].astype(int).value_counts().sort_index()\n\nimdb_years = imdb_data['Release_Year'].value_counts().sort_index()\n\ndouban_years.plot(kind='bar', alpha=0.7, label='Douban', figsize=(10, 5))\nimdb_years.plot(kind='bar', alpha=0.7, label='IMDb', color='orange')\nplt.title('Release Year Distribution')\nplt.xlabel('Year')\nplt.ylabel('Number of Movies')\nplt.legend()\nplt.show()\n\n# Analyze genre distribution\nimdb_genres = imdb_data['IMDB_Genre'].str.split(',').explode().str.strip().value_counts()\nplt.figure(figsize=(10, 5))\nimdb_genres.head(10).plot(kind='bar', alpha=0.7, color='orange')\nplt.title('Top 10 IMDb Genres')\nplt.xlabel('Genre')\nplt.ylabel('Count')\nplt.show()\n\n# Top directors by movie count\ndouban_directors = douban_data['Douban_Info'].str.extract(r'导演: (.+?) ').dropna()\ndouban_top_directors = douban_directors[0].value_counts().head(10)\n\nimdb_top_directors = imdb_data['IMDB_Director'].value_counts().head(10)\n\nplt.figure(figsize=(10, 5))\ndouban_top_directors.plot(kind='bar', alpha=0.7, label='Douban', color='blue')\nplt.title('Top 10 Douban Directors')\nplt.xlabel('Director')\nplt.ylabel('Movie Count')\nplt.show()\n\nplt.figure(figsize=(10, 5))\nimdb_top_directors.plot(kind='bar', alpha=0.7, label='IMDb', color='orange')\nplt.title('Top 10 IMDb Directors')\nplt.xlabel('Director')\nplt.ylabel('Movie Count')\nplt.show()\n\n# Save overlapping movies to a CSV file\noverlap_movies.to_csv('overlap_movies.csv', index=False)\n\n# Print results\nprint(f\"豆瓣平均评分: {douban_avg_score}\")\nprint(f\"IMDb平均评分: {imdb_avg_score}\")\nprint(f\"重叠电影数量: {len(overlap_movies)}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n豆瓣平均评分: 8.9396\nIMDb平均评分: 8.254\n重叠电影数量: 0"
  },
  {
    "objectID": "labs/Practice/Practice3.2.html",
    "href": "labs/Practice/Practice3.2.html",
    "title": "",
    "section": "",
    "text": "2\n\nfrom bs4 import BeautifulSoup\nimport requests\nimport pandas as pd\nfrom time import sleep\n# Downloading imdb top 250 movie's data\nurl = 'http://www.imdb.com/chart/top'\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.text, \"html.parser\")\nmovies = soup.select('li.ipc-metadata-list-summary-item')\n\n\ncrew = [a.attrs.get('title') for a in soup.select('td.titleColumn a')]\nratings = [b.attrs.get('data-value')\n        for b in soup.select('td.posterColumn span[name=ir]')]\n\n\n\n# movie information\nlist = []\n \n# Iterating over movies to extract\n# each movie's details\nfor index in range(0, len(movies)):\n     \n    # Separating movie into: 'place',\n    # 'title', 'year'\n    movie_string = movies[index].get_text()\n    movie = (' '.join(movie_string.split()).replace('.', ''))\n    movie_title = movie[len(str(index))+1:-7]\n    year = re.search('\\((.*?)\\)', movie_string).group(1)\n    place = movie[:len(str(index))-(len(movie))]\n    data = {\"place\": place,\n            \"movie_title\": movie_title,\n            \"rating\": ratings[index],\n            \"year\": year,\n            \"star_cast\": crew[index],\n            }\n    list.append(data)\n\n\nfor movie in list:\n    print(movie['place'], '-', movie['movie_title'], '('+movie['year'] +\n          ') -', 'Starring:', movie['star_cast'], movie['rating'])\n\n\n#saving the list as dataframe\n#then converting into .csv file\ndf = pd.DataFrame(list)\ndf.to_csv('D:\\360MoveData\\Users\\75903\\Desktop\\SuYibo-Homework\\data\\imdb_top_250_movies.csv',index=False)"
  },
  {
    "objectID": "labs/Practice/Empirical.html",
    "href": "labs/Practice/Empirical.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pathlib import Path\nimport pingouin as pg\nfrom lets_plot import *\n\nLetsPlot.setup_html(no_js=True)\n\n\n### You don't need to use these settings yourself\n### — they are just here to make the book look nicer!\n# Set the plot style for prettier charts:\n#plt.style.use(\n#    \"https://raw.githubusercontent.com/aeturrell/core_python/main/plot_style.txt\"\n#)\n\n\n#Python Walkthrough 1.1\ndf = pd.read_csv(\n    \"https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.csv\",\n    skiprows=1,\n    na_values=\"***\",\n)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nYear\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nJ-D\nD-N\nDJF\nMAM\nJJA\nSON\n\n\n\n\n0\n1880\n-0.39\n-0.54\n-0.24\n-0.31\n-0.05\n-0.18\n-0.23\n-0.27\n-0.26\n-0.31\n-0.46\n-0.43\n-0.31\nNaN\nNaN\n-0.20\n-0.23\n-0.34\n\n\n1\n1881\n-0.31\n-0.26\n-0.07\n-0.03\n0.03\n-0.34\n0.08\n-0.06\n-0.29\n-0.45\n-0.38\n-0.24\n-0.19\n-0.21\n-0.33\n-0.02\n-0.11\n-0.37\n\n\n2\n1882\n0.25\n0.20\n0.01\n-0.31\n-0.24\n-0.29\n-0.28\n-0.17\n-0.26\n-0.53\n-0.34\n-0.69\n-0.22\n-0.18\n0.07\n-0.18\n-0.25\n-0.38\n\n\n3\n1883\n-0.58\n-0.66\n-0.15\n-0.30\n-0.26\n-0.11\n-0.06\n-0.23\n-0.34\n-0.16\n-0.45\n-0.15\n-0.29\n-0.33\n-0.64\n-0.24\n-0.13\n-0.32\n\n\n4\n1884\n-0.17\n-0.11\n-0.64\n-0.59\n-0.36\n-0.41\n-0.41\n-0.51\n-0.45\n-0.45\n-0.58\n-0.47\n-0.43\n-0.40\n-0.14\n-0.53\n-0.45\n-0.49\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 145 entries, 0 to 144\nData columns (total 19 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   Year    145 non-null    int64  \n 1   Jan     145 non-null    float64\n 2   Feb     145 non-null    float64\n 3   Mar     145 non-null    float64\n 4   Apr     145 non-null    float64\n 5   May     145 non-null    float64\n 6   Jun     145 non-null    float64\n 7   Jul     145 non-null    float64\n 8   Aug     145 non-null    float64\n 9   Sep     145 non-null    float64\n 10  Oct     145 non-null    float64\n 11  Nov     144 non-null    float64\n 12  Dec     144 non-null    float64\n 13  J-D     144 non-null    float64\n 14  D-N     143 non-null    float64\n 15  DJF     144 non-null    float64\n 16  MAM     145 non-null    float64\n 17  JJA     145 non-null    float64\n 18  SON     144 non-null    float64\ndtypes: float64(18), int64(1)\nmemory usage: 21.6 KB\n\n\n\n#Python Walkthrough 1.2\n\ndf = df.set_index(\"Year\")\ndf.head()\ndf.tail()\n\n\n\n\n\n\n\n\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nJ-D\nD-N\nDJF\nMAM\nJJA\nSON\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2020\n1.59\n1.70\n1.67\n1.40\n1.27\n1.14\n1.10\n1.12\n1.19\n1.21\n1.59\n1.19\n1.35\n1.36\n1.56\n1.44\n1.12\n1.33\n\n\n2021\n1.25\n0.96\n1.21\n1.13\n1.05\n1.21\n1.07\n1.03\n1.05\n1.30\n1.29\n1.17\n1.14\n1.14\n1.13\n1.13\n1.10\n1.21\n\n\n2022\n1.25\n1.17\n1.41\n1.09\n1.02\n1.13\n1.06\n1.17\n1.15\n1.31\n1.10\n1.06\n1.16\n1.17\n1.19\n1.17\n1.12\n1.19\n\n\n2023\n1.30\n1.30\n1.64\n1.02\n1.13\n1.19\n1.45\n1.57\n1.67\n1.88\n1.98\n1.85\n1.50\n1.43\n1.22\n1.26\n1.40\n1.84\n\n\n2024\n1.68\n1.93\n1.78\n1.80\n1.45\n1.54\n1.42\n1.42\n1.58\n1.72\nNaN\nNaN\nNaN\nNaN\n1.82\n1.67\n1.46\nNaN\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\ndf[\"Jan\"].plot(ax=ax)\nax.set_ylabel(\"y label\")\nax.set_xlabel(\"x label\")\nax.set_title(\"title\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\nax.plot(df.index, df[\"Jan\"])\nax.set_ylabel(\"y label\")\nax.set_xlabel(\"x label\")\nax.set_title(\"title\")\nplt.show()\n\n\n\n\n\n\n\n\n\nmonth = \"Jan\"\nfig, ax = plt.subplots()\nax.axhline(0, color=\"orange\")\nax.annotate(\"1951—1980 average\", xy=(0.66, -0.2), xycoords=(\"figure fraction\", \"data\"))\ndf[month].plot(ax=ax)\nax.set_title(\n    f\"Average temperature anomaly in {month} \\n in the northern hemisphere (1880—{df.index.max()})\"\n)\nax.set_ylabel(\"Annual temperature anomalies\")\n\nText(0, 0.5, 'Annual temperature anomalies')\n\n\n\n\n\n\n\n\n\n\n#Python Walkthrough 1.3\nmonth = \"J-D\"\nfig, ax = plt.subplots()\nax.axhline(0, color=\"orange\")\nax.annotate(\"1951—1980 average\", xy=(0.68, -0.2), xycoords=(\"figure fraction\", \"data\"))\ndf[month].plot(ax=ax)\nax.set_title(\n    f\"Average annual temperature anomaly in \\n in the northern hemisphere (1880—{df.index.max()})\"\n)\nax.set_ylabel(\"Annual temperature anomalies\")\n\nText(0, 0.5, 'Annual temperature anomalies')\n\n\n\n\n\n\n\n\n\n\n#Python Walkthrough 1.4\ndf[\"Period\"] = pd.cut(\n    df.index,\n    bins=[1921, 1950, 1980, 2010],\n    labels=[\"1921—1950\", \"1951—1980\", \"1981—2010\"],\n    ordered=True,\n)\n\n\ndf[\"Period\"].tail(20)\n\nYear\n2005    1981—2010\n2006    1981—2010\n2007    1981—2010\n2008    1981—2010\n2009    1981—2010\n2010    1981—2010\n2011          NaN\n2012          NaN\n2013          NaN\n2014          NaN\n2015          NaN\n2016          NaN\n2017          NaN\n2018          NaN\n2019          NaN\n2020          NaN\n2021          NaN\n2022          NaN\n2023          NaN\n2024          NaN\nName: Period, dtype: category\nCategories (3, object): ['1921—1950' &lt; '1951—1980' &lt; '1981—2010']\n\n\n\nlist_of_months = [\"Jun\", \"Jul\", \"Aug\"]\ndf[list_of_months].stack().head()\n\nYear     \n1880  Jun   -0.18\n      Jul   -0.23\n      Aug   -0.27\n1881  Jun   -0.34\n      Jul    0.08\ndtype: float64\n\n\n\nfig, axes = plt.subplots(ncols=3, figsize=(9, 4), sharex=True, sharey=True)\nfor ax, period in zip(axes, df[\"Period\"].dropna().unique()):\n    df.loc[df[\"Period\"] == period, list_of_months].stack().hist(ax=ax)\n    ax.set_title(period)\nplt.suptitle(\"Histogram of temperature anomalies\")\naxes[1].set_xlabel(\"Summer temperature distribution\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n#Python Walkthrough 1.5\n# Create a variable that has years 1951 to 1980, and months Jan to Dec (inclusive)\ntemp_all_months = df.loc[(df.index &gt;= 1951) & (df.index &lt;= 1980), \"Jan\":\"Dec\"]\n# Put all the data in stacked format and give the new columns sensible names\ntemp_all_months = (\n    temp_all_months.stack()\n    .reset_index()\n    .rename(columns={\"level_1\": \"month\", 0: \"values\"})\n)\n# Take a look at this data:\ntemp_all_months\n\n\n\n\n\n\n\n\nYear\nmonth\nvalues\n\n\n\n\n0\n1951\nJan\n-0.36\n\n\n1\n1951\nFeb\n-0.51\n\n\n2\n1951\nMar\n-0.18\n\n\n3\n1951\nApr\n0.06\n\n\n4\n1951\nMay\n0.17\n\n\n...\n...\n...\n...\n\n\n355\n1980\nAug\n0.10\n\n\n356\n1980\nSep\n0.10\n\n\n357\n1980\nOct\n0.12\n\n\n358\n1980\nNov\n0.21\n\n\n359\n1980\nDec\n0.09\n\n\n\n\n360 rows × 3 columns\n\n\n\n\nquantiles = [0.3, 0.7]\nlist_of_percentiles = np.quantile(temp_all_months[\"values\"], q=quantiles)\n\nprint(f\"The cold threshold of {quantiles[0]*100}% is {list_of_percentiles[0]}\")\nprint(f\"The hot threshold of {quantiles[1]*100}% is {list_of_percentiles[1]}\")\n\nThe cold threshold of 30.0% is -0.1\nThe hot threshold of 70.0% is 0.1\n\n\n\n#Python Walkthrough 1.6\n# Create a variable that has years 1981 to 2010, and months Jan to Dec (inclusive)\ntemp_all_months = df.loc[(df.index &gt;= 1981) & (df.index &lt;= 2010), \"Jan\":\"Dec\"]\n# Put all the data in stacked format and give the new columns sensible names\ntemp_all_months = (\n    temp_all_months.stack()\n    .reset_index()\n    .rename(columns={\"level_1\": \"month\", 0: \"values\"})\n)\n# Take a look at the start of this data data:\ntemp_all_months.head()\n\n\n\n\n\n\n\n\nYear\nmonth\nvalues\n\n\n\n\n0\n1981\nJan\n0.80\n\n\n1\n1981\nFeb\n0.62\n\n\n2\n1981\nMar\n0.68\n\n\n3\n1981\nApr\n0.39\n\n\n4\n1981\nMay\n0.18\n\n\n\n\n\n\n\n\nentries_less_than_q30 = temp_all_months[\"values\"] &lt; list_of_percentiles[0]\nproportion_under_q30 = entries_less_than_q30.mean()\nprint(\n    f\"The proportion under {list_of_percentiles[0]} is {proportion_under_q30*100:.2f}%\"\n)\n\nThe proportion under -0.1 is 1.94%\n\n\n\nproportion_over_q70 = (temp_all_months[\"values\"] &gt; list_of_percentiles[1]).mean()\nprint(f\"The proportion over {list_of_percentiles[1]} is {proportion_over_q70*100:.2f}%\")\n\nThe proportion over 0.1 is 84.72%\n\n\n\n#Python Walkthrough 1.7\ntemp_all_months = (\n    df.loc[:, \"DJF\":\"SON\"]\n    .stack()\n    .reset_index()\n    .rename(columns={\"level_1\": \"season\", 0: \"values\"})\n)\ntemp_all_months[\"Period\"] = pd.cut(\n    temp_all_months[\"Year\"],\n    bins=[1921, 1950, 1980, 2010],\n    labels=[\"1921—1950\", \"1951—1980\", \"1981—2010\"],\n    ordered=True,\n)\n# Take a look at a cut of the data using `.iloc`, which provides position\ntemp_all_months.iloc[-135:-125]\n\n\n\n\n\n\n\n\nYear\nseason\nvalues\nPeriod\n\n\n\n\n443\n1991\nDJF\n0.51\n1981—2010\n\n\n444\n1991\nMAM\n0.45\n1981—2010\n\n\n445\n1991\nJJA\n0.42\n1981—2010\n\n\n446\n1991\nSON\n0.32\n1981—2010\n\n\n447\n1992\nDJF\n0.44\n1981—2010\n\n\n448\n1992\nMAM\n0.30\n1981—2010\n\n\n449\n1992\nJJA\n-0.04\n1981—2010\n\n\n450\n1992\nSON\n-0.15\n1981—2010\n\n\n451\n1993\nDJF\n0.37\n1981—2010\n\n\n452\n1993\nMAM\n0.31\n1981—2010\n\n\n\n\n\n\n\n\ngrp_mean_var = temp_all_months.groupby([\"season\", \"Period\"])[\"values\"].agg(\n    [np.mean, np.var]\n)\ngrp_mean_var\n\n\n\n\n\n\n\n\n\nmean\nvar\n\n\nseason\nPeriod\n\n\n\n\n\n\nDJF\n1921—1950\n-0.026207\n0.057303\n\n\n1951—1980\n-0.002333\n0.050494\n\n\n1981—2010\n0.524333\n0.079646\n\n\nJJA\n1921—1950\n-0.052414\n0.021290\n\n\n1951—1980\n-0.000333\n0.014631\n\n\n1981—2010\n0.400333\n0.067727\n\n\nMAM\n1921—1950\n-0.041724\n0.031236\n\n\n1951—1980\n0.000333\n0.025245\n\n\n1981—2010\n0.510333\n0.076238\n\n\nSON\n1921—1950\n0.083103\n0.027751\n\n\n1951—1980\n-0.001000\n0.026258\n\n\n1981—2010\n0.429333\n0.111731\n\n\n\n\n\n\n\n\nmin_year = 1880\n(\n    ggplot(temp_all_months, aes(x=\"Year\", y=\"values\", color=\"season\"))\n    + geom_abline(slope=0, color=\"black\", size=1)\n    + geom_line(size=1)\n    + labs(\n        title=f\"Average annual temperature anomaly in \\n in the northern hemisphere ({min_year}—{temp_all_months['Year'].max()})\",\n        y=\"Annual temperature anomalies\",\n    )\n    + scale_x_continuous(format=\"d\")\n    + geom_text(\n        x=min_year, y=0.1, label=\"1951—1980 average\", hjust=\"left\", color=\"black\"\n    )\n)\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n      \n      \n        \n          \n            \n              \n              \n            \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n            \n              \n                \n                  \n                    1951—1980 average\n                  \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                1880\n              \n            \n          \n          \n            \n            \n            \n              \n                1900\n              \n            \n          \n          \n            \n            \n            \n              \n                1920\n              \n            \n          \n          \n            \n            \n            \n              \n                1940\n              \n            \n          \n          \n            \n            \n            \n              \n                1960\n              \n            \n          \n          \n            \n            \n            \n              \n                1980\n              \n            \n          \n          \n            \n            \n            \n              \n                2000\n              \n            \n          \n          \n            \n            \n            \n              \n                2020\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                -1.0\n              \n            \n          \n          \n            \n              \n                -0.5\n              \n            \n          \n          \n            \n              \n                0.0\n              \n            \n          \n          \n            \n              \n                0.5\n              \n            \n          \n          \n            \n              \n                1.0\n              \n            \n          \n          \n            \n              \n                1.5\n              \n            \n          \n        \n      \n    \n    \n      \n        Average annual temperature anomaly in \n      \n      \n         in the northern hemisphere (1880—2024)\n      \n    \n    \n      \n        Annual temperature anomalies\n      \n    \n    \n      \n        Year\n      \n    \n    \n      \n      \n      \n        \n          \n            season\n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                MAM\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                JJA\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                SON\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                DJF\n              \n            \n          \n        \n      \n    \n    \n    \n  \n  \n  \n\n\n\n\n#Python Walkthrough 1.8\ndf_co2 = pd.read_csv(\"D:\\360MoveData\\Users\\75903\\Desktop\\SuYibo-Homework\\Practice1 SuYibo/CO2-data.csv\")\ndf_co2.head()\n\n\n\n\n\n\n\n\nYear\nMonth\nMonthly average\nInterpolated\nTrend\n\n\n\n\n0\n1958\n3\n315.71\n315.71\n314.62\n\n\n1\n1958\n4\n317.45\n317.45\n315.29\n\n\n2\n1958\n5\n317.50\n317.50\n314.71\n\n\n3\n1958\n6\n-99.99\n317.10\n314.85\n\n\n4\n1958\n7\n315.86\n315.86\n314.98\n\n\n\n\n\n\n\n\ndf_co2_june = df_co2.loc[df_co2[\"Month\"] == 6]\ndf_co2_june.head()\n\n\n\n\n\n\n\n\nYear\nMonth\nMonthly average\nInterpolated\nTrend\n\n\n\n\n3\n1958\n6\n-99.99\n317.10\n314.85\n\n\n15\n1959\n6\n318.15\n318.15\n315.92\n\n\n27\n1960\n6\n319.59\n319.59\n317.36\n\n\n39\n1961\n6\n319.77\n319.77\n317.48\n\n\n51\n1962\n6\n320.55\n320.55\n318.27\n\n\n\n\n\n\n\n\ndf_temp_co2 = pd.merge(df_co2_june, df, on=\"Year\")\ndf_temp_co2[[\"Year\", \"Jun\", \"Trend\"]].head()\n\n\n\n\n\n\n\n\nYear\nJun\nTrend\n\n\n\n\n0\n1958\n0.05\n314.85\n\n\n1\n1959\n0.14\n315.92\n\n\n2\n1960\n0.18\n317.36\n\n\n3\n1961\n0.18\n317.48\n\n\n4\n1962\n-0.13\n318.27\n\n\n\n\n\n\n\n\n(\n    ggplot(df_temp_co2, aes(x=\"Jun\", y=\"Trend\"))\n    + geom_point(color=\"black\", size=3)\n    + labs(\n        title=\"Scatterplot of temperature anomalies vs carbon dioxide emissions\",\n        y=\"Carbon dioxide levels (trend, mole fraction)\",\n        x=\"Temperature anomaly (degrees Celsius)\",\n    )\n)\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                -0.2\n              \n            \n          \n          \n            \n            \n            \n              \n                0.0\n              \n            \n          \n          \n            \n            \n            \n              \n                0.2\n              \n            \n          \n          \n            \n            \n            \n              \n                0.4\n              \n            \n          \n          \n            \n            \n            \n              \n                0.6\n              \n            \n          \n          \n            \n            \n            \n              \n                0.8\n              \n            \n          \n          \n            \n            \n            \n              \n                1.0\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                320\n              \n            \n          \n          \n            \n              \n                340\n              \n            \n          \n          \n            \n              \n                360\n              \n            \n          \n          \n            \n              \n                380\n              \n            \n          \n          \n            \n              \n                400\n              \n            \n          \n        \n      \n    \n    \n      \n        Scatterplot of temperature anomalies vs carbon dioxide emissions\n      \n    \n    \n      \n        Carbon dioxide levels (trend, mole fraction)\n      \n    \n    \n      \n        Temperature anomaly (degrees Celsius)\n      \n    \n    \n    \n  \n  \n  \n\n\n\n\ndf_temp_co2[[\"Jun\", \"Trend\"]].corr(method=\"pearson\")\n\n\n\n\n\n\n\n\nJun\nTrend\n\n\n\n\nJun\n1.00000\n0.91495\n\n\nTrend\n0.91495\n1.00000\n\n\n\n\n\n\n\n\n(\n    ggplot(df_temp_co2, aes(x=\"Year\", y=\"Jun\"))\n    + geom_line(size=1)\n    + labs(\n        title=\"June temperature anomalies\",\n    )\n    + scale_x_continuous(format=\"d\")\n)\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                1960\n              \n            \n          \n          \n            \n            \n            \n              \n                1970\n              \n            \n          \n          \n            \n            \n            \n              \n                1980\n              \n            \n          \n          \n            \n            \n            \n              \n                1990\n              \n            \n          \n          \n            \n            \n            \n              \n                2000\n              \n            \n          \n          \n            \n            \n            \n              \n                2010\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                -0.2\n              \n            \n          \n          \n            \n              \n                0.0\n              \n            \n          \n          \n            \n              \n                0.2\n              \n            \n          \n          \n            \n              \n                0.4\n              \n            \n          \n          \n            \n              \n                0.6\n              \n            \n          \n          \n            \n              \n                0.8\n              \n            \n          \n          \n            \n              \n                1.0\n              \n            \n          \n        \n      \n    \n    \n      \n        June temperature anomalies\n      \n    \n    \n      \n        Jun\n      \n    \n    \n      \n        Year\n      \n    \n    \n    \n  \n  \n  \n\n\n\n\nbase_plot = ggplot(df_temp_co2) + scale_x_continuous(format=\"d\")\nplot_p = (\n    base_plot\n    + geom_line(aes(x=\"Year\", y=\"Jun\"), size=1)\n    + labs(title=\"June temperature anomalies\")\n)\nplot_q = (\n    base_plot\n    + geom_line(aes(x=\"Year\", y=\"Trend\"), size=1)\n    + labs(title=\"Carbon dioxide emissions\")\n)\ngggrid([plot_p, plot_q], ncol=2)\n\n\n  \n    \n    \n  \n  \n    \n    \n      \n      \n      \n        \n          \n            \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n            \n          \n          \n            \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n            \n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n          \n          \n            \n              \n              \n            \n          \n        \n        \n          \n            \n              \n              \n              \n                \n                  1960\n                \n              \n            \n            \n              \n              \n              \n                \n                  1970\n                \n              \n            \n            \n              \n              \n              \n                \n                  1980\n                \n              \n            \n            \n              \n              \n              \n                \n                  1990\n                \n              \n            \n            \n              \n              \n              \n                \n                  2000\n                \n              \n            \n            \n              \n              \n              \n                \n                  2010\n                \n              \n            \n            \n            \n          \n          \n            \n              \n                \n                  -0.2\n                \n              \n            \n            \n              \n                \n                  0.0\n                \n              \n            \n            \n              \n                \n                  0.2\n                \n              \n            \n            \n              \n                \n                  0.4\n                \n              \n            \n            \n              \n                \n                  0.6\n                \n              \n            \n            \n              \n                \n                  0.8\n                \n              \n            \n            \n              \n                \n                  1.0\n                \n              \n            \n          \n        \n      \n      \n        \n          June temperature anomalies\n        \n      \n      \n        \n          Jun\n        \n      \n      \n        \n          Year\n        \n      \n      \n      \n    \n    \n    \n  \n  \n    \n    \n      \n      \n      \n        \n          \n            \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n            \n          \n          \n            \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n            \n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n          \n          \n            \n              \n              \n            \n          \n        \n        \n          \n            \n              \n              \n              \n                \n                  1960\n                \n              \n            \n            \n              \n              \n              \n                \n                  1970\n                \n              \n            \n            \n              \n              \n              \n                \n                  1980\n                \n              \n            \n            \n              \n              \n              \n                \n                  1990\n                \n              \n            \n            \n              \n              \n              \n                \n                  2000\n                \n              \n            \n            \n              \n              \n              \n                \n                  2010\n                \n              \n            \n            \n            \n          \n          \n            \n              \n                \n                  320\n                \n              \n            \n            \n              \n                \n                  340\n                \n              \n            \n            \n              \n                \n                  360\n                \n              \n            \n            \n              \n                \n                  380\n                \n              \n            \n            \n              \n                \n                  400\n                \n              \n            \n          \n        \n      \n      \n        \n          Carbon dioxide emissions\n        \n      \n      \n        \n          Trend\n        \n      \n      \n        \n          Year"
  },
  {
    "objectID": "labs/Labexercises/03Chipotle-Exercises-with-solutions(1).html",
    "href": "labs/Labexercises/03Chipotle-Exercises-with-solutions(1).html",
    "title": "Visualizing Chipotle’s Data",
    "section": "",
    "text": "This time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# set this so the graphs open internally\n%matplotlib inline\n\n\n\nStep 2. Import the dataset from this address.\n\n\nStep 3. Assign it to a variable called chipo.\n\nurl = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv'\n    \nchipo = pd.read_csv(url, sep = '\\t')\n\n\n\nStep 4. See the first 10 entries\n\nchipo.head(10)\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nchoice_description\nitem_price\n\n\n\n\n0\n1\n1\nChips and Fresh Tomato Salsa\nNaN\n$2.39\n\n\n1\n1\n1\nIzze\n[Clementine]\n$3.39\n\n\n2\n1\n1\nNantucket Nectar\n[Apple]\n$3.39\n\n\n3\n1\n1\nChips and Tomatillo-Green Chili Salsa\nNaN\n$2.39\n\n\n4\n2\n2\nChicken Bowl\n[Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n$16.98\n\n\n5\n3\n1\nChicken Bowl\n[Fresh Tomato Salsa (Mild), [Rice, Cheese, Sou...\n$10.98\n\n\n6\n3\n1\nSide of Chips\nNaN\n$1.69\n\n\n7\n4\n1\nSteak Burrito\n[Tomatillo Red Chili Salsa, [Fajita Vegetables...\n$11.75\n\n\n8\n4\n1\nSteak Soft Tacos\n[Tomatillo Green Chili Salsa, [Pinto Beans, Ch...\n$9.25\n\n\n9\n5\n1\nSteak Burrito\n[Fresh Tomato Salsa, [Rice, Black Beans, Pinto...\n$9.25\n\n\n\n\n\n\n\n\n\nStep 5. Create a histogram of the top 5 items bought\n\n# get the Series of the names\nx = chipo.item_name\n\n# use the Counter class from collections to create a dictionary with keys(text) and frequency\nletter_counts = Counter(x)\n\n# convert the dictionary to a DataFrame\ndf = pd.DataFrame.from_dict(letter_counts, orient='index')\n\n# sort the values from the top to the least value and slice the first 5 items\ndf = df[0].sort_values(ascending = True)[45:50]\n\n# create the plot\ndf.plot(kind='bar')\n\n# Set the title and labels\nplt.xlabel('Items')\nplt.ylabel('Number of Times Ordered')\nplt.title('Most ordered Chipotle\\'s Items')\n\n# show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nStep 6. Create a scatterplot with the number of items orderered per order price\n\nHint: Price should be in the X-axis and Items ordered in the Y-axis\n\n# create a list of prices\nchipo.item_price = [float(value[1:-1]) for value in chipo.item_price] # strip the dollar sign and trailing space\n\n# then groupby the orders and sum\norders = chipo.groupby('order_id').sum()\n\n# creates the scatterplot\n# plt.scatter(orders.quantity, orders.item_price, s = 50, c = 'green')\nplt.scatter(x = orders.item_price, y = orders.quantity, s = 50, c = 'green')\n\n# Set the title and labels\nplt.xlabel('Order Price')\nplt.ylabel('Items ordered')\nplt.title('Number of items ordered per order price')\nplt.ylim(0)\n\n(0.0, 36.7)\n\n\n\n\n\nStep 7. BONUS: Create a question and a graph to answer your own question."
  },
  {
    "objectID": "labs/Labexercises/02Chipotle-Exercises-with-solutions.html",
    "href": "labs/Labexercises/02Chipotle-Exercises-with-solutions.html",
    "title": "Ex1 - Filtering and Sorting Data",
    "section": "",
    "text": "This time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\n\n\n\nStep 2. Import the dataset from this address.\n\n\nStep 3. Assign it to a variable called chipo.\n\nurl = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv'\n\nchipo = pd.read_csv(url, sep = '\\t')\n\n\n\nStep 4. How many products cost more than $10.00?\n\n\nprices = [float(value[1 : -1]) for value in chipo.item_price]\n\nchipo.item_price = prices\n\nchipo_filtered = chipo.drop_duplicates(['item_name','quantity','choice_description'])\n\nchipo_one_prod = chipo_filtered[chipo_filtered.quantity == 1]\nchipo_one_prod\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nchoice_description\nitem_price\n\n\n\n\n0\n1\n1\nChips and Fresh Tomato Salsa\nNaN\n2.39\n\n\n1\n1\n1\nIzze\n[Clementine]\n3.39\n\n\n2\n1\n1\nNantucket Nectar\n[Apple]\n3.39\n\n\n3\n1\n1\nChips and Tomatillo-Green Chili Salsa\nNaN\n2.39\n\n\n5\n3\n1\nChicken Bowl\n[Fresh Tomato Salsa (Mild), [Rice, Cheese, Sou...\n10.98\n\n\n...\n...\n...\n...\n...\n...\n\n\n4602\n1827\n1\nBarbacoa Burrito\n[Tomatillo Green Chili Salsa]\n9.25\n\n\n4607\n1829\n1\nSteak Burrito\n[Tomatillo Green Chili Salsa, [Rice, Cheese, S...\n11.75\n\n\n4610\n1830\n1\nSteak Burrito\n[Fresh Tomato Salsa, [Rice, Sour Cream, Cheese...\n11.75\n\n\n4611\n1830\n1\nVeggie Burrito\n[Tomatillo Green Chili Salsa, [Rice, Fajita Ve...\n11.25\n\n\n4612\n1831\n1\nCarnitas Bowl\n[Fresh Tomato Salsa, [Fajita Vegetables, Rice,...\n9.25\n\n\n\n\n1806 rows × 5 columns\n\n\n\n\nchipo.query('item_price &gt; 10').item_name.nunique()\n\n31\n\n\n\n\nStep 5. What is the price of each item?\n\nprint a data frame with only two columns item_name and item_price\n\n\nchipo_filtered = chipo.drop_duplicates(['item_name','quantity'])\n\nchipo[(chipo['item_name'] == 'Chicken Bowl') & (chipo['quantity'] == 1)]\n\nchipo_one_prod = chipo_filtered[chipo_filtered.quantity == 1]\n\nprice_per_item = chipo_one_prod[['item_name', 'item_price']]\n\nprice_per_item.sort_values(by = \"item_price\", ascending = False).head(20)\n\n\n\n\n\n\n\n\nitem_name\nitem_price\n\n\n\n\n606\nSteak Salad Bowl\n11.89\n\n\n1229\nBarbacoa Salad Bowl\n11.89\n\n\n1132\nCarnitas Salad Bowl\n11.89\n\n\n7\nSteak Burrito\n11.75\n\n\n168\nBarbacoa Crispy Tacos\n11.75\n\n\n39\nBarbacoa Bowl\n11.75\n\n\n738\nVeggie Soft Tacos\n11.25\n\n\n186\nVeggie Salad Bowl\n11.25\n\n\n62\nVeggie Bowl\n11.25\n\n\n57\nVeggie Burrito\n11.25\n\n\n250\nChicken Salad\n10.98\n\n\n5\nChicken Bowl\n10.98\n\n\n8\nSteak Soft Tacos\n9.25\n\n\n554\nCarnitas Crispy Tacos\n9.25\n\n\n237\nCarnitas Soft Tacos\n9.25\n\n\n56\nBarbacoa Soft Tacos\n9.25\n\n\n92\nSteak Crispy Tacos\n9.25\n\n\n664\nSteak Salad\n8.99\n\n\n54\nSteak Bowl\n8.99\n\n\n3750\nCarnitas Salad\n8.99\n\n\n\n\n\n\n\n\n\n\nStep 6. Sort by the name of the item\n\nchipo.item_name.sort_values()\n\n# OR\n\nchipo.sort_values(by = \"item_name\")\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nchoice_description\nitem_price\n\n\n\n\n3389\n1360\n2\n6 Pack Soft Drink\n[Diet Coke]\n12.98\n\n\n341\n148\n1\n6 Pack Soft Drink\n[Diet Coke]\n6.49\n\n\n1849\n749\n1\n6 Pack Soft Drink\n[Coke]\n6.49\n\n\n1860\n754\n1\n6 Pack Soft Drink\n[Diet Coke]\n6.49\n\n\n2713\n1076\n1\n6 Pack Soft Drink\n[Coke]\n6.49\n\n\n...\n...\n...\n...\n...\n...\n\n\n2384\n948\n1\nVeggie Soft Tacos\n[Roasted Chili Corn Salsa, [Fajita Vegetables,...\n8.75\n\n\n781\n322\n1\nVeggie Soft Tacos\n[Fresh Tomato Salsa, [Black Beans, Cheese, Sou...\n8.75\n\n\n2851\n1132\n1\nVeggie Soft Tacos\n[Roasted Chili Corn Salsa (Medium), [Black Bea...\n8.49\n\n\n1699\n688\n1\nVeggie Soft Tacos\n[Fresh Tomato Salsa, [Fajita Vegetables, Rice,...\n11.25\n\n\n1395\n567\n1\nVeggie Soft Tacos\n[Fresh Tomato Salsa (Mild), [Pinto Beans, Rice...\n8.49\n\n\n\n\n4622 rows × 5 columns\n\n\n\n\n\nStep 7. What was the quantity of the most expensive item ordered?\n\nchipo.sort_values(by = \"item_price\", ascending = False).head(1)\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nchoice_description\nitem_price\n\n\n\n\n3598\n1443\n15\nChips and Fresh Tomato Salsa\nNaN\n44.25\n\n\n\n\n\n\n\n\n\nStep 8. How many times was a Veggie Salad Bowl ordered?\n\nchipo_salad = chipo[chipo.item_name == \"Veggie Salad Bowl\"]\n# chipo_salad = chipo.query('item_name == \"Veggie Salad Bowl\"')\n\nlen(chipo_salad)\n\n18\n\n\n\n\nStep 9. How many times did someone order more than one Canned Soda?\n\nchipo_drink_steak_bowl = chipo[(chipo.item_name == \"Canned Soda\") & (chipo.quantity &gt; 1)]\n# chipo_drink_steak_bowl = chipo.query('item_name == \"Canned Soda\" & quantity &gt; 1')\n\nlen(chipo_drink_steak_bowl)\n\n20"
  },
  {
    "objectID": "labs/Labexercises/01Occupation-Exercises-with-solutions.html",
    "href": "labs/Labexercises/01Occupation-Exercises-with-solutions.html",
    "title": "Ex3 - Getting and Knowing your Data",
    "section": "",
    "text": "This time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\nimport numpy as np\n\n\n\nStep 2. Import the dataset from this address.\n\n\nStep 3. Assign it to a variable called users and use the ‘user_id’ as index\n\nusers = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT8/master/data/u.user', \n                      sep='|', index_col='user_id')\n\n\n\nStep 4. See the first 25 entries\n\nusers.head(25)\n\n\n\n\n\n\n\n\nage\ngender\noccupation\nzip_code\n\n\nuser_id\n\n\n\n\n\n\n\n\n1\n24\nM\ntechnician\n85711\n\n\n2\n53\nF\nother\n94043\n\n\n3\n23\nM\nwriter\n32067\n\n\n4\n24\nM\ntechnician\n43537\n\n\n5\n33\nF\nother\n15213\n\n\n6\n42\nM\nexecutive\n98101\n\n\n7\n57\nM\nadministrator\n91344\n\n\n8\n36\nM\nadministrator\n05201\n\n\n9\n29\nM\nstudent\n01002\n\n\n10\n53\nM\nlawyer\n90703\n\n\n11\n39\nF\nother\n30329\n\n\n12\n28\nF\nother\n06405\n\n\n13\n47\nM\neducator\n29206\n\n\n14\n45\nM\nscientist\n55106\n\n\n15\n49\nF\neducator\n97301\n\n\n16\n21\nM\nentertainment\n10309\n\n\n17\n30\nM\nprogrammer\n06355\n\n\n18\n35\nF\nother\n37212\n\n\n19\n40\nM\nlibrarian\n02138\n\n\n20\n42\nF\nhomemaker\n95660\n\n\n21\n26\nM\nwriter\n30068\n\n\n22\n25\nM\nwriter\n40206\n\n\n23\n30\nF\nartist\n48197\n\n\n24\n21\nF\nartist\n94533\n\n\n25\n39\nM\nengineer\n55107\n\n\n\n\n\n\n\n\n\nStep 5. See the last 10 entries\n\nusers.tail(10)\n\n\n\n\n\n\n\n\nage\ngender\noccupation\nzip_code\n\n\nuser_id\n\n\n\n\n\n\n\n\n934\n61\nM\nengineer\n22902\n\n\n935\n42\nM\ndoctor\n66221\n\n\n936\n24\nM\nother\n32789\n\n\n937\n48\nM\neducator\n98072\n\n\n938\n38\nF\ntechnician\n55038\n\n\n939\n26\nF\nstudent\n33319\n\n\n940\n32\nM\nadministrator\n02215\n\n\n941\n20\nM\nstudent\n97229\n\n\n942\n48\nF\nlibrarian\n78209\n\n\n943\n22\nM\nstudent\n77841\n\n\n\n\n\n\n\n\n\nStep 6. What is the number of observations in the dataset?\n\nusers.shape[0]\n\n943\n\n\n\n\nStep 7. What is the number of columns in the dataset?\n\nusers.shape[1]\n\n4\n\n\n\n\nStep 8. Print the name of all the columns.\n\nusers.columns\n\nIndex(['age', 'gender', 'occupation', 'zip_code'], dtype='object')\n\n\n\n\nStep 9. How is the dataset indexed?\n\nusers.index\n\nIndex([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,\n       ...\n       934, 935, 936, 937, 938, 939, 940, 941, 942, 943],\n      dtype='int64', name='user_id', length=943)\n\n\n\n\nStep 10. What is the data type of each column?\n\nusers.dtypes\n\nage            int64\ngender        object\noccupation    object\nzip_code      object\ndtype: object\n\n\n\n\nStep 11. Print only the occupation column\n\nusers.occupation\n\n#or\n\nusers['occupation']\n\nuser_id\n1         technician\n2              other\n3             writer\n4         technician\n5              other\n           ...      \n939          student\n940    administrator\n941          student\n942        librarian\n943          student\nName: occupation, Length: 943, dtype: object\n\n\n\n\nStep 12. How many different occupations are in this dataset?\n\nusers.occupation.nunique()\n\n21\n\n\n\n\nStep 13. What is the most frequent occupation?\n\nusers.occupation.value_counts().head(1).index[0]\n\n'student'\n\n\n\n\nStep 14. Summarize the DataFrame.\n\nusers.describe() \n\n\n\n\n\n\n\n\nage\n\n\n\n\ncount\n943.000000\n\n\nmean\n34.051962\n\n\nstd\n12.192740\n\n\nmin\n7.000000\n\n\n25%\n25.000000\n\n\n50%\n31.000000\n\n\n75%\n43.000000\n\n\nmax\n73.000000\n\n\n\n\n\n\n\n\n\nStep 15. Summarize all the columns\n\nusers.describe(include = \"all\")\n\n\n\n\n\n\n\n\nage\ngender\noccupation\nzip_code\n\n\n\n\ncount\n943.000000\n943\n943\n943\n\n\nunique\nNaN\n2\n21\n795\n\n\ntop\nNaN\nM\nstudent\n55414\n\n\nfreq\nNaN\n670\n196\n9\n\n\nmean\n34.051962\nNaN\nNaN\nNaN\n\n\nstd\n12.192740\nNaN\nNaN\nNaN\n\n\nmin\n7.000000\nNaN\nNaN\nNaN\n\n\n25%\n25.000000\nNaN\nNaN\nNaN\n\n\n50%\n31.000000\nNaN\nNaN\nNaN\n\n\n75%\n43.000000\nNaN\nNaN\nNaN\n\n\nmax\n73.000000\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\nStep 16. Summarize only the occupation column\n\nusers.occupation.describe()\n\ncount         943\nunique         21\ntop       student\nfreq          196\nName: occupation, dtype: object\n\n\n\n\nStep 17. What is the mean age of users?\n\nround(users.age.mean())\n\n34\n\n\n\n\nStep 18. What is the age with least occurrence?\n\nusers.age.value_counts().tail()\n\nage\n7     1\n66    1\n11    1\n10    1\n73    1\nName: count, dtype: int64"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, Im Su Yibo",
    "section": "",
    "text": "Hello, and thanks for visiting!\nWelcome to my website and data analysis portfolio.\nHere, I’ll feature my projects for the Fall 2024 Informational technologies in Business class.\nPlease use the Menu Bar above to look around."
  },
  {
    "objectID": "homework/Lecture5_SuYibo_Homework.html",
    "href": "homework/Lecture5_SuYibo_Homework.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\nfrom lets_plot import *\n\nLetsPlot.setup_html()\n\n\n            \n            \n            \n\n\n\ndf = pd.read_csv('/D:\\360MoveData\\Users\\75903\\Desktop\\SuYibo-Homework/Lecture5 SuYibo Homework/plastic-waste.csv')\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 240 entries, 0 to 239\nData columns (total 10 columns):\n #   Column                            Non-Null Count  Dtype  \n---  ------                            --------------  -----  \n 0   code                              240 non-null    object \n 1   entity                            240 non-null    object \n 2   continent                         240 non-null    object \n 3   year                              240 non-null    int64  \n 4   gdp_per_cap                       195 non-null    float64\n 5   plastic_waste_per_cap             189 non-null    float64\n 6   mismanaged_plastic_waste_per_cap  189 non-null    float64\n 7   mismanaged_plastic_waste          189 non-null    float64\n 8   coastal_pop                       189 non-null    float64\n 9   total_pop                         230 non-null    float64\ndtypes: float64(6), int64(1), object(3)\nmemory usage: 18.9+ KB\n\n\n\n#Plot, using histograms, the distribution of plastic waste per capita faceted by continent. What can you say about how the continents compare to each other in terms of their plastic waste per capita?\ndf['plastic_waste_per_cap'] = pd.to_numeric(df['plastic_waste_per_cap'], errors='coerce')\ndf.dropna(subset=['plastic_waste_per_cap'], inplace=True)\n\n\n\n(\n    ggplot(df, aes(x='plastic_waste_per_cap')) + \\\n        geom_histogram(binwidth=0.05, fill='blue', color='black', alpha=0.7) + \\\n        facet_wrap('continent', scales='free_y') + \\\n        labs(x='Plastic Waste per Capita (kg)', y='Frequency', title='Distribution of Plastic Waste per Capita by Continent') + \\\n        theme_minimal()\n    \n)\n\n   \n   \n\n\n\n#Convert your side-by-side box plots from the previous task to violin plots. What do the violin plots reveal that box plots do not? What features are apparent in the box plots but not in the violin plots?\ndf['plastic_waste_per_cap'] = pd.to_numeric(df['plastic_waste_per_cap'], errors='coerce')\ndf.dropna(subset=['plastic_waste_per_cap'], inplace=True)\n\n\n\n(\n    ggplot(df, aes(x='continent', y='plastic_waste_per_cap')) + \\\n        geom_violin(trim=False, fill='lightblue', color='black') + \\\n        facet_wrap('continent', scales='free_y') + \\\n        labs(x='Continent', y='Plastic Waste per Capita (kg)', title='Distribution of Plastic Waste per Capita by Continent') + \\\n        theme_minimal()\n)\n\n   \n   \n\n\n\n#Visualize the relationship between plastic waste per capita and mismanaged plastic waste per capita using a scatterplot. Describe the relationship.\ndf['plastic_waste_per_cap'] = pd.to_numeric(df['plastic_waste_per_cap'], errors='coerce')\ndf['mismanaged_plastic_waste_per_cap'] = pd.to_numeric(df['mismanaged_plastic_waste_per_cap'], errors='coerce')\ndf.dropna(subset=['plastic_waste_per_cap', 'mismanaged_plastic_waste_per_cap'], inplace=True)\n\n\n\n(\n    ggplot(df, aes(x='plastic_waste_per_cap', y='mismanaged_plastic_waste_per_cap')) + \\\n        geom_point(alpha=0.7, color='blue') + \\\n        labs(x='Plastic Waste per Capita (kg)', y='Mismanaged Plastic Waste per Capita (kg)', title='Relationship between Plastic Waste and Mismanaged Plastic Waste per Capita') + \\\n        theme_minimal()\n)\n\n   \n   \n\n\n\n#Colour the points in the scatterplot by continent. Does there seem to be any clear distinctions between continents with respect to how plastic waste per capita and mismanaged plastic waste per capita are associated?\ndf['plastic_waste_per_cap'] = pd.to_numeric(df['plastic_waste_per_cap'], errors='coerce')\ndf['mismanaged_plastic_waste_per_cap'] = pd.to_numeric(df['mismanaged_plastic_waste_per_cap'], errors='coerce')\ndf.dropna(subset=['plastic_waste_per_cap', 'mismanaged_plastic_waste_per_cap'], inplace=True)\n\n\n\n(\n    ggplot(df, aes(x='plastic_waste_per_cap', y='mismanaged_plastic_waste_per_cap', color='continent')) + \\\n        geom_point(alpha=0.7) + \\\n        labs(x='Plastic Waste per Capita (kg)', y='Mismanaged Plastic Waste per Capita (kg)', title='Relationship between Plastic Waste and Mismanaged Plastic Waste per Capita by Continent') + \\\n        theme_minimal() + \\\n        scale_color_discrete(name='Continent') + \\\n        guides(color=guide_legend(title='Continent'))\n\n)\n\n   \n   \n\n\n\n#Visualize the relationship between plastic waste per capita and total population as well as plastic waste per capita and coastal population. You will need to make two separate plots. Do either of these pairs of variables appear to be more strongly linearly associated?\ndf['plastic_waste_per_cap'] = pd.to_numeric(df['plastic_waste_per_cap'], errors='coerce')\ndf['total_pop'] = pd.to_numeric(df['total_pop'], errors='coerce')\ndf['coastal_pop'] = pd.to_numeric(df['coastal_pop'], errors='coerce')\ndf.dropna(subset=['plastic_waste_per_cap', 'total_pop', 'coastal_pop'], inplace=True)\n\n\n\n(\n    ggplot(df, aes(x='coastal_pop', y='plastic_waste_per_cap')) + \\\n        geom_point(alpha=0.7) + \\\n        labs(x='Coastal Population', y='Plastic Waste per Capita (kg)', title='Relationship between Plastic Waste per Capita and Coastal Population') + \\\n        theme_minimal()\n)\n\n   \n   \n\n\n\n#Recreate the following plot, and interpret what you see in context of the data.\n\ndf['plastic_waste_per_cap'] = pd.to_numeric(df['plastic_waste_per_cap'], errors='coerce')\ndf['total_pop'] = pd.to_numeric(df['total_pop'], errors='coerce')\ndf['coastal_pop'] = pd.to_numeric(df['coastal_pop'], errors='coerce')\ndf.dropna(subset=['plastic_waste_per_cap', 'total_pop', 'coastal_pop'], inplace=True)\n\n# Calculate coastal population proportion\ndf['coastal_pop_prop'] = df['coastal_pop'] / df['total_pop']\n\n\n\n(\n    ggplot(df, aes(x='coastal_pop_prop', y='plastic_waste_per_cap', color='continent')) + \n        geom_point(alpha=0.7) + \n        geom_smooth(method='loess', se=True, color='black') + \n        labs(x='Coastal population proportion (Coastal / total population)', y='Plastic waste per capita', title='Plastic waste vs. coastal population proportion by continent') + \\\n        theme_minimal() + \n        scale_color_discrete(name='Continent')\n)"
  },
  {
    "objectID": "homework/Lecture3_SuYibo_Homework2.html",
    "href": "homework/Lecture3_SuYibo_Homework2.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\nfrom skimpy import skim\ndf = pd.read_csv(\"shujuji2.csv\")\nskim(df)\n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types                                                                 │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n│ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                                                          │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n│ │ Number of rows    │ 891    │ │ int32       │ 5     │                                                          │\n│ │ Number of columns │ 12     │ │ string      │ 5     │                                                          │\n│ └───────────────────┴────────┘ │ float64     │ 2     │                                                          │\n│                                └─────────────┴───────┘                                                          │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┓  │\n│ ┃ column_name    ┃ NA   ┃ NA %    ┃ mean     ┃ sd       ┃ p0    ┃ p25    ┃ p50    ┃ p75    ┃ p100   ┃ hist   ┃  │\n│ ┡━━━━━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━┩  │\n│ │ PassengerId    │    0 │       0 │      446 │    257.4 │     1 │  223.5 │    446 │  668.5 │    891 │ ▇▇▇▇▇▇ │  │\n│ │ Survived       │    0 │       0 │   0.3838 │   0.4866 │     0 │      0 │      0 │      1 │      1 │ ▇    ▅ │  │\n│ │ Pclass         │    0 │       0 │    2.309 │   0.8361 │     1 │      2 │      3 │      3 │      3 │ ▃  ▃ ▇ │  │\n│ │ Age            │  177 │   19.87 │     29.7 │    14.53 │  0.42 │  20.12 │     28 │     38 │     80 │ ▂▇▇▃▁  │  │\n│ │ SibSp          │    0 │       0 │    0.523 │    1.103 │     0 │      0 │      0 │      1 │      8 │   ▇    │  │\n│ │ Parch          │    0 │       0 │   0.3816 │   0.8061 │     0 │      0 │      0 │      0 │      6 │  ▇▁▁   │  │\n│ │ Fare           │    0 │       0 │     32.2 │    49.69 │     0 │   7.91 │  14.45 │     31 │  512.3 │   ▇    │  │\n│ └────────────────┴──────┴─────────┴──────────┴──────────┴───────┴────────┴────────┴────────┴────────┴────────┘  │\n│                                                     string                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓  │\n│ ┃ column_name              ┃ NA       ┃ NA %       ┃ words per row                ┃ total words              ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩  │\n│ │ Name                     │        0 │          0 │                          4.1 │                     3626 │  │\n│ │ Sex                      │        0 │          0 │                            1 │                      891 │  │\n│ │ Ticket                   │        0 │          0 │                          1.3 │                     1130 │  │\n│ │ Cabin                    │      687 │       77.1 │                         0.27 │                      238 │  │\n│ │ Embarked                 │        2 │       0.22 │                            1 │                      889 │  │\n│ └──────────────────────────┴──────────┴────────────┴──────────────────────────────┴──────────────────────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n\n\n\n\nprint(df.head())\n\n   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  \n\n\n\nprint(df.describe())\n\n       PassengerId    Survived      Pclass         Age       SibSp  \\\ncount   891.000000  891.000000  891.000000  714.000000  891.000000   \nmean    446.000000    0.383838    2.308642   29.699118    0.523008   \nstd     257.353842    0.486592    0.836071   14.526497    1.102743   \nmin       1.000000    0.000000    1.000000    0.420000    0.000000   \n25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n75%     668.500000    1.000000    3.000000   38.000000    1.000000   \nmax     891.000000    1.000000    3.000000   80.000000    8.000000   \n\n            Parch        Fare  \ncount  891.000000  891.000000  \nmean     0.381594   32.204208  \nstd      0.806057   49.693429  \nmin      0.000000    0.000000  \n25%      0.000000    7.910400  \n50%      0.000000   14.454200  \n75%      0.000000   31.000000  \nmax      6.000000  512.329200  \n\n\n\nprint(df.isnull().sum())\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\n\n\nprint(df.dtypes)\n\nPassengerId      int64\nSurvived         int64\nPclass           int64\nName            object\nSex             object\nAge            float64\nSibSp            int64\nParch            int64\nTicket          object\nFare           float64\nCabin           object\nEmbarked        object\ndtype: object\n\n\n\nimport matplotlib.pyplot as plt\nsurvival_counts = df['Survived'].value_counts()\nsurvival_counts.plot(kind='bar')\nplt.title('Survival Counts')\nplt.xlabel('Survived')\nplt.ylabel('Count')\nplt.xticks(ticks=[0, 1], labels=['No', 'Yes'], rotation=0)\nplt.show()\n\n\n\n\n\n\n\n\n\ngender_survival = df.groupby(['Sex', 'Survived']).size().unstack()\ngender_survival.plot(kind='bar', stacked=True)\nplt.title('Survival by Gender')\nplt.xlabel('Gender')\nplt.ylabel('Count')\nplt.xticks(rotation=0)\nplt.show()"
  },
  {
    "objectID": "homework/Lecture2_SuYibo_Homework.html",
    "href": "homework/Lecture2_SuYibo_Homework.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndf = pd.read_csv('/Users/apple/Desktop/archive/seattle_pet_licenses.csv')\n\n\nprint(df)\n\n      animal_s_name       license_issue_date  license_number  \\\n0              Ozzy  2005-03-29T00:00:00.000        130651.0   \n1              Jack  2009-12-23T00:00:00.000        898148.0   \n2            Ginger  2006-01-20T00:00:00.000         29654.0   \n3            Pepper  2006-02-07T00:00:00.000         75432.0   \n4              Addy  2006-08-04T00:00:00.000        729899.0   \n...             ...                      ...             ...   \n66037          Lily  2016-12-27T00:00:00.000             NaN   \n66038         Ellie  2016-11-29T00:00:00.000             NaN   \n66039         Sammy  2016-12-05T00:00:00.000             NaN   \n66040         Buddy  2016-12-06T00:00:00.000             NaN   \n66041           Aku  2016-12-07T00:00:00.000             NaN   \n\n                           primary_breed      secondary_breed species zip_code  \n0      Dachshund, Standard Smooth Haired                  NaN     Dog    98104  \n1                   Schnauzer, Miniature         Terrier, Rat     Dog    98107  \n2                      Retriever, Golden  Retriever, Labrador     Dog    98117  \n3                                   Manx                  Mix     Cat    98103  \n4                      Retriever, Golden                  NaN     Dog    98105  \n...                                  ...                  ...     ...      ...  \n66037                 Domestic Shorthair                  Mix     Cat    98117  \n66038                    German Shepherd                  Mix     Dog    98105  \n66039                            Terrier              Maltese     Dog    98105  \n66040                        Bullmastiff                  Mix     Dog    98105  \n66041              Chihuahua, Short Coat              Terrier     Dog    98106  \n\n[66042 rows x 7 columns]\n\n\n\ninfor_df = df.info()\n\nprint(infor_df)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 66042 entries, 0 to 66041\nData columns (total 7 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   animal_s_name       64685 non-null  object \n 1   license_issue_date  66042 non-null  object \n 2   license_number      43885 non-null  float64\n 3   primary_breed       66042 non-null  object \n 4   secondary_breed     22538 non-null  object \n 5   species             66042 non-null  object \n 6   zip_code            65884 non-null  object \ndtypes: float64(1), object(6)\nmemory usage: 3.5+ MB\nNone\n\n\n\ntop_three_ani = df['animal_s_name'].value_counts()\n\nprint(top_three_ani)\n\nanimal_s_name\nLucy       566\nBella      451\nCharlie    447\nMax        374\nLuna       361\n          ... \nFinigan      1\nLiame        1\nHitler       1\nRustie       1\nChino        1\nName: count, Length: 15795, dtype: int64\n\n\n\nf, ax = plt.subplots(1, 2, figsize=(20, 7))\ntop_three_ani = df['animal_s_name'].value_counts()\n\nd = top_three_ani[0:3].plot(kind='bar', \n                         ax=ax[0], \n                         title='Top tnree ani names')\n\nd.set_ylabel('Count')\n\nplt.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Su Yibo’s website and data analysis portfolio",
    "section": "",
    "text": "Hello, and thanks for visiting! Welcome to my website and data analytics portfolio. Here, I’ll present my projects for the Fall 2024 Informational technologies in Business class. Please use the Menu Bar above to browse.\n\nI am a student majoring in education management. I come from Henan Province, which is in the middle of China. The reason why I choose this major is that I am a teacher working in a Chinese university. My undergraduate major is pedagogy.\nXXXXX"
  },
  {
    "objectID": "about.html#education-experience",
    "href": "about.html#education-experience",
    "title": "Su Yibo’s website and data analysis portfolio",
    "section": "Education Experience",
    "text": "Education Experience\nDuring my undergraduate years, I majored in education, embarking on a profound journey of intellectual exploration. The curriculum was diverse and meticulously detailed. From foundational psychology courses that fortified my professional competencies to challenging courses on the history of education within pedagogy, which inspired me to envision the vast possibilities of teaching, I made steadfast progress at every turn. Regarding academic practice, I was an avid participant in numerous projects. I engaged in a variety of educational initiatives organized by the university, traveling to schools of differing levels for investigative research and practical internships. Through specialized training, I have acquired proficiency in an array of computer software and teaching methodologies, enabling me to work efficiently and produce high-quality results. This enriching educational experience has molded me into the competent professional I am today."
  },
  {
    "objectID": "about.html#work-experience",
    "href": "about.html#work-experience",
    "title": "Su Yibo’s website and data analysis portfolio",
    "section": "Work Experience",
    "text": "Work Experience\nI have been working in the University’s Student Development Office since 2022. In daily work, I based on the reality of the school, closely around the school work, adhere to the student-oriented, to promote the comprehensive development of students as the focus, and further improve the organization and system construction of the Youth League committee, to do a good job in student management, students to carry out various activities to lay a good foundation."
  },
  {
    "objectID": "about.html#hobbies-fun-facts",
    "href": "about.html#hobbies-fun-facts",
    "title": "Su Yibo’s website and data analysis portfolio",
    "section": "Hobbies & Fun Facts",
    "text": "Hobbies & Fun Facts\nMy interests include: Sports, playing basketball and going to the gym is my daily routine Watch movies and TV series, especially artsy films and mystery films Listening to music, classical music and R&B music are my favorites"
  },
  {
    "objectID": "homework/Lecture3_SuYibo_Homework1.html",
    "href": "homework/Lecture3_SuYibo_Homework1.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd \n\nread_file = pd.read_excel (\"/D:\\360MoveData\\Users\\75903\\Desktop\\SuYibo-Homework\\Lecture3 SuYibo Homework/nobel.xlsx\") \n\nread_file.to_csv (\"nobel.csv\", \n                index = None, \n                header=True) \n    \n#read nobel.csv\ndf = pd.DataFrame(pd.read_csv(\"nobel.csv\")) \nprint(df)\n\n      id       firstname    surname  year   category  \\\n0      1  Wilhelm Conrad    Röntgen  1901    Physics   \n1      2      Hendrik A.    Lorentz  1902    Physics   \n2      3          Pieter     Zeeman  1902    Physics   \n3      4           Henri  Becquerel  1903    Physics   \n4      5          Pierre      Curie  1903    Physics   \n..   ...             ...        ...   ...        ...   \n930  965  Sir Gregory P.     Winter  2018  Chemistry   \n931  966           Denis    Mukwege  2018      Peace   \n932  967           Nadia      Murad  2018      Peace   \n933  968      William D.   Nordhaus  2018  Economics   \n934  969         Paul M.      Romer  2018  Economics   \n\n                                           affiliation          city  \\\n0                                    Munich University        Munich   \n1                                    Leiden University        Leiden   \n2                                 Amsterdam University     Amsterdam   \n3                                  École Polytechnique         Paris   \n4    École municipale de physique et de chimie indu...         Paris   \n..                                                 ...           ...   \n930                MRC Laboratory of Molecular Biology     Cambridge   \n931                                                NaN           NaN   \n932                                                NaN           NaN   \n933                                    Yale University  New Haven CT   \n934                       NYU Stern School of Business   New York NY   \n\n            country            born_date   died_date  ... died_country_code  \\\n0           Germany           1845-03-27  1923-02-10  ...                DE   \n1       Netherlands           1853-07-18  1928-02-04  ...                NL   \n2       Netherlands           1865-05-25  1943-10-09  ...                NL   \n3            France           1852-12-15  1908-08-25  ...                FR   \n4            France           1859-05-15  1906-04-19  ...                FR   \n..              ...                  ...         ...  ...               ...   \n930  United Kingdom  1951-04-14 00:00:00         NaN  ...               NaN   \n931             NaN  1955-03-01 00:00:00         NaN  ...               NaN   \n932             NaN                  NaN         NaN  ...               NaN   \n933             USA  1941-05-31 00:00:00         NaN  ...               NaN   \n934             USA                  NaN         NaN  ...               NaN   \n\n    overall_motivation share  \\\n0                  NaN     1   \n1                  NaN     2   \n2                  NaN     2   \n3                  NaN     2   \n4                  NaN     4   \n..                 ...   ...   \n930                NaN     4   \n931                NaN     2   \n932                NaN     2   \n933                NaN     2   \n934                NaN     2   \n\n                                            motivation  \\\n0    \"in recognition of the extraordinary services ...   \n1    \"in recognition of the extraordinary service t...   \n2    \"in recognition of the extraordinary service t...   \n3    \"in recognition of the extraordinary services ...   \n4    \"in recognition of the extraordinary services ...   \n..                                                 ...   \n930  \"for the phage display of peptides and antibod...   \n931  \"for their efforts to end the use of sexual vi...   \n932  \"for their efforts to end the use of sexual vi...   \n933  \"for integrating climate change into long-run ...   \n934  \"for integrating technological innovations int...   \n\n                                 born_country_original  \\\n0                                Prussia (now Germany)   \n1                                      the Netherlands   \n2                                      the Netherlands   \n3                                               France   \n4                                               France   \n..                                                 ...   \n930                                     United Kingdom   \n931  Belgian Congo (now Democratic Republic of the ...   \n932                                               Iraq   \n933                                                USA   \n934                                                USA   \n\n         born_city_original died_country_original died_city_original  \\\n0    Lennep (now Remscheid)               Germany             Munich   \n1                    Arnhem       the Netherlands                NaN   \n2                Zonnemaire       the Netherlands          Amsterdam   \n3                     Paris                France                NaN   \n4                     Paris                France              Paris   \n..                      ...                   ...                ...   \n930               Leicester                   NaN                NaN   \n931                  Bukavu                   NaN                NaN   \n932                    Kojo                   NaN                NaN   \n933          Albuquerque NM                   NaN                NaN   \n934               Denver CO                   NaN                NaN   \n\n     city_original country_original  \n0           Munich          Germany  \n1           Leiden  the Netherlands  \n2        Amsterdam  the Netherlands  \n3            Paris           France  \n4            Paris           France  \n..             ...              ...  \n930      Cambridge   United Kingdom  \n931            NaN              NaN  \n932            NaN              NaN  \n933   New Haven CT              USA  \n934    New York NY              USA  \n\n[935 rows x 26 columns]\n\n\n\ninfor = df.info()\nprint(infor)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 935 entries, 0 to 934\nData columns (total 26 columns):\n #   Column                 Non-Null Count  Dtype \n---  ------                 --------------  ----- \n 0   id                     935 non-null    int64 \n 1   firstname              935 non-null    object\n 2   surname                906 non-null    object\n 3   year                   935 non-null    int64 \n 4   category               935 non-null    object\n 5   affiliation            685 non-null    object\n 6   city                   680 non-null    object\n 7   country                681 non-null    object\n 8   born_date              902 non-null    object\n 9   died_date              627 non-null    object\n 10  gender                 935 non-null    object\n 11  born_city              907 non-null    object\n 12  born_country           907 non-null    object\n 13  born_country_code      907 non-null    object\n 14  died_city              608 non-null    object\n 15  died_country           614 non-null    object\n 16  died_country_code      614 non-null    object\n 17  overall_motivation     17 non-null     object\n 18  share                  935 non-null    int64 \n 19  motivation             935 non-null    object\n 20  born_country_original  907 non-null    object\n 21  born_city_original     907 non-null    object\n 22  died_country_original  614 non-null    object\n 23  died_city_original     608 non-null    object\n 24  city_original          680 non-null    object\n 25  country_original       681 non-null    object\ndtypes: int64(3), object(23)\nmemory usage: 190.1+ KB\nNone\n\n\n\nnobel_living = df[\n    df['country'].notna() &\n    (df['gender'] != 'org') &\n    df['died_date'].isna()        \n]\nprint(nobel_living)\n\n      id       firstname   surname  year   category  \\\n68    68       Chen Ning      Yang  1957    Physics   \n69    69       Tsung-Dao       Lee  1957    Physics   \n94    95         Leon N.    Cooper  1972    Physics   \n96    97             Leo     Esaki  1973    Physics   \n97    98            Ivar   Giaever  1973    Physics   \n..   ...             ...       ...   ...        ...   \n928  963      Frances H.    Arnold  2018  Chemistry   \n929  964       George P.     Smith  2018  Chemistry   \n930  965  Sir Gregory P.    Winter  2018  Chemistry   \n933  968      William D.  Nordhaus  2018  Economics   \n934  969         Paul M.     Romer  2018  Economics   \n\n                                      affiliation                 city  \\\n68                   Institute for Advanced Study         Princeton NJ   \n69                            Columbia University          New York NY   \n94                               Brown University        Providence RI   \n96           IBM Thomas J. Watson Research Center  Yorktown Heights NY   \n97                       General Electric Company       Schenectady NY   \n..                                            ...                  ...   \n928  California Institute of Technology (Caltech)          Pasadena CA   \n929                        University of Missouri             Columbia   \n930           MRC Laboratory of Molecular Biology            Cambridge   \n933                               Yale University         New Haven CT   \n934                  NYU Stern School of Business          New York NY   \n\n            country            born_date died_date  ... died_country_code  \\\n68              USA  1922-09-22 00:00:00       NaN  ...               NaN   \n69              USA  1926-11-24 00:00:00       NaN  ...               NaN   \n94              USA  1930-02-28 00:00:00       NaN  ...               NaN   \n96              USA  1925-03-12 00:00:00       NaN  ...               NaN   \n97              USA  1929-04-05 00:00:00       NaN  ...               NaN   \n..              ...                  ...       ...  ...               ...   \n928             USA  1956-07-25 00:00:00       NaN  ...               NaN   \n929             USA  1941-03-10 00:00:00       NaN  ...               NaN   \n930  United Kingdom  1951-04-14 00:00:00       NaN  ...               NaN   \n933             USA  1941-05-31 00:00:00       NaN  ...               NaN   \n934             USA                  NaN       NaN  ...               NaN   \n\n    overall_motivation share  \\\n68                 NaN     2   \n69                 NaN     2   \n94                 NaN     3   \n96                 NaN     4   \n97                 NaN     4   \n..                 ...   ...   \n928                NaN     2   \n929                NaN     4   \n930                NaN     4   \n933                NaN     2   \n934                NaN     2   \n\n                                            motivation born_country_original  \\\n68   \"for their penetrating investigation of the so...                 China   \n69   \"for their penetrating investigation of the so...                 China   \n94   \"for their jointly developed theory of superco...                   USA   \n96   \"for their experimental discoveries regarding ...                 Japan   \n97   \"for their experimental discoveries regarding ...                Norway   \n..                                                 ...                   ...   \n928            \"for the directed evolution of enzymes\"                   USA   \n929  \"for the phage display of peptides and antibod...                   USA   \n930  \"for the phage display of peptides and antibod...        United Kingdom   \n933  \"for integrating climate change into long-run ...                   USA   \n934  \"for integrating technological innovations int...                   USA   \n\n    born_city_original died_country_original died_city_original  \\\n68        Hofei Anhwei                   NaN                NaN   \n69            Shanghai                   NaN                NaN   \n94         New York NY                   NaN                NaN   \n96               Osaka                   NaN                NaN   \n97              Bergen                   NaN                NaN   \n..                 ...                   ...                ...   \n928      Pittsburgh PA                   NaN                NaN   \n929         Norwalk CT                   NaN                NaN   \n930          Leicester                   NaN                NaN   \n933     Albuquerque NM                   NaN                NaN   \n934          Denver CO                   NaN                NaN   \n\n           city_original country_original  \n68          Princeton NJ              USA  \n69           New York NY              USA  \n94         Providence RI              USA  \n96   Yorktown Heights NY              USA  \n97        Schenectady NY              USA  \n..                   ...              ...  \n928          Pasadena CA              USA  \n929             Columbia              USA  \n930            Cambridge   United Kingdom  \n933         New Haven CT              USA  \n934          New York NY              USA  \n\n[228 rows x 26 columns]\n\n\n\nprint(nobel_living.shape[0])\n\n228\n\n\n\nx,y = df['country'].value_counts().idxmax(), df['country'].value_counts().max()\nprint(x,y)\n\nUSA 350"
  },
  {
    "objectID": "homework/Lecture4_SuYibo_Homework.html",
    "href": "homework/Lecture4_SuYibo_Homework.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nallages = pd.read_csv('/Users/apple/Documents/GitHub/SuYibo_Homework/all-ages.csv')\ngardstu = pd.read_csv('/Users/apple/Documents/GitHub/SuYibo_Homework/all-ages.csv')\nmajor_list = pd.read_csv('/Users/apple/Documents/GitHub/SuYibo_Homework/all-ages.csv')\nrecgards = pd.read_csv('/Users/apple/Documents/GitHub/SuYibo_Homework/all-ages.csv')\nwomenstem = pd.read_csv('/Users/apple/Documents/GitHub/SuYibo_Homework/all-ages.csv')\n\n\n#Which major has the lowest unemployment rate\n# Sort the data by the unemployment rate in ascending order\ndf_sorted = allages.sort_values(by='Unemployment_rate')\n\n# Find the major with the lowest unemployment rate\nlowest_unemployment_major = df_sorted.head(1)\n\n# Print the result\nprint(\"Major with the lowest unemployment rate:\")\nprint(lowest_unemployment_major[['Major', 'Unemployment_rate']])\n\nMajor with the lowest unemployment rate:\n                                         Major  Unemployment_rate\n26  EDUCATIONAL ADMINISTRATION AND SUPERVISION                0.0\n\n\n\n#Which major has the highest percentage of women\n# Calculate the proportion of women for each major\nwomenstem['Proportion_of_Women'] = womenstem['Women'] / womenstem['Total']\n\n# Sort the data in descending order with respect to the proportion of women\ndf_sorted = womenstem.sort_values(by='Proportion_of_Women', ascending=False)\n\n# Display only the top 3 majors\ntop_3_women_majors = df_sorted.head(3)[['Major', 'Total', 'Proportion_of_Women']]\n\n# Print the result\nprint(\"Top 3 majors with the highest percentage of women:\")\nprint(top_3_women_majors)\n\nTop 3 majors with the highest percentage of women:\n                                            Major   Total  Proportion_of_Women\n74  COMMUNICATION DISORDERS SCIENCES AND SERVICES   38279             0.967998\n40                     MEDICAL ASSISTING SERVICES   11123             0.927807\n26                                        NURSING  209394             0.896019\n\n\n#How do the distributions of median income compare across major categories ’‘’The median is often chosen over the mean because it is less affected by outliers or extreme values in the data set. The mean can be skewed by very high or very low incomes, while the median represents the middle value, providing a more robust measure of central tendency for skewed distributions.’’’\n\n# Calculate bin edges\nmin_income = allages['Median'].min()\nmax_income = allages['Median'].max()\nbin_edges = np.arange(min_income, max_income + 1000, 1000)  # Adjust the step size as needed\n\n# Plot the histogram with specified bin edges\nplt.figure(figsize=(12, 6))\nplt.hist(allages['Median'], bins=bin_edges, edgecolor='black')\nplt.title('Histogram of Median Income (Binwidth = $1000)')\nplt.xlabel('Median Income')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Plot the distribution of median income by major category with a bin width of $1000\nsns.histplot(data=allages, x='Median', hue='Major_category', binwidth=1000, element='step', stat='count', common_norm=False)\nplt.title('Distribution of Median Income by Major Category')\nplt.xlabel('Median Income')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n\n\n#All STEM fields aren't the same\nstem_categories = [\n    'Engineering', 'Computers & Mathematics', 'Biology & Life Science', \n    'Physical Sciences', 'Agriculture & Natural Resources'\n]\n\nallages['Is_STEM'] = allages['Major_category'].apply(lambda x: x in stem_categories)\n\noverall_median_income = allages['Median'].median()\n\nstem_majors_below_median = allages[(allages['Is_STEM']) & (allages['Median'] &lt;= overall_median_income)]\n\nresult = stem_majors_below_median[['Major', 'Median', 'P25th', 'P75th']].sort_values(by='Median', ascending=False)\n\nprint(result)\n\n                                  Major  Median  P25th    P75th\n79                 BIOCHEMICAL SCIENCES   53000  33000  82000.0\n98  COGNITIVE SCIENCE AND BIOPSYCHOLOGY   53000  31500  93000.0\n7             MISCELLANEOUS AGRICULTURE   52000  35000  75000.0\n8                 ENVIRONMENTAL SCIENCE   52000  38000  75000.0\n10         NATURAL RESOURCES MANAGEMENT   52000  37100  75000.0\n89                MISCELLANEOUS BIOLOGY   52000  33500  72800.0\n78                              BIOLOGY   51000  35000  80000.0\n5            PLANT SCIENCE AND AGRONOMY   50000  35000  75000.0\n17           COMMUNICATION TECHNOLOGIES   50000  34500  75000.0\n80                               BOTANY   50000  32000  75000.0\n86                           PHYSIOLOGY   50000  30000  75000.0\n0                   GENERAL AGRICULTURE   50000  34000  80000.0\n83                             GENETICS   48000  33000  80000.0\n82                              ECOLOGY   47500  32000  73000.0\n3                       ANIMAL SCIENCES   46000  30000  72000.0\n81                    MOLECULAR BIOLOGY   45000  30000  70000.0\n88                         NEUROSCIENCE   35000  28000  52000.0\n\n\n\n# Define STEM categories\nstem_categories = [\n    'Engineering', 'Computers & Mathematics', 'Biology & Life Science', \n    'Physical Sciences', 'Agriculture & Natural Resources'\n]\n\n# Create a new column to indicate if the major is STEM\nallages['Is_STEM'] = allages['Major_category'].apply(lambda x: 'Yes' if x in stem_categories else 'No')\n\n# Calculate the proportion of women for each major\nallages['Proportion_of_Women'] = allages['Employed'] / allages['Total']\n\n# Create a scatterplot\nplt.figure(figsize=(10, 8))\nsns.scatterplot(x='Median', y='Proportion_of_Women', hue='Is_STEM', style='Is_STEM', data=allages, palette='coolwarm', s=100)\n\n# Add regression line if desired\nsns.regplot(x='Median', y='Proportion_of_Women', scatter=False, data=allages, color='gray')\n\n# Add titles and labels\nplt.title('Median Income vs. Proportion of Women by STEM Field')\nplt.xlabel('Median Income')\nplt.ylabel('Proportion of Women')\nplt.legend(title='Is Major STEM?', labels=['No', 'Yes'], loc='upper right', bbox_to_anchor=(1.05, 1), frameon=False)\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "Lecture2",
    "section": "",
    "text": "Lecture2\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndf = pd.read_csv('/Users/apple/Desktop/archive/seattle_pet_licenses.csv')\n\n\nprint(df)\n\n      animal_s_name       license_issue_date  license_number  \\\n0              Ozzy  2005-03-29T00:00:00.000        130651.0   \n1              Jack  2009-12-23T00:00:00.000        898148.0   \n2            Ginger  2006-01-20T00:00:00.000         29654.0   \n3            Pepper  2006-02-07T00:00:00.000         75432.0   \n4              Addy  2006-08-04T00:00:00.000        729899.0   \n...             ...                      ...             ...   \n66037          Lily  2016-12-27T00:00:00.000             NaN   \n66038         Ellie  2016-11-29T00:00:00.000             NaN   \n66039         Sammy  2016-12-05T00:00:00.000             NaN   \n66040         Buddy  2016-12-06T00:00:00.000             NaN   \n66041           Aku  2016-12-07T00:00:00.000             NaN   \n\n                           primary_breed      secondary_breed species zip_code  \n0      Dachshund, Standard Smooth Haired                  NaN     Dog    98104  \n1                   Schnauzer, Miniature         Terrier, Rat     Dog    98107  \n2                      Retriever, Golden  Retriever, Labrador     Dog    98117  \n3                                   Manx                  Mix     Cat    98103  \n4                      Retriever, Golden                  NaN     Dog    98105  \n...                                  ...                  ...     ...      ...  \n66037                 Domestic Shorthair                  Mix     Cat    98117  \n66038                    German Shepherd                  Mix     Dog    98105  \n66039                            Terrier              Maltese     Dog    98105  \n66040                        Bullmastiff                  Mix     Dog    98105  \n66041              Chihuahua, Short Coat              Terrier     Dog    98106  \n\n[66042 rows x 7 columns]\n\n\n\ninfor_df = df.info()\n\nprint(infor_df)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 66042 entries, 0 to 66041\nData columns (total 7 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   animal_s_name       64685 non-null  object \n 1   license_issue_date  66042 non-null  object \n 2   license_number      43885 non-null  float64\n 3   primary_breed       66042 non-null  object \n 4   secondary_breed     22538 non-null  object \n 5   species             66042 non-null  object \n 6   zip_code            65884 non-null  object \ndtypes: float64(1), object(6)\nmemory usage: 3.5+ MB\nNone\n\n\n\ntop_three_ani = df['animal_s_name'].value_counts()\n\nprint(top_three_ani)\n\nanimal_s_name\nLucy       566\nBella      451\nCharlie    447\nMax        374\nLuna       361\n          ... \nFinigan      1\nLiame        1\nHitler       1\nRustie       1\nChino        1\nName: count, Length: 15795, dtype: int64\n\n\n\nf, ax = plt.subplots(1, 2, figsize=(20, 7))\ntop_three_ani = df['animal_s_name'].value_counts()\n\nd = top_three_ani[0:3].plot(kind='bar', \n                         ax=ax[0], \n                         title='Top tnree ani names')\n\nd.set_ylabel('Count')\n\nplt.show()\n\n\n\n\n\n\n\n\nSource: Lecture2_SuYibo_Homework.ipynb\n\n\nLecture3\n\n\nimport pandas as pd \n\nread_file = pd.read_excel (\"/D:\\360MoveData\\Users\\75903\\Desktop\\SuYibo-Homework\\Lecture3 SuYibo Homework/nobel.xlsx\") \n\nread_file.to_csv (\"nobel.csv\", \n                index = None, \n                header=True) \n    \n#read nobel.csv\ndf = pd.DataFrame(pd.read_csv(\"nobel.csv\")) \nprint(df)\n\n      id       firstname    surname  year   category  \\\n0      1  Wilhelm Conrad    Röntgen  1901    Physics   \n1      2      Hendrik A.    Lorentz  1902    Physics   \n2      3          Pieter     Zeeman  1902    Physics   \n3      4           Henri  Becquerel  1903    Physics   \n4      5          Pierre      Curie  1903    Physics   \n..   ...             ...        ...   ...        ...   \n930  965  Sir Gregory P.     Winter  2018  Chemistry   \n931  966           Denis    Mukwege  2018      Peace   \n932  967           Nadia      Murad  2018      Peace   \n933  968      William D.   Nordhaus  2018  Economics   \n934  969         Paul M.      Romer  2018  Economics   \n\n                                           affiliation          city  \\\n0                                    Munich University        Munich   \n1                                    Leiden University        Leiden   \n2                                 Amsterdam University     Amsterdam   \n3                                  École Polytechnique         Paris   \n4    École municipale de physique et de chimie indu...         Paris   \n..                                                 ...           ...   \n930                MRC Laboratory of Molecular Biology     Cambridge   \n931                                                NaN           NaN   \n932                                                NaN           NaN   \n933                                    Yale University  New Haven CT   \n934                       NYU Stern School of Business   New York NY   \n\n            country            born_date   died_date  ... died_country_code  \\\n0           Germany           1845-03-27  1923-02-10  ...                DE   \n1       Netherlands           1853-07-18  1928-02-04  ...                NL   \n2       Netherlands           1865-05-25  1943-10-09  ...                NL   \n3            France           1852-12-15  1908-08-25  ...                FR   \n4            France           1859-05-15  1906-04-19  ...                FR   \n..              ...                  ...         ...  ...               ...   \n930  United Kingdom  1951-04-14 00:00:00         NaN  ...               NaN   \n931             NaN  1955-03-01 00:00:00         NaN  ...               NaN   \n932             NaN                  NaN         NaN  ...               NaN   \n933             USA  1941-05-31 00:00:00         NaN  ...               NaN   \n934             USA                  NaN         NaN  ...               NaN   \n\n    overall_motivation share  \\\n0                  NaN     1   \n1                  NaN     2   \n2                  NaN     2   \n3                  NaN     2   \n4                  NaN     4   \n..                 ...   ...   \n930                NaN     4   \n931                NaN     2   \n932                NaN     2   \n933                NaN     2   \n934                NaN     2   \n\n                                            motivation  \\\n0    \"in recognition of the extraordinary services ...   \n1    \"in recognition of the extraordinary service t...   \n2    \"in recognition of the extraordinary service t...   \n3    \"in recognition of the extraordinary services ...   \n4    \"in recognition of the extraordinary services ...   \n..                                                 ...   \n930  \"for the phage display of peptides and antibod...   \n931  \"for their efforts to end the use of sexual vi...   \n932  \"for their efforts to end the use of sexual vi...   \n933  \"for integrating climate change into long-run ...   \n934  \"for integrating technological innovations int...   \n\n                                 born_country_original  \\\n0                                Prussia (now Germany)   \n1                                      the Netherlands   \n2                                      the Netherlands   \n3                                               France   \n4                                               France   \n..                                                 ...   \n930                                     United Kingdom   \n931  Belgian Congo (now Democratic Republic of the ...   \n932                                               Iraq   \n933                                                USA   \n934                                                USA   \n\n         born_city_original died_country_original died_city_original  \\\n0    Lennep (now Remscheid)               Germany             Munich   \n1                    Arnhem       the Netherlands                NaN   \n2                Zonnemaire       the Netherlands          Amsterdam   \n3                     Paris                France                NaN   \n4                     Paris                France              Paris   \n..                      ...                   ...                ...   \n930               Leicester                   NaN                NaN   \n931                  Bukavu                   NaN                NaN   \n932                    Kojo                   NaN                NaN   \n933          Albuquerque NM                   NaN                NaN   \n934               Denver CO                   NaN                NaN   \n\n     city_original country_original  \n0           Munich          Germany  \n1           Leiden  the Netherlands  \n2        Amsterdam  the Netherlands  \n3            Paris           France  \n4            Paris           France  \n..             ...              ...  \n930      Cambridge   United Kingdom  \n931            NaN              NaN  \n932            NaN              NaN  \n933   New Haven CT              USA  \n934    New York NY              USA  \n\n[935 rows x 26 columns]\n\n\n\ninfor = df.info()\nprint(infor)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 935 entries, 0 to 934\nData columns (total 26 columns):\n #   Column                 Non-Null Count  Dtype \n---  ------                 --------------  ----- \n 0   id                     935 non-null    int64 \n 1   firstname              935 non-null    object\n 2   surname                906 non-null    object\n 3   year                   935 non-null    int64 \n 4   category               935 non-null    object\n 5   affiliation            685 non-null    object\n 6   city                   680 non-null    object\n 7   country                681 non-null    object\n 8   born_date              902 non-null    object\n 9   died_date              627 non-null    object\n 10  gender                 935 non-null    object\n 11  born_city              907 non-null    object\n 12  born_country           907 non-null    object\n 13  born_country_code      907 non-null    object\n 14  died_city              608 non-null    object\n 15  died_country           614 non-null    object\n 16  died_country_code      614 non-null    object\n 17  overall_motivation     17 non-null     object\n 18  share                  935 non-null    int64 \n 19  motivation             935 non-null    object\n 20  born_country_original  907 non-null    object\n 21  born_city_original     907 non-null    object\n 22  died_country_original  614 non-null    object\n 23  died_city_original     608 non-null    object\n 24  city_original          680 non-null    object\n 25  country_original       681 non-null    object\ndtypes: int64(3), object(23)\nmemory usage: 190.1+ KB\nNone\n\n\n\nnobel_living = df[\n    df['country'].notna() &\n    (df['gender'] != 'org') &\n    df['died_date'].isna()        \n]\nprint(nobel_living)\n\n      id       firstname   surname  year   category  \\\n68    68       Chen Ning      Yang  1957    Physics   \n69    69       Tsung-Dao       Lee  1957    Physics   \n94    95         Leon N.    Cooper  1972    Physics   \n96    97             Leo     Esaki  1973    Physics   \n97    98            Ivar   Giaever  1973    Physics   \n..   ...             ...       ...   ...        ...   \n928  963      Frances H.    Arnold  2018  Chemistry   \n929  964       George P.     Smith  2018  Chemistry   \n930  965  Sir Gregory P.    Winter  2018  Chemistry   \n933  968      William D.  Nordhaus  2018  Economics   \n934  969         Paul M.     Romer  2018  Economics   \n\n                                      affiliation                 city  \\\n68                   Institute for Advanced Study         Princeton NJ   \n69                            Columbia University          New York NY   \n94                               Brown University        Providence RI   \n96           IBM Thomas J. Watson Research Center  Yorktown Heights NY   \n97                       General Electric Company       Schenectady NY   \n..                                            ...                  ...   \n928  California Institute of Technology (Caltech)          Pasadena CA   \n929                        University of Missouri             Columbia   \n930           MRC Laboratory of Molecular Biology            Cambridge   \n933                               Yale University         New Haven CT   \n934                  NYU Stern School of Business          New York NY   \n\n            country            born_date died_date  ... died_country_code  \\\n68              USA  1922-09-22 00:00:00       NaN  ...               NaN   \n69              USA  1926-11-24 00:00:00       NaN  ...               NaN   \n94              USA  1930-02-28 00:00:00       NaN  ...               NaN   \n96              USA  1925-03-12 00:00:00       NaN  ...               NaN   \n97              USA  1929-04-05 00:00:00       NaN  ...               NaN   \n..              ...                  ...       ...  ...               ...   \n928             USA  1956-07-25 00:00:00       NaN  ...               NaN   \n929             USA  1941-03-10 00:00:00       NaN  ...               NaN   \n930  United Kingdom  1951-04-14 00:00:00       NaN  ...               NaN   \n933             USA  1941-05-31 00:00:00       NaN  ...               NaN   \n934             USA                  NaN       NaN  ...               NaN   \n\n    overall_motivation share  \\\n68                 NaN     2   \n69                 NaN     2   \n94                 NaN     3   \n96                 NaN     4   \n97                 NaN     4   \n..                 ...   ...   \n928                NaN     2   \n929                NaN     4   \n930                NaN     4   \n933                NaN     2   \n934                NaN     2   \n\n                                            motivation born_country_original  \\\n68   \"for their penetrating investigation of the so...                 China   \n69   \"for their penetrating investigation of the so...                 China   \n94   \"for their jointly developed theory of superco...                   USA   \n96   \"for their experimental discoveries regarding ...                 Japan   \n97   \"for their experimental discoveries regarding ...                Norway   \n..                                                 ...                   ...   \n928            \"for the directed evolution of enzymes\"                   USA   \n929  \"for the phage display of peptides and antibod...                   USA   \n930  \"for the phage display of peptides and antibod...        United Kingdom   \n933  \"for integrating climate change into long-run ...                   USA   \n934  \"for integrating technological innovations int...                   USA   \n\n    born_city_original died_country_original died_city_original  \\\n68        Hofei Anhwei                   NaN                NaN   \n69            Shanghai                   NaN                NaN   \n94         New York NY                   NaN                NaN   \n96               Osaka                   NaN                NaN   \n97              Bergen                   NaN                NaN   \n..                 ...                   ...                ...   \n928      Pittsburgh PA                   NaN                NaN   \n929         Norwalk CT                   NaN                NaN   \n930          Leicester                   NaN                NaN   \n933     Albuquerque NM                   NaN                NaN   \n934          Denver CO                   NaN                NaN   \n\n           city_original country_original  \n68          Princeton NJ              USA  \n69           New York NY              USA  \n94         Providence RI              USA  \n96   Yorktown Heights NY              USA  \n97        Schenectady NY              USA  \n..                   ...              ...  \n928          Pasadena CA              USA  \n929             Columbia              USA  \n930            Cambridge   United Kingdom  \n933         New Haven CT              USA  \n934          New York NY              USA  \n\n[228 rows x 26 columns]\n\n\n\nprint(nobel_living.shape[0])\n\n228\n\n\n\nx,y = df['country'].value_counts().idxmax(), df['country'].value_counts().max()\nprint(x,y)\n\nUSA 350\n\n\nSource: Lecture3_SuYibo_Homework1.ipynb\n\n\nLecture3\n\n\nimport pandas as pd\nfrom skimpy import skim\ndf = pd.read_csv(\"shujuji2.csv\")\nskim(df)\n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types                                                                 │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n│ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                                                          │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n│ │ Number of rows    │ 891    │ │ int32       │ 5     │                                                          │\n│ │ Number of columns │ 12     │ │ string      │ 5     │                                                          │\n│ └───────────────────┴────────┘ │ float64     │ 2     │                                                          │\n│                                └─────────────┴───────┘                                                          │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┓  │\n│ ┃ column_name    ┃ NA   ┃ NA %    ┃ mean     ┃ sd       ┃ p0    ┃ p25    ┃ p50    ┃ p75    ┃ p100   ┃ hist   ┃  │\n│ ┡━━━━━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━┩  │\n│ │ PassengerId    │    0 │       0 │      446 │    257.4 │     1 │  223.5 │    446 │  668.5 │    891 │ ▇▇▇▇▇▇ │  │\n│ │ Survived       │    0 │       0 │   0.3838 │   0.4866 │     0 │      0 │      0 │      1 │      1 │ ▇    ▅ │  │\n│ │ Pclass         │    0 │       0 │    2.309 │   0.8361 │     1 │      2 │      3 │      3 │      3 │ ▃  ▃ ▇ │  │\n│ │ Age            │  177 │   19.87 │     29.7 │    14.53 │  0.42 │  20.12 │     28 │     38 │     80 │ ▂▇▇▃▁  │  │\n│ │ SibSp          │    0 │       0 │    0.523 │    1.103 │     0 │      0 │      0 │      1 │      8 │   ▇    │  │\n│ │ Parch          │    0 │       0 │   0.3816 │   0.8061 │     0 │      0 │      0 │      0 │      6 │  ▇▁▁   │  │\n│ │ Fare           │    0 │       0 │     32.2 │    49.69 │     0 │   7.91 │  14.45 │     31 │  512.3 │   ▇    │  │\n│ └────────────────┴──────┴─────────┴──────────┴──────────┴───────┴────────┴────────┴────────┴────────┴────────┘  │\n│                                                     string                                                      │\n│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓  │\n│ ┃ column_name              ┃ NA       ┃ NA %       ┃ words per row                ┃ total words              ┃  │\n│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩  │\n│ │ Name                     │        0 │          0 │                          4.1 │                     3626 │  │\n│ │ Sex                      │        0 │          0 │                            1 │                      891 │  │\n│ │ Ticket                   │        0 │          0 │                          1.3 │                     1130 │  │\n│ │ Cabin                    │      687 │       77.1 │                         0.27 │                      238 │  │\n│ │ Embarked                 │        2 │       0.22 │                            1 │                      889 │  │\n│ └──────────────────────────┴──────────┴────────────┴──────────────────────────────┴──────────────────────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n\n\n\n\nprint(df.head())\n\n   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  \n\n\n\nprint(df.describe())\n\n       PassengerId    Survived      Pclass         Age       SibSp  \\\ncount   891.000000  891.000000  891.000000  714.000000  891.000000   \nmean    446.000000    0.383838    2.308642   29.699118    0.523008   \nstd     257.353842    0.486592    0.836071   14.526497    1.102743   \nmin       1.000000    0.000000    1.000000    0.420000    0.000000   \n25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n75%     668.500000    1.000000    3.000000   38.000000    1.000000   \nmax     891.000000    1.000000    3.000000   80.000000    8.000000   \n\n            Parch        Fare  \ncount  891.000000  891.000000  \nmean     0.381594   32.204208  \nstd      0.806057   49.693429  \nmin      0.000000    0.000000  \n25%      0.000000    7.910400  \n50%      0.000000   14.454200  \n75%      0.000000   31.000000  \nmax      6.000000  512.329200  \n\n\n\nprint(df.isnull().sum())\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\n\n\nprint(df.dtypes)\n\nPassengerId      int64\nSurvived         int64\nPclass           int64\nName            object\nSex             object\nAge            float64\nSibSp            int64\nParch            int64\nTicket          object\nFare           float64\nCabin           object\nEmbarked        object\ndtype: object\n\n\n\nimport matplotlib.pyplot as plt\nsurvival_counts = df['Survived'].value_counts()\nsurvival_counts.plot(kind='bar')\nplt.title('Survival Counts')\nplt.xlabel('Survived')\nplt.ylabel('Count')\nplt.xticks(ticks=[0, 1], labels=['No', 'Yes'], rotation=0)\nplt.show()\n\n\n\n\n\n\n\n\n\ngender_survival = df.groupby(['Sex', 'Survived']).size().unstack()\ngender_survival.plot(kind='bar', stacked=True)\nplt.title('Survival by Gender')\nplt.xlabel('Gender')\nplt.ylabel('Count')\nplt.xticks(rotation=0)\nplt.show()\n\n\n\n\n\n\n\n\nSource: Lecture3_SuYibo_Homework2.ipynb\n\n\nLecture4\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nallages = pd.read_csv('/Users/apple/Documents/GitHub/SuYibo_Homework/all-ages.csv')\ngardstu = pd.read_csv('/Users/apple/Documents/GitHub/SuYibo_Homework/all-ages.csv')\nmajor_list = pd.read_csv('/Users/apple/Documents/GitHub/SuYibo_Homework/all-ages.csv')\nrecgards = pd.read_csv('/Users/apple/Documents/GitHub/SuYibo_Homework/all-ages.csv')\nwomenstem = pd.read_csv('/Users/apple/Documents/GitHub/SuYibo_Homework/all-ages.csv')\n\n\n#Which major has the lowest unemployment rate\n# Sort the data by the unemployment rate in ascending order\ndf_sorted = allages.sort_values(by='Unemployment_rate')\n\n# Find the major with the lowest unemployment rate\nlowest_unemployment_major = df_sorted.head(1)\n\n# Print the result\nprint(\"Major with the lowest unemployment rate:\")\nprint(lowest_unemployment_major[['Major', 'Unemployment_rate']])\n\nMajor with the lowest unemployment rate:\n                                         Major  Unemployment_rate\n26  EDUCATIONAL ADMINISTRATION AND SUPERVISION                0.0\n\n\n\n#Which major has the highest percentage of women\n# Calculate the proportion of women for each major\nwomenstem['Proportion_of_Women'] = womenstem['Women'] / womenstem['Total']\n\n# Sort the data in descending order with respect to the proportion of women\ndf_sorted = womenstem.sort_values(by='Proportion_of_Women', ascending=False)\n\n# Display only the top 3 majors\ntop_3_women_majors = df_sorted.head(3)[['Major', 'Total', 'Proportion_of_Women']]\n\n# Print the result\nprint(\"Top 3 majors with the highest percentage of women:\")\nprint(top_3_women_majors)\n\nTop 3 majors with the highest percentage of women:\n                                            Major   Total  Proportion_of_Women\n74  COMMUNICATION DISORDERS SCIENCES AND SERVICES   38279             0.967998\n40                     MEDICAL ASSISTING SERVICES   11123             0.927807\n26                                        NURSING  209394             0.896019\n\n\n#How do the distributions of median income compare across major categories ’‘’The median is often chosen over the mean because it is less affected by outliers or extreme values in the data set. The mean can be skewed by very high or very low incomes, while the median represents the middle value, providing a more robust measure of central tendency for skewed distributions.’’’\n\n# Calculate bin edges\nmin_income = allages['Median'].min()\nmax_income = allages['Median'].max()\nbin_edges = np.arange(min_income, max_income + 1000, 1000)  # Adjust the step size as needed\n\n# Plot the histogram with specified bin edges\nplt.figure(figsize=(12, 6))\nplt.hist(allages['Median'], bins=bin_edges, edgecolor='black')\nplt.title('Histogram of Median Income (Binwidth = $1000)')\nplt.xlabel('Median Income')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Plot the distribution of median income by major category with a bin width of $1000\nsns.histplot(data=allages, x='Median', hue='Major_category', binwidth=1000, element='step', stat='count', common_norm=False)\nplt.title('Distribution of Median Income by Major Category')\nplt.xlabel('Median Income')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n\n\n#All STEM fields aren't the same\nstem_categories = [\n    'Engineering', 'Computers & Mathematics', 'Biology & Life Science', \n    'Physical Sciences', 'Agriculture & Natural Resources'\n]\n\nallages['Is_STEM'] = allages['Major_category'].apply(lambda x: x in stem_categories)\n\noverall_median_income = allages['Median'].median()\n\nstem_majors_below_median = allages[(allages['Is_STEM']) & (allages['Median'] &lt;= overall_median_income)]\n\nresult = stem_majors_below_median[['Major', 'Median', 'P25th', 'P75th']].sort_values(by='Median', ascending=False)\n\nprint(result)\n\n                                  Major  Median  P25th    P75th\n79                 BIOCHEMICAL SCIENCES   53000  33000  82000.0\n98  COGNITIVE SCIENCE AND BIOPSYCHOLOGY   53000  31500  93000.0\n7             MISCELLANEOUS AGRICULTURE   52000  35000  75000.0\n8                 ENVIRONMENTAL SCIENCE   52000  38000  75000.0\n10         NATURAL RESOURCES MANAGEMENT   52000  37100  75000.0\n89                MISCELLANEOUS BIOLOGY   52000  33500  72800.0\n78                              BIOLOGY   51000  35000  80000.0\n5            PLANT SCIENCE AND AGRONOMY   50000  35000  75000.0\n17           COMMUNICATION TECHNOLOGIES   50000  34500  75000.0\n80                               BOTANY   50000  32000  75000.0\n86                           PHYSIOLOGY   50000  30000  75000.0\n0                   GENERAL AGRICULTURE   50000  34000  80000.0\n83                             GENETICS   48000  33000  80000.0\n82                              ECOLOGY   47500  32000  73000.0\n3                       ANIMAL SCIENCES   46000  30000  72000.0\n81                    MOLECULAR BIOLOGY   45000  30000  70000.0\n88                         NEUROSCIENCE   35000  28000  52000.0\n\n\n\n# Define STEM categories\nstem_categories = [\n    'Engineering', 'Computers & Mathematics', 'Biology & Life Science', \n    'Physical Sciences', 'Agriculture & Natural Resources'\n]\n\n# Create a new column to indicate if the major is STEM\nallages['Is_STEM'] = allages['Major_category'].apply(lambda x: 'Yes' if x in stem_categories else 'No')\n\n# Calculate the proportion of women for each major\nallages['Proportion_of_Women'] = allages['Employed'] / allages['Total']\n\n# Create a scatterplot\nplt.figure(figsize=(10, 8))\nsns.scatterplot(x='Median', y='Proportion_of_Women', hue='Is_STEM', style='Is_STEM', data=allages, palette='coolwarm', s=100)\n\n# Add regression line if desired\nsns.regplot(x='Median', y='Proportion_of_Women', scatter=False, data=allages, color='gray')\n\n# Add titles and labels\nplt.title('Median Income vs. Proportion of Women by STEM Field')\nplt.xlabel('Median Income')\nplt.ylabel('Proportion of Women')\nplt.legend(title='Is Major STEM?', labels=['No', 'Yes'], loc='upper right', bbox_to_anchor=(1.05, 1), frameon=False)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nSource: Sort the data by the unemployment rate in ascending order\n\n\nLecture5\n\n\nimport pandas as pd\nfrom lets_plot import *\n\nLetsPlot.setup_html()\n\n\n            \n            \n            \n\n\n\ndf = pd.read_csv('/D:\\360MoveData\\Users\\75903\\Desktop\\SuYibo-Homework/Lecture5 SuYibo Homework/plastic-waste.csv')\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 240 entries, 0 to 239\nData columns (total 10 columns):\n #   Column                            Non-Null Count  Dtype  \n---  ------                            --------------  -----  \n 0   code                              240 non-null    object \n 1   entity                            240 non-null    object \n 2   continent                         240 non-null    object \n 3   year                              240 non-null    int64  \n 4   gdp_per_cap                       195 non-null    float64\n 5   plastic_waste_per_cap             189 non-null    float64\n 6   mismanaged_plastic_waste_per_cap  189 non-null    float64\n 7   mismanaged_plastic_waste          189 non-null    float64\n 8   coastal_pop                       189 non-null    float64\n 9   total_pop                         230 non-null    float64\ndtypes: float64(6), int64(1), object(3)\nmemory usage: 18.9+ KB\n\n\n\n#Plot, using histograms, the distribution of plastic waste per capita faceted by continent. What can you say about how the continents compare to each other in terms of their plastic waste per capita?\ndf['plastic_waste_per_cap'] = pd.to_numeric(df['plastic_waste_per_cap'], errors='coerce')\ndf.dropna(subset=['plastic_waste_per_cap'], inplace=True)\n\n\n\n(\n    ggplot(df, aes(x='plastic_waste_per_cap')) + \\\n        geom_histogram(binwidth=0.05, fill='blue', color='black', alpha=0.7) + \\\n        facet_wrap('continent', scales='free_y') + \\\n        labs(x='Plastic Waste per Capita (kg)', y='Frequency', title='Distribution of Plastic Waste per Capita by Continent') + \\\n        theme_minimal()\n    \n)\n\n   \n   \n\n\n\n#Convert your side-by-side box plots from the previous task to violin plots. What do the violin plots reveal that box plots do not? What features are apparent in the box plots but not in the violin plots?\ndf['plastic_waste_per_cap'] = pd.to_numeric(df['plastic_waste_per_cap'], errors='coerce')\ndf.dropna(subset=['plastic_waste_per_cap'], inplace=True)\n\n\n\n(\n    ggplot(df, aes(x='continent', y='plastic_waste_per_cap')) + \\\n        geom_violin(trim=False, fill='lightblue', color='black') + \\\n        facet_wrap('continent', scales='free_y') + \\\n        labs(x='Continent', y='Plastic Waste per Capita (kg)', title='Distribution of Plastic Waste per Capita by Continent') + \\\n        theme_minimal()\n)\n\n   \n   \n\n\n\n#Visualize the relationship between plastic waste per capita and mismanaged plastic waste per capita using a scatterplot. Describe the relationship.\ndf['plastic_waste_per_cap'] = pd.to_numeric(df['plastic_waste_per_cap'], errors='coerce')\ndf['mismanaged_plastic_waste_per_cap'] = pd.to_numeric(df['mismanaged_plastic_waste_per_cap'], errors='coerce')\ndf.dropna(subset=['plastic_waste_per_cap', 'mismanaged_plastic_waste_per_cap'], inplace=True)\n\n\n\n(\n    ggplot(df, aes(x='plastic_waste_per_cap', y='mismanaged_plastic_waste_per_cap')) + \\\n        geom_point(alpha=0.7, color='blue') + \\\n        labs(x='Plastic Waste per Capita (kg)', y='Mismanaged Plastic Waste per Capita (kg)', title='Relationship between Plastic Waste and Mismanaged Plastic Waste per Capita') + \\\n        theme_minimal()\n)\n\n   \n   \n\n\n\n#Colour the points in the scatterplot by continent. Does there seem to be any clear distinctions between continents with respect to how plastic waste per capita and mismanaged plastic waste per capita are associated?\ndf['plastic_waste_per_cap'] = pd.to_numeric(df['plastic_waste_per_cap'], errors='coerce')\ndf['mismanaged_plastic_waste_per_cap'] = pd.to_numeric(df['mismanaged_plastic_waste_per_cap'], errors='coerce')\ndf.dropna(subset=['plastic_waste_per_cap', 'mismanaged_plastic_waste_per_cap'], inplace=True)\n\n\n\n(\n    ggplot(df, aes(x='plastic_waste_per_cap', y='mismanaged_plastic_waste_per_cap', color='continent')) + \\\n        geom_point(alpha=0.7) + \\\n        labs(x='Plastic Waste per Capita (kg)', y='Mismanaged Plastic Waste per Capita (kg)', title='Relationship between Plastic Waste and Mismanaged Plastic Waste per Capita by Continent') + \\\n        theme_minimal() + \\\n        scale_color_discrete(name='Continent') + \\\n        guides(color=guide_legend(title='Continent'))\n\n)\n\n   \n   \n\n\n\n#Visualize the relationship between plastic waste per capita and total population as well as plastic waste per capita and coastal population. You will need to make two separate plots. Do either of these pairs of variables appear to be more strongly linearly associated?\ndf['plastic_waste_per_cap'] = pd.to_numeric(df['plastic_waste_per_cap'], errors='coerce')\ndf['total_pop'] = pd.to_numeric(df['total_pop'], errors='coerce')\ndf['coastal_pop'] = pd.to_numeric(df['coastal_pop'], errors='coerce')\ndf.dropna(subset=['plastic_waste_per_cap', 'total_pop', 'coastal_pop'], inplace=True)\n\n\n\n(\n    ggplot(df, aes(x='coastal_pop', y='plastic_waste_per_cap')) + \\\n        geom_point(alpha=0.7) + \\\n        labs(x='Coastal Population', y='Plastic Waste per Capita (kg)', title='Relationship between Plastic Waste per Capita and Coastal Population') + \\\n        theme_minimal()\n)\n\n   \n   \n\n\n\n#Recreate the following plot, and interpret what you see in context of the data.\n\ndf['plastic_waste_per_cap'] = pd.to_numeric(df['plastic_waste_per_cap'], errors='coerce')\ndf['total_pop'] = pd.to_numeric(df['total_pop'], errors='coerce')\ndf['coastal_pop'] = pd.to_numeric(df['coastal_pop'], errors='coerce')\ndf.dropna(subset=['plastic_waste_per_cap', 'total_pop', 'coastal_pop'], inplace=True)\n\n# Calculate coastal population proportion\ndf['coastal_pop_prop'] = df['coastal_pop'] / df['total_pop']\n\n\n\n(\n    ggplot(df, aes(x='coastal_pop_prop', y='plastic_waste_per_cap', color='continent')) + \n        geom_point(alpha=0.7) + \n        geom_smooth(method='loess', se=True, color='black') + \n        labs(x='Coastal population proportion (Coastal / total population)', y='Plastic waste per capita', title='Plastic waste vs. coastal population proportion by continent') + \\\n        theme_minimal() + \n        scale_color_discrete(name='Continent')\n)\n\n   \n   \n\n\nSource: Calculate coastal population proportion"
  },
  {
    "objectID": "labs/Labexercises/01Chipotle-Exercises-with-solutions.html",
    "href": "labs/Labexercises/01Chipotle-Exercises-with-solutions.html",
    "title": "Ex2 - Getting and Knowing your Data",
    "section": "",
    "text": "This time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\nimport numpy as np\n\n\n\nStep 2. Import the dataset from this address.\n\nchipo = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv', sep= '\\t')\n\n\n\nStep 3. Assign it to a variable called chipo.\n\nchipo = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv', sep= '\\t')\n\n\n\nStep 4. See the first 10 entries\n\nchipo.head(10)\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nchoice_description\nitem_price\n\n\n\n\n0\n1\n1\nChips and Fresh Tomato Salsa\nNaN\n$2.39\n\n\n1\n1\n1\nIzze\n[Clementine]\n$3.39\n\n\n2\n1\n1\nNantucket Nectar\n[Apple]\n$3.39\n\n\n3\n1\n1\nChips and Tomatillo-Green Chili Salsa\nNaN\n$2.39\n\n\n4\n2\n2\nChicken Bowl\n[Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n$16.98\n\n\n5\n3\n1\nChicken Bowl\n[Fresh Tomato Salsa (Mild), [Rice, Cheese, Sou...\n$10.98\n\n\n6\n3\n1\nSide of Chips\nNaN\n$1.69\n\n\n7\n4\n1\nSteak Burrito\n[Tomatillo Red Chili Salsa, [Fajita Vegetables...\n$11.75\n\n\n8\n4\n1\nSteak Soft Tacos\n[Tomatillo Green Chili Salsa, [Pinto Beans, Ch...\n$9.25\n\n\n9\n5\n1\nSteak Burrito\n[Fresh Tomato Salsa, [Rice, Black Beans, Pinto...\n$9.25\n\n\n\n\n\n\n\n\n\nStep 5. What is the number of observations in the dataset?\n\n# Solution 1\n\nchipo.shape\n\n(4622, 5)\n\n\n\n# Solution 2\n\nchipo.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4622 entries, 0 to 4621\nData columns (total 5 columns):\n #   Column              Non-Null Count  Dtype \n---  ------              --------------  ----- \n 0   order_id            4622 non-null   int64 \n 1   quantity            4622 non-null   int64 \n 2   item_name           4622 non-null   object\n 3   choice_description  3376 non-null   object\n 4   item_price          4622 non-null   object\ndtypes: int64(2), object(3)\nmemory usage: 180.7+ KB\n\n\n\n\nStep 6. What is the number of columns in the dataset?\n\nchipo.shape[1]\n\n5\n\n\n\n\nStep 7. Print the name of all the columns.\n\nchipo.columns\n\nIndex(['order_id', 'quantity', 'item_name', 'choice_description',\n       'item_price'],\n      dtype='object')\n\n\n\n\nStep 8. How is the dataset indexed?\n\nchipo.index\n\nRangeIndex(start=0, stop=4622, step=1)\n\n\n\n\nStep 9. Which was the most-ordered item?\n\nchipo.groupby(by=\"item_name\").sum().sort_values('quantity',ascending=False).head(1)\n\n\n\n\n\n\n\n\norder_id\nquantity\nchoice_description\nitem_price\n\n\nitem_name\n\n\n\n\n\n\n\n\nChicken Bowl\n713926\n761\n[Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n$16.98 $10.98 $11.25 $8.75 $8.49 $11.25 $8.75 ...\n\n\n\n\n\n\n\n\n\nStep 10. For the most-ordered item, how many items were ordered?\n\nchipo.groupby(by=\"item_name\").sum().sort_values('quantity',ascending=False).head(1)\n\n\n\n\n\n\n\n\norder_id\nquantity\nchoice_description\nitem_price\n\n\nitem_name\n\n\n\n\n\n\n\n\nChicken Bowl\n713926\n761\n[Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n$16.98 $10.98 $11.25 $8.75 $8.49 $11.25 $8.75 ...\n\n\n\n\n\n\n\n\n\nStep 11. What was the most ordered item in the choice_description column?\n\nchipo.groupby(by=\"choice_description\").sum().sort_values('quantity',ascending=False).head(1)\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nitem_price\n\n\nchoice_description\n\n\n\n\n\n\n\n\n[Diet Coke]\n123455\n159\nCanned SodaCanned SodaCanned Soda6 Pack Soft D...\n$2.18 $1.09 $1.09 $6.49 $2.18 $1.25 $1.09 $6.4...\n\n\n\n\n\n\n\n\n\nStep 12. How many items were orderd in total?\n\nchipo.item_name.count()\n\n4622\n\n\n\n\nStep 13. Turn the item price into a float\n\nStep 13.a. Check the item price type\n\nchipo.item_price.dtype\n\ndtype('O')\n\n\n\n\nStep 13.b. Create a lambda function and change the type of item price\n\ndollarizer = lambda x: float(x[1:-1])\nchipo.item_price = chipo.item_price.apply(dollarizer)\n\n\n\nStep 13.c. Check the item price type\n\nchipo.item_price.dtype\n\ndtype('float64')\n\n\n\n\n\nStep 14. How much was the revenue for the period in the dataset?\n\nrevenue =  (chipo.item_price * chipo.quantity).sum()\nprint('Revenue is : $ '+ str(revenue))\n\nRevenue is : $ 39237.02\n\n\n\n\nStep 15. How many orders were made in the period?\n\nchipo.order_id.value_counts().count()\n\n1834\n\n\n\n\nStep 16. What is the average revenue amount per order?\n\n# Solution 1\nchipo['revenue'] = chipo['quantity'] * chipo['item_price']\norder_grouped = chipo.groupby(by=['order_id']).sum()\norder_grouped['revenue'].mean()\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[8], line 4\n      2 chipo['revenue'] = chipo['quantity'] * chipo['item_price']\n      3 order_grouped = chipo.groupby(by=['order_id']).sum()\n----&gt; 4 order_grouped['revenue'].mean()\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:6549, in Series.mean(self, axis, skipna, numeric_only, **kwargs)\n   6541 @doc(make_doc(\"mean\", ndim=1))\n   6542 def mean(\n   6543     self,\n   (...)\n   6547     **kwargs,\n   6548 ):\n-&gt; 6549     return NDFrame.mean(self, axis, skipna, numeric_only, **kwargs)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:12420, in NDFrame.mean(self, axis, skipna, numeric_only, **kwargs)\n  12413 def mean(\n  12414     self,\n  12415     axis: Axis | None = 0,\n   (...)\n  12418     **kwargs,\n  12419 ) -&gt; Series | float:\n&gt; 12420     return self._stat_function(\n  12421         \"mean\", nanops.nanmean, axis, skipna, numeric_only, **kwargs\n  12422     )\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:12377, in NDFrame._stat_function(self, name, func, axis, skipna, numeric_only, **kwargs)\n  12373 nv.validate_func(name, (), kwargs)\n  12375 validate_bool_kwarg(skipna, \"skipna\", none_allowed=False)\n&gt; 12377 return self._reduce(\n  12378     func, name=name, axis=axis, skipna=skipna, numeric_only=numeric_only\n  12379 )\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:6457, in Series._reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\n   6452     # GH#47500 - change to TypeError to match other methods\n   6453     raise TypeError(\n   6454         f\"Series.{name} does not allow {kwd_name}={numeric_only} \"\n   6455         \"with non-numeric dtypes.\"\n   6456     )\n-&gt; 6457 return op(delegate, skipna=skipna, **kwds)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:147, in bottleneck_switch.__call__.&lt;locals&gt;.f(values, axis, skipna, **kwds)\n    145         result = alt(values, axis=axis, skipna=skipna, **kwds)\n    146 else:\n--&gt; 147     result = alt(values, axis=axis, skipna=skipna, **kwds)\n    149 return result\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:404, in _datetimelike_compat.&lt;locals&gt;.new_func(values, axis, skipna, mask, **kwargs)\n    401 if datetimelike and mask is None:\n    402     mask = isna(values)\n--&gt; 404 result = func(values, axis=axis, skipna=skipna, mask=mask, **kwargs)\n    406 if datetimelike:\n    407     result = _wrap_results(result, orig_values.dtype, fill_value=iNaT)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:720, in nanmean(values, axis, skipna, mask)\n    718 count = _get_counts(values.shape, mask, axis, dtype=dtype_count)\n    719 the_sum = values.sum(axis, dtype=dtype_sum)\n--&gt; 720 the_sum = _ensure_numeric(the_sum)\n    722 if axis is not None and getattr(the_sum, \"ndim\", False):\n    723     count = cast(np.ndarray, count)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:1701, in _ensure_numeric(x)\n   1698 elif not (is_float(x) or is_integer(x) or is_complex(x)):\n   1699     if isinstance(x, str):\n   1700         # GH#44008, GH#36703 avoid casting e.g. strings to numeric\n-&gt; 1701         raise TypeError(f\"Could not convert string '{x}' to numeric\")\n   1702     try:\n   1703         x = float(x)\n\nTypeError: Could not convert string '$2.39 $3.39 $3.39 $2.39 $16.98 $16.98 $10.98 $1.69 $11.75 $9.25 $9.25 $4.45 $8.75 $8.75 $11.25 $4.45 $2.39 $8.49 $8.49 $2.18 $2.18 $8.75 $4.45 $8.99 $3.39 $10.98 $3.39 $2.39 $8.49 $8.99 $1.09 $8.49 $2.39 $8.99 $1.69 $8.99 $1.09 $8.75 $8.75 $4.45 $2.95 $11.75 $2.15 $4.45 $11.25 $11.75 $8.75 $10.98 $8.99 $3.39 $8.99 $3.99 $8.99 $2.18 $2.18 $10.98 $1.09 $8.99 $2.39 $9.25 $11.25 $11.75 $2.15 $4.45 $9.25 $11.25 $8.75 $8.99 $8.99 $3.39 $8.99 $10.98 $8.99 $1.69 $8.99 $3.99 $8.75 $4.45 $8.75 $8.75 $2.15 $8.75 $11.25 $2.15 $9.25 $8.75 $8.75 $9.25 $8.49 $8.99 $1.09 $9.25 $2.95 $11.75 $11.75 $9.25 $11.75 $4.45 $9.25 $4.45 $11.75 $8.75 $8.75 $4.45 $8.99 $8.99 $3.99 $8.49 $3.39 $8.99 $1.09 $9.25 $4.45 $8.75 $2.95 $4.45 $2.39 $8.49 $8.99 $8.49 $1.09 $8.99 $3.99 $8.75 $9.25 $4.45 $11.25 $4.45 $8.99 $1.09 $9.25 $2.95 $4.45 $11.75 $4.45 $8.49 $2.39 $10.98 $22.50 $22.50 $11.75 $4.45 $11.25 $4.45 $11.25 $4.45 $11.25 $11.25 $11.75 $9.25 $4.45 $11.48 $17.98 $17.98 $1.69 $17.50 $17.50 $4.45 $8.49 $2.39 $17.50 $17.50 $4.45 $4.45 $11.25 $11.75 $10.98 $8.49 $10.98 $2.18 $2.18 $11.48 $8.49 $2.39 $4.45 $11.25 $11.75 $8.75 $8.49 $2.18 $2.18 $8.49 $3.39 $8.49 $8.99 $10.98 $11.48 $8.49 $1.09 $1.09 $9.25 $8.75 $2.95 $9.25 $4.45 $11.25 $11.48 $8.49 $8.49 $8.99 $2.39 $11.25 $8.75 $2.95 $1.09 $8.99 $8.49 $2.39 $10.98 $1.09 $3.99 $11.25 $8.75 $8.49 $3.39 $8.75 $9.25 $2.15 $11.25 $11.25 $11.25 $4.45 $22.50 $22.50 $4.45 $11.75 $8.75 $17.50 $17.50 $8.75 $9.25 $8.75 $2.15 $9.25 $4.30 $4.30 $8.75 $11.25 $2.15 $8.99 $1.09 $8.99 $3.99 $8.75 $2.95 $2.95 $11.75 $5.90 $5.90 $9.25 $9.25 $11.75 $9.25 $2.95 $17.50 $17.50 $8.75 $9.25 $10.98 $8.99 $1.09 $1.09 $1.09 $8.99 $10.98 $1.09 $8.75 $8.75 $9.25 $9.25 $8.75 $8.75 $8.99 $8.99 $8.99 $1.09 $11.75 $1.25 $8.99 $2.39 $9.25 $2.95 $8.99 $3.99 $8.49 $2.39 $8.49 $8.49 $8.49 $1.69 $8.49 $3.99 $8.99 $1.69 $1.09 $23.78 $23.78 $17.50 $17.50 $2.15 $8.75 $9.25 $9.25 $8.75 $4.45 $8.75 $11.25 $11.25 $1.25 $9.25 $4.45 $11.25 $11.75 $11.75 $6.49 $8.99 $2.39 $8.49 $2.39 $11.25 $8.75 $2.15 $8.99 $1.69 $8.75 $11.25 $2.15 $4.45 $8.75 $8.49 $8.99 $17.50 $17.50 $8.49 $1.09 $1.09 $8.75 $1.25 $2.15 $11.08 $8.49 $8.49 $8.99 $2.39 $8.75 $2.15 $1.50 $11.25 $2.15 $8.49 $8.49 $11.75 $9.25 $11.75 $1.25 $11.25 $8.75 $4.45 $6.49 $9.25 $2.95 $11.25 $4.45 $1.25 $1.25 $8.49 $2.39 $2.18 $2.18 $8.49 $2.18 $2.18 $22.16 $22.16 $17.50 $17.50 $8.75 $2.95 $6.49 $8.99 $3.39 $3.39 $8.99 $8.49 $11.25 $2.15 $11.25 $2.95 $11.25 $1.25 $8.99 $1.09 $8.75 $8.75 $9.25 $2.95 $11.75 $1.50 $8.99 $1.09 $11.25 $1.25 $1.25 $11.25 $11.75 $2.15 $8.99 $1.69 $11.75 $6.49 $8.75 $9.25 $11.25 $4.45 $1.25 $11.25 $4.45 $8.49 $8.99 $8.49 $8.99 $11.25 $1.25 $11.75 $1.25 $11.75 $9.25 $4.45 $11.25 $2.15 $32.94 $32.94 $32.94 $1.25 $11.25 $11.48 $1.69 $1.09 $17.50 $17.50 $4.45 $6.49 $9.25 $8.75 $9.25 $9.25 $8.75 $8.75 $2.15 $2.95 $17.50 $17.50 $10.98 $11.48 $11.48 $3.39 $8.99 $1.69 $8.99 $1.09 $10.98 $3.39 $8.99 $1.09 $9.25 $8.75 $11.25 $4.45 $2.95 $9.25 $22.20 $22.20 $22.20 $8.49 $8.99 $8.75 $8.75 $11.75 $8.75 $11.25 $9.25 $11.25 $11.25 $8.75 $11.25 $2.95 $1.25 $8.49 $1.69 $11.75 $11.25 $8.75 $8.75 $4.45 $8.49 $3.99 $8.49 $3.99 $11.48 $1.69 $1.09 $11.25 $1.50 $10.58 $1.69 $9.25 $11.25 $8.75 $9.25 $11.25 $11.25 $8.75 $11.75 $8.75 $8.75 $8.75 $2.15 $11.25 $11.75 $2.50 $2.50 $4.45 $9.25 $4.45 $11.25 $8.49 $3.99 $9.25 $9.25 $11.25 $9.25 $11.75 $11.25 $1.25 $23.50 $23.50 $1.25 $8.99 $8.49 $7.40 $7.40 $8.75 $1.25 $4.45 $8.75 $2.15 $8.75 $4.45 $7.40 $7.40 $7.40 $8.99 $3.99 $8.99 $1.69 $8.75 $8.75 $8.75 $8.75 $11.25 $11.25 $2.95 $8.75 $18.50 $18.50 $8.49 $3.99 $2.95 $9.25 $9.25 $3.00 $3.00 $1.25 $8.75 $9.25 $4.45 $8.75 $11.25 $4.45 $10.98 $22.16 $22.16 $4.45 $8.75 $9.25 $6.49 $9.25 $11.25 $8.75 $9.25 $2.15 $9.25 $4.45 $9.25 $2.95 $9.25 $8.75 $9.25 $1.25 $1.25 $8.75 $8.75 $9.25 $4.45 $11.75 $11.75 $11.75 $9.25 $9.25 $16.98 $16.98 $2.39 $3.39 $3.39 $9.25 $11.75 $11.25 $2.15 $8.75 $9.25 $4.45 $10.98 $11.25 $9.25 $22.50 $22.50 $9.25 $2.95 $1.50 $11.48 $8.49 $1.69 $8.49 $8.49 $8.49 $6.78 $6.78 $11.75 $4.45 $8.75 $4.45 $11.89 $9.39 $8.75 $2.95 $1.25 $9.25 $8.75 $23.78 $23.78 $8.75 $9.25 $2.15 $2.15 $1.25 $8.49 $3.99 $10.98 $1.09 $8.75 $4.45 $8.75 $11.75 $2.95 $4.45 $9.25 $8.75 $8.49 $3.99 $22.50 $22.50 $11.25 $1.25 $8.75 $8.75 $18.50 $18.50 $6.49 $8.75 $8.75 $4.45 $8.49 $3.99 $8.99 $1.09 $8.49 $2.39 $11.48 $1.69 $2.50 $2.50 $9.25 $1.50 $17.50 $17.50 $2.95 $8.75 $4.45 $11.75 $8.75 $8.49 $1.69 $8.49 $3.99 $8.99 $8.99 $3.99 $8.99 $11.25 $4.45 $1.25 $3.99 $10.98 $7.40 $3.00 $7.40 $4.00 $8.49 $3.99 $9.25 $4.45 $11.25 $1.25 $11.75 $1.25 $11.25 $2.15 $11.25 $4.45 $3.75 $3.75 $3.75 $11.75 $8.99 $2.39 $8.75 $4.45 $1.25 $8.99 $8.49 $2.18 $2.18 $8.49 $2.18 $2.18 $1.09 $8.75 $2.95 $1.25 $1.50 $11.25 $9.25 $2.95 $1.25 $8.49 $3.99 $11.48 $3.99 $8.49 $11.25 $1.25 $8.99 $1.69 $11.25 $1.25 $6.49 $8.75 $9.25 $8.75 $2.95 $8.75 $11.75 $8.69 $8.69 $2.29 $3.99 $8.49 $8.75 $8.75 $1.25 $11.75 $11.25 $11.25 $11.25 $1.25 $9.25 $11.75 $6.49 $3.99 $8.49 $11.25 $2.15 $11.25 $11.89 $8.99 $1.69 $8.99 $8.99 $3.99 $8.99 $9.25 $9.25 $2.15 $7.40 $7.40 $8.75 $8.75 $9.25 $4.45 $11.25 $1.25 $11.75 $11.25 $1.25 $3.99 $8.49 $8.49 $8.49 $8.99 $8.75 $2.15 $1.25 $8.49 $1.09 $1.09 $8.75 $2.95 $1.25 $9.25 $1.25 $2.15 $11.25 $1.25 $4.45 $8.75 $2.50 $2.50 $8.90 $8.90 $8.75 $8.75 $8.75 $11.25 $11.25 $10.98 $3.99 $10.98 $3.99 $1.69 $8.99 $9.25 $8.75 $8.99 $1.09 $9.25 $2.95 $8.75 $9.25 $3.99 $8.49 $8.75 $8.75 $22.50 $22.50 $10.98 $3.27 $3.27 $3.27 $3.99 $8.99 $1.09 $11.08 $8.75 $4.45 $11.08 $3.99 $8.49 $4.30 $4.30 $9.25 $8.75 $11.25 $11.25 $9.25 $8.49 $8.99 $8.49 $8.75 $2.95 $4.45 $9.25 $2.95 $9.25 $8.75 $11.25 $4.45 $16.98 $16.98 $8.49 $2.39 $11.25 $3.75 $3.75 $3.75 $9.25 $4.45 $9.25 $9.25 $4.45 $8.75 $9.25 $8.75 $9.25 $9.25 $9.25 $11.48 $8.99 $22.50 $22.50 $11.75 $11.25 $1.25 $8.75 $2.15 $1.25 $11.25 $8.75 $1.25 $11.25 $1.50 $11.25 $11.25 $9.25 $6.49 $8.90 $8.90 $8.75 $4.45 $11.25 $1.25 $17.50 $17.50 $9.25 $8.75 $11.75 $3.00 $3.00 $8.49 $8.49 $10.98 $8.99 $3.99 $8.75 $4.45 $8.99 $1.69 $11.75 $8.75 $11.25 $4.45 $11.75 $1.25 $11.75 $2.95 $8.99 $8.99 $2.18 $2.18 $17.98 $17.98 $8.99 $8.49 $1.69 $11.75 $11.25 $2.95 $3.75 $3.75 $3.75 $9.25 $11.75 $8.75 $2.15 $1.50 $8.49 $8.49 $3.39 $8.69 $3.89 $8.75 $4.45 $8.75 $11.25 $2.15 $8.75 $8.49 $1.69 $8.49 $8.49 $1.25 $8.75 $11.75 $11.75 $8.99 $1.09 $8.75 $4.45 $8.75 $2.95 $8.75 $2.15 $3.99 $8.49 $8.99 $3.99 $8.49 $1.69 $1.09 $8.99 $1.09 $9.25 $8.75 $8.99 $2.39 $1.25 $1.25 $11.25 $11.25 $9.25 $9.25 $11.25 $1.50 $3.99 $8.49 $11.25 $9.25 $11.25 $17.50 $17.50 $8.75 $8.90 $8.90 $8.75 $8.75 $8.99 $2.39 $11.25 $9.25 $2.15 $11.25 $1.25 $11.75 $1.25 $11.25 $11.75 $1.25 $11.25 $11.25 $8.49 $10.98 $8.75 $1.25 $8.75 $8.49 $8.49 $1.50 $1.50 $8.75 $4.45 $11.25 $1.25 $11.75 $8.49 $2.39 $9.25 $4.45 $9.25 $8.75 $8.99 $1.69 $17.50 $17.50 $2.39 $8.99 $8.99 $11.25 $4.45 $8.75 $4.45 $9.25 $6.49 $10.98 $8.49 $8.49 $1.09 $1.69 $9.25 $4.45 $8.75 $1.25 $2.95 $3.99 $8.49 $11.75 $11.75 $2.15 $11.48 $8.75 $2.15 $1.25 $11.25 $2.15 $1.25 $8.75 $8.75 $6.49 $1.69 $8.99 $8.75 $11.75 $10.98 $1.09 $8.49 $3.39 $8.75 $2.15 $1.25 $11.48 $10.98 $10.98 $8.49 $2.95 $9.25 $9.25 $11.75 $4.45 $11.48 $11.25 $8.75 $4.45 $1.69 $8.99 $8.75 $4.45 $1.50 $11.75 $2.15 $8.99 $2.39 $8.75 $2.95 $1.25 $8.75 $2.15 $1.25 $2.18 $2.18 $2.18 $2.18 $11.48 $8.75 $2.95 $11.75 $11.75 $1.25 $10.58 $8.99 $2.39 $11.75 $4.45 $11.25 $11.25 $17.50 $17.50 $8.75 $8.75 $8.75 $22.50 $22.50 $9.25 $8.75 $4.45 $11.75 $1.25 $11.25 $11.25 $2.95 $8.99 $1.69 $11.25 $4.45 $8.75 $6.49 $8.75 $4.45 $9.25 $4.45 $11.75 $11.75 $4.45 $11.89 $11.75 $11.25 $2.95 $1.50 $4.45 $8.75 $8.99 $1.09 $8.99 $1.09 $3.99 $11.48 $8.49 $9.25 $4.45 $11.48 $9.25 $2.95 $9.25 $8.49 $8.99 $8.99 $8.49 $8.75 $2.95 $4.45 $11.89 $10.58 $8.19 $1.69 $8.75 $2.15 $1.25 $17.50 $17.50 $6.49 $9.25 $2.15 $8.75 $4.45 $8.75 $1.25 $11.48 $11.48 $8.99 $2.18 $2.18 $8.49 $8.99 $2.39 $2.39 $2.18 $2.18 $8.75 $4.45 $11.25 $9.25 $9.25 $11.25 $11.25 $4.45 $2.95 $11.75 $8.49 $8.49 $8.99 $1.69 $9.25 $11.25 $11.75 $9.25 $8.75 $11.75 $8.75 $8.75 $11.25 $11.25 $10.98 $11.25 $4.45 $10.98 $8.49 $8.99 $3.39 $3.99 $8.99 $1.09 $1.09 $2.39 $17.50 $17.50 $4.45 $11.25 $11.25 $4.45 $9.25 $4.45 $8.75 $2.15 $1.25 $11.89 $2.95 $11.75 $1.25 $11.25 $4.45 $11.48 $11.48 $2.95 $9.25 $8.75 $9.25 $2.95 $11.25 $1.25 $11.75 $1.25 $8.99 $2.39 $1.25 $11.25 $1.25 $11.25 $8.49 $3.99 $35.00 $35.00 $35.00 $35.00 $27.75 $27.75 $27.75 $8.75 $11.80 $11.80 $11.80 $11.80 $8.90 $8.90 $5.90 $5.90 $6.49 $10.98 $17.98 $17.98 $2.39 $9.25 $8.75 $2.15 $8.75 $4.45 $8.49 $1.69 $8.19 $8.69 $10.98 $3.99 $11.48 $11.48 $4.45 $8.75 $6.49 $8.75 $8.75 $9.25 $1.25 $4.45 $8.49 $1.69 $9.25 $4.45 $8.99 $1.09 $11.25 $2.95 $11.08 $11.08 $3.89 $10.98 $11.25 $8.75 $11.25 $9.25 $4.30 $4.30 $8.75 $8.49 $3.99 $1.69 $8.99 $8.49 $1.69 $11.75 $11.25 $11.89 $9.25 $2.95 $9.25 $2.95 $8.75 $4.45 $4.45 $8.75 $10.98 $11.48 $8.49 $9.25 $4.45 $11.75 $11.89 $8.99 $8.49 $8.75 $9.25 $8.75 $8.75 $11.75 $11.75 $4.45 $11.25 $11.75 $2.50 $2.50 $8.99 $1.69 $11.75 $2.15 $1.25 $9.25 $8.75 $8.90 $8.90 $9.25 $2.95 $8.75 $11.25 $8.90 $8.90 $11.25 $11.75 $11.48 $1.69 $3.39 $9.25 $2.95 $8.99 $1.69 $8.49 $10.98 $11.25 $2.95 $8.99 $1.69 $8.75 $2.15 $1.25 $8.75 $2.95 $9.25 $2.50 $2.50 $11.25 $1.25 $11.75 $2.50 $2.50 $11.25 $1.50 $8.75 $1.25 $2.95 $11.48 $11.48 $8.75 $8.75 $2.15 $11.75 $1.25 $9.25 $9.25 $6.49 $11.75 $8.49 $8.49 $1.09 $10.98 $8.75 $1.25 $2.15 $11.25 $1.50 $11.25 $11.25 $8.49 $8.49 $8.75 $1.50 $1.25 $1.50 $8.75 $2.50 $2.50 $2.15 $7.40 $7.40 $4.00 $9.25 $9.39 $9.25 $9.25 $9.39 $11.25 $8.90 $8.90 $11.25 $6.00 $6.00 $6.00 $6.00 $11.25 $11.25 $11.25 $22.50 $22.50 $11.48 $1.09 $8.49 $8.49 $17.50 $17.50 $11.25 $1.50 $9.25 $8.75 $3.99 $8.49 $8.75 $8.75 $8.75 $8.75 $8.75 $11.75 $1.50 $11.25 $11.25 $2.95 $8.99 $10.98 $9.25 $8.75 $4.45 $8.49 $1.09 $2.39 $8.75 $8.75 $11.48 $8.99 $8.49 $8.49 $2.39 $10.98 $8.49 $3.99 $11.75 $4.45 $8.75 $2.15 $1.25 $10.98 $8.99 $11.25 $1.50 $8.75 $2.15 $1.25 $8.75 $9.25 $8.75 $11.25 $1.50 $8.75 $1.25 $4.45 $10.98 $8.75 $2.95 $1.25 $8.75 $2.95 $1.25 $8.49 $8.49 $2.39 $11.25 $1.25 $8.75 $8.75 $9.25 $8.75 $11.89 $1.25 $8.75 $2.15 $1.25 $8.99 $1.09 $8.75 $4.45 $26.25 $26.25 $26.25 $8.75 $4.45 $11.75 $2.95 $8.75 $8.75 $11.75 $8.75 $11.25 $11.25 $11.25 $4.45 $1.25 $8.49 $8.49 $8.49 $8.99 $8.99 $2.39 $2.39 $3.99 $8.75 $4.45 $2.15 $9.25 $1.25 $11.25 $11.75 $8.75 $4.45 $11.25 $2.15 $8.75 $4.45 $8.75 $8.75 $1.25 $11.25 $2.15 $8.75 $5.90 $5.90 $11.75 $1.25 $9.25 $3.75 $3.75 $3.75 $8.75 $1.25 $4.45 $11.75 $4.45 $8.75 $23.50 $23.50 $8.75 $2.95 $8.75 $8.75 $11.89 $4.45 $2.95 $1.25 $8.75 $4.45 $2.95 $1.25 $8.75 $2.15 $1.25 $11.75 $2.95 $8.99 $3.39 $9.25 $9.25 $17.50 $17.50 $2.95 $11.89 $1.50 $11.25 $2.95 $9.25 $11.25 $11.25 $2.95 $8.75 $9.25 $4.30 $4.30 $8.75 $8.75 $11.25 $8.75 $4.30 $4.30 $8.75 $1.25 $2.15 $8.49 $8.49 $3.39 $3.39 $10.98 $10.98 $2.39 $11.25 $11.75 $11.75 $1.25 $5.90 $5.90 $8.75 $11.25 $9.25 $4.45 $1.50 $3.39 $8.99 $2.39 $11.25 $2.15 $11.25 $11.75 $11.75 $4.45 $11.75 $4.45 $9.25 $8.75 $8.49 $8.99 $8.49 $8.99 $11.75 $8.75 $8.49 $3.99 $3.89 $11.08 $8.49 $8.99 $8.49 $8.49 $8.49 $11.25 $2.15 $17.50 $17.50 $8.75 $2.95 $8.49 $8.49 $10.98 $1.09 $11.25 $2.15 $2.95 $1.25 $8.75 $9.25 $9.25 $9.25 $2.95 $8.75 $2.15 $1.25 $8.99 $3.99 $11.75 $2.15 $8.99 $3.39 $9.25 $8.75 $11.25 $11.25 $4.45 $8.75 $2.15 $1.25 $11.75 $4.45 $9.25 $2.95 $8.49 $8.49 $11.25 $8.75 $4.45 $11.25 $11.25 $11.25 $11.25 $4.45 $8.49 $1.69 $8.49 $3.39 $8.75 $11.25 $9.25 $8.75 $11.25 $11.25 $11.75 $11.25 $11.75 $11.25 $11.75 $21.96 $21.96 $10.98 $1.69 $11.48 $8.99 $8.49 $1.69 $9.25 $2.15 $1.50 $11.25 $1.50 $8.75 $8.75 $2.95 $8.49 $1.69 $8.75 $2.95 $1.25 $11.25 $2.15 $11.08 $8.49 $8.49 $8.49 $11.75 $1.25 $11.75 $8.75 $8.75 $8.75 $4.45 $11.25 $1.50 $23.50 $23.50 $11.75 $6.49 $8.75 $4.45 $6.49 $8.75 $2.50 $2.50 $2.15 $8.49 $2.39 $8.75 $11.75 $4.45 $8.99 $10.98 $9.25 $2.95 $9.25 $9.25 $11.75 $8.75 $8.75 $8.75 $10.98 $11.25 $9.25 $8.75 $8.75 $2.15 $11.25 $2.15 $4.45 $11.75 $8.49 $2.39 $9.25 $1.25 $1.25 $1.25 $1.25 $8.75 $2.15 $8.49 $1.69 $11.25 $1.50 $8.75 $8.75 $8.49 $3.99 $8.99 $1.09 $11.25 $1.25 $8.49 $2.39 $8.49 $8.75 $9.25 $11.25 $4.45 $11.25 $11.89 $8.99 $8.49 $8.75 $4.45 $8.75 $11.75 $11.75 $8.90 $8.90 $9.39 $2.95 $8.49 $3.99 $8.75 $2.15 $1.25 $21.96 $21.96 $8.49 $1.69 $8.75 $4.45 $8.49 $8.99 $8.49 $3.99 $8.75 $8.75 $2.95 $8.75 $17.50 $17.50 $9.25 $2.95 $8.75 $6.49 $4.30 $4.30 $8.75 $8.75 $2.15 $1.50 $8.49 $8.49 $2.39 $9.25 $4.45 $6.49 $11.75 $4.45 $10.98 $1.69 $9.39 $9.25 $9.25 $2.95 $8.75 $2.15 $1.25 $11.25 $9.25 $8.75 $11.25 $8.75 $11.25 $2.50 $2.50 $2.50 $2.50 $6.00 $6.00 $6.00 $6.00 $8.90 $8.90 $5.90 $5.90 $11.25 $11.25 $8.49 $10.98 $8.75 $2.15 $1.50 $9.25 $1.25 $1.50 $2.15 $1.25 $8.75 $2.95 $8.49 $3.99 $11.25 $4.30 $4.30 $11.75 $2.15 $18.50 $18.50 $8.49 $2.39 $8.75 $4.45 $11.75 $8.99 $3.99 $9.25 $9.25 $1.50 $8.75 $2.95 $6.49 $11.75 $8.49 $8.99 $8.75 $4.45 $6.49 $22.50 $22.50 $9.25 $2.95 $8.49 $1.69 $10.98 $8.75 $4.45 $11.25 $2.95 $8.99 $8.49 $2.39 $11.75 $6.49 $11.25 $11.75 $2.95 $8.99 $1.69 $8.99 $2.18 $2.18 $1.09 $8.99 $8.99 $1.09 $8.99 $8.99 $8.49 $10.98 $1.09 $11.75 $9.25 $11.25 $11.25 $2.15 $11.25 $8.75 $4.45 $2.95 $11.75 $1.50 $8.99 $10.98 $2.39 $8.75 $2.15 $9.25 $1.50 $8.75 $2.15 $3.99 $8.99 $6.49 $8.75 $8.90 $8.90 $8.99 $3.99 $17.50 $17.50 $11.25 $1.25 $10.98 $9.25 $4.45 $1.25 $3.00 $3.00 $11.25 $4.45 $4.45 $2.95 $9.25 $11.25 $2.15 $11.25 $11.25 $4.45 $2.95 $9.25 $11.25 $1.25 $8.75 $2.95 $1.25 $8.75 $4.45 $11.48 $11.48 $8.49 $2.39 $11.25 $11.75 $2.15 $1.50 $2.15 $8.75 $11.25 $8.90 $8.90 $11.25 $11.25 $1.25 $4.45 $9.25 $9.25 $8.75 $9.25 $8.75 $8.75 $9.25 $8.75 $11.75 $11.75 $8.75 $8.75 $8.90 $8.90 $2.95 $10.98 $8.49 $8.49 $10.98 $8.99 $8.99 $11.75 $17.50 $17.50 $11.75 $3.99 $8.49 $10.98 $1.69 $17.50 $17.50 $8.99 $2.39 $8.99 $2.39 $1.25 $8.75 $2.95 $11.75 $11.25 $17.50 $17.50 $8.49 $8.49 $2.39 $11.25 $1.50 $8.75 $3.00 $3.00 $1.25 $8.75 $4.45 $11.75 $11.75 $4.45 $21.96 $21.96 $8.75 $4.45 $8.75 $11.25 $9.25 $8.99 $2.39 $9.25 $8.75 $10.98 $8.49 $3.99 $3.39 $11.75 $1.50 $4.45 $9.25 $8.75 $1.25 $11.75 $8.75 $1.50 $8.75 $8.75 $2.15 $1.50 $8.75 $2.95 $8.75 $8.75 $17.50 $17.50 $8.75 $6.49 $4.45 $11.25 $11.25 $4.30 $4.30 $8.75 $11.25 $4.45 $8.99 $2.39 $9.25 $9.25 $9.25 $4.45 $11.75 $11.25 $2.95 $2.15 $11.25 $11.25 $8.75 $2.15 $1.50 $9.25 $4.45 $10.98 $8.99 $2.18 $2.18 $8.75 $4.45 $1.25 $8.99 $2.39 $4.45 $8.75 $10.98 $11.75 $1.50 $10.98 $8.99 $8.49 $3.99 $8.99 $8.49 $3.99 $8.49 $8.49 $8.99 $11.25 $11.25 $10.98 $10.98 $10.98 $2.39 $3.39 $8.75 $1.25 $2.95 $11.75 $1.50 $10.98 $1.69 $4.45 $8.75 $8.75 $8.75 $8.75 $4.45 $9.25 $8.75 $11.25 $8.75 $3.99 $8.99 $8.49 $11.25 $11.25 $8.75 $4.45 $8.75 $4.45 $1.25 $8.75 $8.75 $1.50 $2.15 $11.75 $11.75 $11.75 $11.75 $11.75 $1.50 $8.75 $9.25 $1.25 $8.75 $2.15 $8.99 $1.09 $4.45 $11.25 $11.75 $2.15 $8.75 $8.75 $1.25 $9.25 $2.15 $11.75 $11.25 $8.75 $11.25 $4.45 $8.49 $1.69 $8.75 $8.75 $8.99 $8.49 $9.25 $11.25 $2.95 $4.45 $11.75 $6.49 $11.48 $8.99 $4.36 $4.36 $4.36 $4.36 $11.48 $8.99 $8.49 $11.48 $8.75 $2.15 $1.50 $8.99 $1.69 $11.25 $1.25 $9.25 $9.25 $8.75 $9.25 $8.90 $8.90 $2.15 $9.25 $10.98 $8.49 $8.75 $9.25 $4.30 $4.30 $9.25 $8.75 $8.75 $2.15 $1.25 $8.75 $1.25 $8.75 $5.90 $5.90 $9.25 $8.75 $9.25 $4.45 $9.25 $11.75 $2.50 $2.50 $9.25 $2.15 $9.25 $1.50 $1.25 $11.25 $2.95 $8.75 $8.75 $8.75 $10.98 $8.75 $8.75 $8.75 $2.15 $1.25 $10.98 $8.75 $2.15 $1.50 $8.75 $2.95 $1.25 $9.25 $9.25 $8.49 $2.39 $8.75 $4.45 $9.25 $8.75 $8.75 $8.75 $9.25 $8.75 $9.25 $8.75 $8.75 $8.75 $8.75 $9.25 $8.75 $9.25 $8.75 $9.25 $8.75 $8.75 $8.75 $9.25 $8.75 $9.25 $8.75 $8.99 $8.99 $8.75 $2.95 $1.25 $11.75 $1.50 $11.25 $11.25 $8.75 $1.50 $2.15 $16.98 $16.98 $11.75 $1.50 $8.75 $4.30 $4.30 $1.50 $8.75 $2.95 $1.25 $1.25 $9.25 $4.45 $11.25 $8.75 $4.45 $8.75 $2.15 $1.25 $10.98 $1.69 $8.75 $1.25 $8.75 $1.25 $11.25 $8.75 $8.75 $8.49 $1.69 $9.25 $11.75 $8.49 $2.39 $9.25 $2.95 $6.49 $8.75 $8.75 $9.25 $8.75 $6.78 $6.78 $17.98 $17.98 $3.39 $11.75 $11.25 $8.75 $4.45 $11.75 $9.25 $8.75 $6.49 $8.99 $2.39 $8.75 $11.25 $11.75 $4.45 $8.75 $2.15 $9.25 $9.25 $9.25 $11.89 $11.75 $11.25 $9.25 $9.25 $8.75 $8.75 $8.49 $1.69 $1.09 $11.25 $1.50 $11.25 $11.25 $11.75 $1.50 $8.49 $8.99 $22.50 $22.50 $8.75 $4.30 $4.30 $8.75 $11.25 $2.15 $11.25 $2.95 $4.45 $11.25 $8.49 $3.39 $2.39 $11.75 $2.15 $11.75 $8.99 $2.39 $8.75 $11.75 $11.89 $1.25 $7.50 $7.50 $7.50 $7.50 $7.50 $11.89 $1.09 $8.49 $2.39 $8.75 $8.75 $8.75 $8.75 $9.25 $11.25 $8.75 $8.90 $8.90 $9.25 $8.75 $8.75 $11.75 $3.00 $3.00 $1.50 $11.25 $11.75 $8.99 $10.98 $4.45 $8.75 $2.15 $9.25 $11.25 $4.45 $1.69 $10.98 $9.25 $11.75 $9.25 $4.45 $10.98 $3.99 $8.49 $1.25 $9.25 $4.45 $10.98 $8.75 $8.75 $11.75 $11.25 $8.49 $11.48 $4.45 $1.25 $11.25 $8.99 $1.09 $2.39 $11.25 $2.15 $8.75 $4.45 $8.49 $1.69 $10.98 $1.69 $9.25 $4.45 $11.25 $8.75 $11.25 $11.75 $11.25 $22.50 $22.50 $8.49 $2.39 $2.50 $2.50 $8.75 $8.75 $9.25 $9.25 $11.25 $8.99 $1.09 $8.99 $1.69 $11.75 $1.25 $21.96 $21.96 $8.75 $2.15 $1.25 $8.75 $11.25 $9.25 $11.25 $8.75 $8.75 $11.25 $2.15 $8.99 $1.09 $1.69 $8.75 $2.15 $1.25 $8.49 $1.09 $1.09 $1.69 $11.48 $8.49 $8.49 $4.78 $4.78 $9.25 $1.25 $1.25 $1.25 $11.25 $11.25 $11.75 $4.45 $11.25 $4.45 $8.99 $1.09 $11.25 $2.15 $11.25 $9.25 $11.75 $11.25 $11.25 $9.25 $2.95 $11.25 $4.45 $8.75 $2.95 $2.95 $11.25 $1.50 $10.98 $16.98 $16.98 $18.50 $18.50 $10.98 $3.99 $1.09 $9.25 $9.25 $8.75 $4.45 $17.50 $17.50 $8.75 $4.45 $8.75 $1.25 $4.45 $9.25 $8.75 $8.75 $17.50 $17.50 $4.45 $9.39 $1.25 $2.95 $11.25 $8.75 $8.75 $11.25 $2.15 $8.90 $8.90 $11.25 $11.89 $10.98 $11.25 $4.45 $11.25 $11.25 $8.49 $10.98 $8.49 $3.39 $9.25 $8.75 $2.95 $3.00 $3.00 $9.39 $11.75 $2.95 $1.50 $11.25 $11.75 $8.75 $2.15 $1.50 $8.49 $3.39 $11.75 $1.25 $17.50 $17.50 $11.25 $1.25 $8.75 $2.95 $1.25 $11.25 $11.75 $13.35 $13.35 $13.35 $11.25 $11.75 $11.25 $11.25 $4.45 $11.25 $8.49 $3.39 $9.25 $2.95 $4.78 $4.78 $2.39 $3.99 $8.99 $8.99 $11.25 $11.25 $8.75 $11.25 $2.95 $4.45 $9.25 $8.75 $4.45 $8.49 $8.49 $10.98 $10.98 $3.99 $11.75 $8.75 $11.75 $4.45 $1.50 $1.25 $8.49 $8.49 $8.75 $8.75 $8.75 $9.25 $8.75 $2.95 $1.25 $11.25 $1.50 $11.25 $4.45 $9.25 $8.75 $8.75 $9.25 $8.75 $4.45 $1.50 $8.75 $8.75 $8.49 $1.69 $8.75 $2.15 $9.25 $2.15 $1.50 $11.25 $11.75 $2.15 $6.49 $9.25 $9.25 $11.25 $11.25 $11.75 $11.75 $11.75 $11.25 $8.75 $2.15 $1.25 $11.75 $9.25 $11.25 $8.75 $5.90 $5.90 $8.75 $4.45 $9.25 $9.25 $4.45 $11.25 $4.45 $11.25 $8.75 $2.15 $11.89 $11.25 $8.75 $2.95 $1.50 $8.75 $4.30 $4.30 $8.75 $11.25 $11.75 $11.75 $2.15 $11.25 $8.99 $1.09 $8.49 $8.49 $8.49 $3.39 $8.99 $10.98 $3.99 $11.75 $2.15 $8.75 $4.45 $2.50 $2.50 $11.48 $1.09 $8.49 $8.49 $16.98 $16.98 $3.99 $10.98 $1.09 $8.75 $2.95 $8.75 $8.75 $2.95 $9.25 $11.25 $2.15 $9.25 $4.45 $4.45 $9.25 $11.75 $11.75 $2.15 $9.25 $8.75 $11.25 $6.49 $8.75 $11.25 $2.95 $10.98 $3.99 $1.50 $9.25 $2.15 $8.75 $11.25 $11.89 $4.45 $1.50 $1.25 $8.75 $8.75 $4.45 $11.25 $11.75 $8.49 $1.09 $1.09 $1.69 $8.99 $3.39 $8.99 $1.69 $8.49 $8.99 $3.27 $3.27 $3.27 $8.99 $8.99 $1.09 $10.98 $1.69 $3.99 $8.49 $1.09 $8.75 $8.75 $11.75 $8.75 $9.25 $8.75 $3.39 $8.99 $8.99 $2.39 $9.25 $8.75 $9.25 $9.25 $8.99 $3.99 $2.39 $8.49 $1.09 $8.49 $8.99 $3.39 $11.25 $1.25 $8.99 $3.99 $8.75 $8.90 $8.90 $6.49 $8.75 $9.25 $11.25 $11.25 $11.25 $1.25 $8.75 $9.25 $4.45 $1.25 $8.75 $1.25 $2.15 $17.98 $17.98 $8.99 $8.75 $2.95 $1.25 $11.75 $1.50 $1.50 $8.75 $8.75 $11.08 $8.99 $1.69 $8.99 $1.69 $10.98 $3.99 $3.39 $11.75 $2.15 $11.75 $2.95 $8.75 $8.75 $11.75 $11.25 $11.75 $11.25 $4.45 $11.25 $1.25 $2.18 $2.18 $2.18 $2.18 $2.39 $8.49 $8.99 $2.39 $11.25 $8.75 $11.75 $11.75 $11.25 $4.45 $2.15 $8.19 $10.58 $4.45 $9.25 $1.09 $8.99 $11.25 $1.50 $8.99 $3.99 $4.45 $11.75 $2.15 $11.25 $8.75 $4.45 $8.75 $9.25 $6.45 $6.45 $6.45 $8.75 $11.25 $11.25 $8.75 $11.75 $21.96 $21.96 $8.99 $5.07 $5.07 $5.07 $8.49 $9.25 $11.25 $4.45 $3.39 $8.49 $8.99 $8.49 $17.50 $17.50 $22.96 $22.96 $8.75 $11.25 $11.89 $11.25 $8.49 $1.69 $1.09 $8.99 $8.99 $9.25 $8.75 $9.25 $2.95 $8.49 $3.99 $8.99 $8.49 $7.17 $7.17 $7.17 $8.49 $8.99 $17.50 $17.50 $9.25 $9.25 $11.25 $1.25 $8.99 $1.09 $8.75 $4.45 $11.25 $2.15 $11.75 $11.25 $11.25 $8.75 $8.75 $4.45 $1.25 $11.75 $11.75 $2.50 $2.50 $8.49 $8.99 $2.18 $2.18 $11.25 $4.45 $11.25 $11.75 $8.49 $8.99 $1.69 $1.09 $8.99 $8.99 $11.25 $6.49 $11.25 $8.75 $4.45 $8.99 $1.69 $11.48 $11.75 $2.50 $2.50 $8.49 $1.09 $1.09 $1.69 $8.49 $2.39 $11.75 $1.25 $8.49 $1.69 $8.49 $1.69 $11.75 $4.45 $8.75 $8.75 $4.45 $8.75 $11.25 $11.25 $8.75 $7.98 $7.98 $8.49 $1.09 $8.49 $3.99 $8.49 $3.99 $8.99 $3.99 $11.25 $4.45 $8.49 $2.39 $8.49 $2.39 $3.99 $8.49 $1.25 $11.25 $4.45 $9.25 $4.45 $1.09 $8.99 $3.99 $11.25 $8.90 $8.90 $9.25 $11.25 $8.75 $11.25 $11.25 $11.25 $11.25 $11.25 $8.99 $8.49 $8.75 $8.75 $4.45 $16.98 $16.98 $11.75 $11.25 $9.25 $4.45 $9.25 $2.95 $8.49 $1.69 $3.75 $3.75 $3.75 $4.45 $9.25 $1.50 $11.25 $11.48 $11.25 $2.15 $8.75 $9.39 $8.49 $3.99 $8.19 $2.29 $11.48 $1.69 $11.48 $3.99 $8.49 $1.69 $9.25 $2.95 $8.49 $1.69 $11.25 $4.45 $9.39 $9.25 $8.75 $8.75 $4.45 $11.89 $4.45 $4.45 $8.75 $8.75 $8.75 $2.15 $8.75 $3.75 $3.75 $3.75 $9.25 $11.25 $4.45 $6.49 $16.98 $16.98 $18.50 $18.50 $2.50 $2.50 $2.95 $3.99 $8.49 $8.19 $11.08 $6.49 $11.75 $2.39 $8.99 $1.09 $11.25 $4.45 $11.25 $8.99 $1.69 $21.96 $21.96 $2.18 $2.18 $8.99 $8.99 $2.39 $8.69 $1.69 $8.90 $8.90 $2.50 $2.50 $8.75 $8.99 $1.09 $8.49 $8.49 $8.75 $4.45 $17.50 $17.50 $8.75 $9.25 $8.49 $2.39 $8.75 $4.45 $11.25 $11.25 $11.75 $8.75 $8.49 $8.49 $8.49 $8.99 $8.75 $4.45 $11.48 $8.75 $1.25 $2.15 $9.25 $4.45 $11.75 $2.15 $11.25 $8.99 $2.39 $8.69 $8.69 $11.75 $2.95 $11.75 $1.50 $9.25 $4.45 $1.50 $11.48 $8.99 $2.39 $11.25 $11.89 $2.15 $1.25 $11.75 $4.45 $8.75 $8.75 $11.25 $4.45 $11.25 $2.15 $4.45 $8.49 $1.09 $3.99 $11.25 $11.25 $8.49 $2.39 $8.99 $2.39 $11.25 $2.15 $8.75 $2.95 $1.25 $8.75 $11.25 $17.50 $17.50 $11.75 $11.75 $11.25 $11.25 $4.45 $2.50 $2.50 $8.75 $8.99 $8.99 $1.69 $8.99 $1.69 $11.25 $1.25 $11.08 $8.69 $8.99 $1.09 $11.25 $11.25 $2.95 $1.25 $8.75 $1.25 $8.75 $8.75 $2.15 $1.25 $8.49 $3.99 $8.49 $2.39 $8.49 $7.17 $7.17 $7.17 $8.75 $4.45 $11.48 $8.75 $8.75 $11.48 $8.75 $9.25 $8.49 $3.99 $1.50 $11.25 $11.25 $8.75 $8.75 $4.45 $9.25 $4.45 $8.75 $8.75 $4.30 $4.30 $2.95 $8.75 $4.50 $4.50 $4.50 $9.25 $11.25 $4.45 $11.25 $11.25 $8.75 $9.25 $8.75 $2.15 $1.25 $8.75 $2.15 $1.25 $8.99 $8.49 $8.75 $8.75 $1.25 $11.75 $4.50 $4.50 $4.50 $8.75 $8.75 $3.99 $3.39 $8.49 $2.39 $8.99 $1.50 $11.25 $11.25 $8.75 $8.75 $2.15 $8.75 $2.15 $1.25 $21.96 $21.96 $8.49 $1.69 $26.07 $26.07 $26.07 $11.75 $1.50 $8.99 $8.99 $11.48 $9.25 $9.25 $8.75 $8.75 $8.75 $9.25 $8.75 $8.75 $2.15 $1.25 $11.89 $8.75 $11.75 $4.45 $18.50 $18.50 $9.25 $9.39 $8.49 $2.39 $8.49 $1.69 $1.09 $8.99 $8.49 $2.18 $2.18 $11.25 $1.25 $8.49 $3.39 $8.49 $3.99 $11.25 $8.75 $8.49 $1.69 $16.98 $16.98 $9.25 $9.25 $11.75 $1.25 $11.25 $8.75 $8.49 $8.49 $8.75 $1.25 $1.25 $1.25 $11.25 $12.98 $12.98 $11.75 $11.75 $4.45 $11.25 $11.75 $10.98 $8.49 $8.49 $2.39 $9.25 $11.25 $8.75 $2.95 $1.50 $11.25 $2.95 $9.25 $2.95 $9.25 $8.75 $11.25 $8.75 $8.75 $4.45 $11.25 $1.25 $8.75 $2.95 $2.50 $2.50 $9.25 $9.25 $9.25 $6.49 $17.50 $17.50 $8.49 $1.69 $8.49 $3.99 $8.75 $2.95 $2.95 $8.49 $8.99 $8.99 $1.09 $8.75 $4.45 $1.25 $11.25 $11.75 $11.25 $9.25 $11.25 $1.50 $11.25 $2.15 $10.98 $8.75 $11.75 $2.95 $11.25 $11.25 $9.25 $8.75 $9.25 $8.75 $8.75 $8.75 $1.25 $11.25 $1.50 $2.15 $8.75 $8.75 $11.25 $8.75 $8.75 $11.25 $4.45 $8.49 $8.49 $8.49 $8.49 $8.99 $1.69 $2.39 $1.09 $1.09 $11.25 $2.95 $35.25 $35.25 $35.25 $8.75 $2.95 $1.25 $11.25 $2.15 $9.25 $4.45 $8.75 $8.75 $8.75 $4.45 $1.25 $11.89 $8.75 $2.15 $1.25 $8.49 $1.09 $1.09 $1.69 $8.69 $8.69 $9.25 $2.95 $10.98 $2.39 $22.50 $22.50 $21.96 $21.96 $10.98 $8.49 $1.69 $11.75 $2.95 $8.75 $11.25 $8.75 $9.25 $8.75 $8.19 $10.58 $8.75 $2.95 $1.50 $2.50 $2.50 $9.25 $4.45 $9.25 $11.25 $8.69 $8.69 $3.89 $8.69 $1.69 $4.45 $9.25 $11.25 $4.45 $2.15 $8.19 $8.69 $4.45 $11.25 $1.25 $11.25 $8.75 $11.89 $11.75 $11.75 $9.25 $8.75 $2.15 $1.50 $11.75 $4.45 $3.99 $8.99 $10.98 $2.39 $1.25 $2.95 $8.75 $2.95 $9.25 $9.25 $9.25 $8.75 $2.15 $9.25 $9.25 $3.39 $8.49 $8.49 $2.39 $10.98 $1.09 $1.09 $8.49 $11.25 $1.25 $8.99 $1.09 $8.75 $2.15 $1.50 $3.99 $8.49 $8.75 $2.15 $1.50 $8.49 $10.98 $2.18 $2.18 $8.75 $2.15 $1.50 $4.45 $9.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $10.50 $10.50 $10.50 $10.50 $10.50 $10.50 $10.50 $6.49 $33.75 $33.75 $33.75 $35.00 $35.00 $35.00 $35.00 $27.75 $27.75 $27.75 $3.00 $3.00 $11.25 $11.75 $10.98 $2.39 $2.50 $2.50 $8.75 $4.45 $16.98 $16.98 $8.75 $6.49 $16.98 $16.98 $17.98 $17.98 $16.98 $16.98 $16.98 $16.98 $8.99 $8.99 $8.49 $9.25 $8.75 $2.95 $11.48 $2.39 $8.99 $2.39 $11.25 $4.45 $8.75 $9.25 $6.49 $26.25 $26.25 $26.25 $8.75 $26.25 $26.25 $26.25 $8.75 $8.75 $11.25 $11.25 $2.15 $1.25 $11.75 $8.75 $2.15 $1.50 $11.25 $2.15 $8.99 $2.39 $11.25 $11.25 $2.15 $11.25 $11.25 $8.75 $4.78 $4.78 $21.96 $21.96 $8.49 $2.39 $9.25 $2.95 $16.98 $16.98 $8.19 $3.89 $8.99 $1.09 $8.99 $3.39 $9.25 $4.45 $10.98 $10.98 $17.50 $17.50 $11.25 $1.25 $8.75 $11.75 $4.45 $11.25 $11.25 $8.99 $1.09 $10.98 $8.49 $1.69 $11.25 $9.25 $16.98 $16.98 $8.75 $4.45 $11.25 $6.49 $11.75 $9.25 $9.25 $8.75 $4.45 $2.50 $2.50 $8.75 $11.75 $8.75 $9.25 $11.75 $9.25 $8.75 $9.25 $8.75 $11.25 $11.75 $9.25 $8.75 $11.75 $8.49 $1.09 $1.09 $8.49 $1.09 $1.69 $11.25 $1.25 $8.75 $2.15 $1.50 $8.49 $1.69 $1.25 $8.75 $2.95 $8.49 $3.99 $8.49 $8.49 $8.75 $11.25 $2.15 $1.50 $11.75 $8.99 $1.09 $10.98 $10.98 $11.25 $1.25 $8.75 $4.45 $4.45 $1.25 $11.89 $8.99 $8.99 $11.25 $4.45 $23.50 $23.50 $8.49 $3.99 $9.25 $4.45 $4.45 $9.25 $8.75 $4.45 $8.75 $8.75 $11.75 $6.49 $17.50 $17.50 $4.45 $8.75 $2.95 $1.50 $8.75 $8.75 $8.75 $4.45 $11.25 $11.25 $11.75 $11.25 $2.95 $11.25 $4.45 $3.00 $3.00 $1.25 $2.95 $9.25 $8.99 $2.39 $6.49 $8.75 $8.90 $8.90 $11.48 $1.09 $10.98 $9.25 $9.25 $11.25 $8.75 $11.75 $11.25 $11.25 $1.25 $9.25 $4.45 $9.25 $6.49 $11.75 $11.75 $8.99 $2.39 $8.49 $8.49 $9.25 $9.25 $1.25 $8.75 $2.95 $11.75 $2.15 $8.49 $8.49 $8.69 $16.38 $16.38 $8.19 $3.89 $2.29 $11.75 $8.75 $8.75 $8.75 $4.45 $8.49 $8.49 $9.25 $8.75 $6.49 $2.95 $11.25 $11.25 $2.15 $9.25 $11.75 $21.96 $21.96 $8.49 $3.39 $1.69 $8.49 $8.75 $4.45 $8.49 $3.99 $11.25 $8.75 $11.25 $2.15 $11.75 $4.45 $11.25 $9.25 $8.75 $18.50 $18.50 $1.50 $8.75 $2.15 $11.48 $2.18 $2.18 $3.99 $11.25 $1.50 $8.99 $2.39 $11.75 $1.50 $11.25 $6.49 $4.45 $11.25 $8.49 $3.99 $2.50 $2.50 $8.75 $9.25 $3.99 $8.99 $8.75 $6.49 $13.52 $13.52 $13.52 $13.52 $13.52 $13.52 $13.52 $13.52 $16.98 $16.98 $16.98 $16.98 $17.98 $17.98 $16.98 $16.98 $8.75 $8.75 $11.25 $11.25 $8.49 $1.09 $1.69 $1.25 $9.25 $2.95 $8.69 $8.19 $8.49 $2.39 $8.49 $2.39 $10.98 $8.99 $8.99 $1.69 $8.49 $8.75 $8.75 $11.25 $4.45 $4.45 $17.50 $17.50 $8.75 $4.45 $8.75 $2.15 $1.50 $1.50 $8.99 $1.09 $8.75 $4.45 $8.75 $8.75 $11.25 $4.30 $4.30 $8.49 $1.69 $1.09 $1.09 $8.75 $2.15 $1.50 $8.99 $8.49 $3.99 $8.75 $2.15 $1.25 $9.25 $2.95 $11.25 $4.45 $9.25 $2.95 $3.99 $8.99 $8.49 $8.75 $8.75 $11.25 $9.25 $8.75 $4.45 $11.25 $1.25 $9.25 $11.25 $4.45 $2.95 $10.98 $8.75 $8.75 $18.50 $18.50 $9.25 $9.25 $5.00 $5.00 $5.00 $5.00 $8.75 $2.95 $16.98 $16.98 $11.25 $2.95 $8.75 $2.15 $1.50 $8.49 $2.39 $9.25 $2.15 $1.25 $8.19 $8.69 $8.19 $8.19 $8.75 $2.95 $1.25 $9.25 $2.95 $11.25 $8.75 $11.25 $11.25 $8.99 $1.09 $9.25 $9.25 $4.45 $8.49 $3.99 $2.39 $1.09 $8.99 $8.49 $8.75 $8.75 $11.25 $11.75 $4.45 $2.50 $2.50 $8.75 $8.49 $3.39 $8.75 $9.25 $4.45 $1.25 $11.25 $2.15 $4.45 $2.50 $2.50 $8.99 $3.99 $8.75 $2.15 $11.75 $11.75 $1.25 $8.75 $9.39 $11.25 $9.25 $9.25 $2.95 $9.25 $4.45 $1.25 $9.25 $8.75 $11.75 $1.50 $8.75 $4.45 $8.99 $1.09 $9.25 $2.95 $8.99 $1.69 $8.69 $1.69 $11.25 $4.45 $8.75 $8.75 $4.45 $11.25 $8.75 $2.95 $1.50 $8.19 $8.69 $1.09 $1.69 $8.49 $8.75 $2.95 $1.25 $8.49 $3.99 $10.98 $3.39 $11.25 $11.25 $2.15 $18.50 $18.50 $8.49 $8.49 $11.25 $1.50 $8.49 $2.39 $8.99 $2.39 $11.75 $4.45 $17.50 $17.50 $9.25 $9.25 $8.75 $4.45 $3.75 $3.75 $3.75 $8.75 $4.45 $11.75 $2.95 $1.25 $4.45 $8.75 $1.25 $1.50 $9.25 $11.25 $11.25 $11.25 $11.25 $9.25 $11.25 $4.45 $8.75 $9.25 $8.75 $8.75 $4.45 $8.75 $1.25 $2.15 $8.75 $8.75 $4.30 $4.30 $8.75 $1.25 $2.95 $9.25 $2.95 $4.45 $11.25 $11.25 $9.25 $9.25 $4.50 $4.50 $4.50 $11.75 $1.25 $11.75 $11.75 $1.25 $1.25 $9.25 $4.45 $11.25 $11.75 $17.50 $17.50 $2.15 $1.25 $11.25 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $11.25 $4.45 $4.45 $2.95 $11.25 $2.15 $1.25 $1.50 $8.75 $11.25 $2.95 $11.25 $1.25 $2.15 $11.25 $9.25 $6.49 $1.25 $8.75 $2.15 $8.75 $6.49 $11.25 $1.50 $8.75 $4.45 $8.75 $4.45 $9.25 $9.25 $1.25 $1.25 $8.75 $4.50 $4.50 $4.50 $11.25 $1.25 $1.50 $9.25 $2.15 $11.25 $4.45 $11.25 $4.45 $8.75 $4.45 $9.25 $4.45 $1.25 $11.25 $4.45 $8.75 $4.45 $8.75 $2.15 $8.75 $4.45 $8.75 $11.75 $1.50 $11.25 $4.45 $8.75 $2.15 $1.50 $8.75 $4.45 $8.75 $11.75 $8.75 $8.75 $11.25 $11.25 $1.50 $8.75 $2.15 $11.75 $2.15 $9.25 $2.95 $18.50 $18.50 $1.25 $4.45 $8.50 $8.50 $8.50 $8.50 $11.25 $11.89 $1.25 $9.39 $4.45 $8.75 $2.15 $1.50 $11.75 $8.75 $9.25 $9.25 $4.45 $1.25 $11.25 $9.25 $2.95 $8.75 $8.75 $2.15 $8.75 $11.25 $11.25 $11.25 $11.75 $11.25 $2.15 $11.25 $2.15 $8.99 $8.99 $8.75 $9.25 $9.25 $11.25 $8.75 $4.45 $8.75 $1.25 $4.45 $11.25 $1.25 $8.75 $8.75 $11.25 $8.75 $1.25 $1.25 $1.25 $9.25 $11.75 $2.15 $8.75 $4.45 $8.75 $4.45 $11.25 $9.25 $8.75 $9.25 $4.45 $11.75 $4.45 $1.25 $4.45 $11.75 $9.25 $11.25 $2.15 $23.50 $23.50 $9.25 $2.15 $18.50 $18.50 $8.75 $5.90 $5.90 $11.89 $4.45 $8.75 $4.45 $9.25 $2.95 $11.25 $2.95 $11.25 $11.25 $11.25 $2.95 $11.75 $9.25 $9.25 $2.95 $11.25 $2.15 $9.25 $8.90 $8.90 $8.75 $11.25 $11.25 $11.25 $8.75 $2.15 $1.25 $11.25 $1.25 $8.75 $4.45 $1.25 $11.25 $11.25 $11.75 $1.25 $11.25 $11.25 $11.25 $8.75 $8.75 $18.50 $18.50 $11.75 $1.25 $4.45 $9.25 $6.49 $4.45 $8.75 $11.25 $6.49 $11.75 $8.75 $9.25 $11.25 $2.15 $8.75 $4.45 $11.25 $4.45 $8.75 $9.25 $4.45 $1.25 $1.25 $8.75 $11.25 $11.75 $2.15 $11.75 $4.45 $11.75 $1.25 $11.75 $11.75 $11.25 $4.30 $4.30 $9.39 $9.39 $8.75 $1.25 $9.25 $4.45 $11.25 $1.25 $8.75 $1.25 $2.95 $11.25 $4.45 $8.75 $1.50 $4.45 $4.45 $9.25 $8.75 $2.95 $1.25 $11.25 $2.95 $8.75 $8.75 $8.75 $8.75 $4.30 $4.30 $9.25 $9.39 $4.45 $9.25 $1.25 $22.50 $22.50 $4.45 $2.95 $2.15 $23.50 $23.50 $11.75 $2.15 $1.25 $9.25 $4.45 $11.25 $11.75 $17.50 $17.50 $8.75 $11.75 $11.25 $8.75 $4.45 $11.75 $1.50 $8.75 $8.75 $11.75 $9.25 $11.25 $4.45 $11.75 $9.25 $4.45 $11.25 $8.75 $8.75 $2.15 $1.50 $8.75 $4.45 $9.25 $8.75 $1.50 $1.25 $1.25 $1.25 $8.75 $2.95 $11.25 $11.25 $1.50 $11.75 $11.25 $2.15 $9.25 $8.75 $11.75 $2.95 $1.50 $8.75 $1.50 $1.25 $8.75 $11.75 $11.25 $11.25 $11.75 $11.25 $11.75 $8.75 $17.80 $17.80 $17.80 $17.80 $5.00 $5.00 $5.00 $5.00 $5.00 $5.00 $5.00 $5.00 $8.75 $2.95 $1.25 $11.25 $1.25 $2.15 $11.25 $2.50 $2.50 $9.25 $1.25 $11.25 $2.95 $11.75 $2.15 $11.25 $1.50 $8.99 $1.99 $11.49 $8.75 $4.45 $1.25 $8.75 $4.45 $1.25 $1.50 $11.75 $8.75 $8.75 $11.25 $6.49 $11.75 $8.75 $2.15 $1.25 $6.49 $8.75 $4.45 $8.75 $4.45 $8.75 $11.25 $4.45 $6.49 $9.25 $8.75 $1.25 $4.45 $11.25 $8.75 $1.50 $8.75 $1.50 $1.25 $9.25 $9.39 $4.45 $9.25 $8.75 $4.45 $1.25 $11.25 $11.75 $8.75 $11.25 $9.25 $8.75 $11.25 $2.50 $2.50 $17.50 $17.50 $9.25 $4.45 $11.25 $1.25 $8.75 $4.45 $1.50 $8.75 $1.50 $1.25 $9.39 $8.75 $8.75 $4.45 $11.25 $1.25 $9.25 $4.45 $11.25 $8.75 $3.00 $3.00 $8.75 $2.15 $1.25 $11.25 $11.25 $4.45 $11.25 $11.25 $8.75 $11.75 $11.75 $11.75 $8.75 $4.45 $1.25 $1.50 $8.75 $4.45 $1.25 $9.25 $9.25 $8.75 $4.45 $1.25 $11.75 $11.25 $1.25 $11.75 $11.25 $9.25 $2.15 $1.50 $8.75 $4.45 $11.75 $11.75 $11.25 $8.75 $8.75 ' to numeric\n\n\n\n\n# Solution 2\n\nchipo.groupby(by=['order_id']).sum()['revenue'].mean()\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[9], line 3\n      1 # Solution 2\n----&gt; 3 chipo.groupby(by=['order_id']).sum()['revenue'].mean()\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:6549, in Series.mean(self, axis, skipna, numeric_only, **kwargs)\n   6541 @doc(make_doc(\"mean\", ndim=1))\n   6542 def mean(\n   6543     self,\n   (...)\n   6547     **kwargs,\n   6548 ):\n-&gt; 6549     return NDFrame.mean(self, axis, skipna, numeric_only, **kwargs)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:12420, in NDFrame.mean(self, axis, skipna, numeric_only, **kwargs)\n  12413 def mean(\n  12414     self,\n  12415     axis: Axis | None = 0,\n   (...)\n  12418     **kwargs,\n  12419 ) -&gt; Series | float:\n&gt; 12420     return self._stat_function(\n  12421         \"mean\", nanops.nanmean, axis, skipna, numeric_only, **kwargs\n  12422     )\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:12377, in NDFrame._stat_function(self, name, func, axis, skipna, numeric_only, **kwargs)\n  12373 nv.validate_func(name, (), kwargs)\n  12375 validate_bool_kwarg(skipna, \"skipna\", none_allowed=False)\n&gt; 12377 return self._reduce(\n  12378     func, name=name, axis=axis, skipna=skipna, numeric_only=numeric_only\n  12379 )\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:6457, in Series._reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\n   6452     # GH#47500 - change to TypeError to match other methods\n   6453     raise TypeError(\n   6454         f\"Series.{name} does not allow {kwd_name}={numeric_only} \"\n   6455         \"with non-numeric dtypes.\"\n   6456     )\n-&gt; 6457 return op(delegate, skipna=skipna, **kwds)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:147, in bottleneck_switch.__call__.&lt;locals&gt;.f(values, axis, skipna, **kwds)\n    145         result = alt(values, axis=axis, skipna=skipna, **kwds)\n    146 else:\n--&gt; 147     result = alt(values, axis=axis, skipna=skipna, **kwds)\n    149 return result\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:404, in _datetimelike_compat.&lt;locals&gt;.new_func(values, axis, skipna, mask, **kwargs)\n    401 if datetimelike and mask is None:\n    402     mask = isna(values)\n--&gt; 404 result = func(values, axis=axis, skipna=skipna, mask=mask, **kwargs)\n    406 if datetimelike:\n    407     result = _wrap_results(result, orig_values.dtype, fill_value=iNaT)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:720, in nanmean(values, axis, skipna, mask)\n    718 count = _get_counts(values.shape, mask, axis, dtype=dtype_count)\n    719 the_sum = values.sum(axis, dtype=dtype_sum)\n--&gt; 720 the_sum = _ensure_numeric(the_sum)\n    722 if axis is not None and getattr(the_sum, \"ndim\", False):\n    723     count = cast(np.ndarray, count)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:1701, in _ensure_numeric(x)\n   1698 elif not (is_float(x) or is_integer(x) or is_complex(x)):\n   1699     if isinstance(x, str):\n   1700         # GH#44008, GH#36703 avoid casting e.g. strings to numeric\n-&gt; 1701         raise TypeError(f\"Could not convert string '{x}' to numeric\")\n   1702     try:\n   1703         x = float(x)\n\nTypeError: Could not convert string '$2.39 $3.39 $3.39 $2.39 $16.98 $16.98 $10.98 $1.69 $11.75 $9.25 $9.25 $4.45 $8.75 $8.75 $11.25 $4.45 $2.39 $8.49 $8.49 $2.18 $2.18 $8.75 $4.45 $8.99 $3.39 $10.98 $3.39 $2.39 $8.49 $8.99 $1.09 $8.49 $2.39 $8.99 $1.69 $8.99 $1.09 $8.75 $8.75 $4.45 $2.95 $11.75 $2.15 $4.45 $11.25 $11.75 $8.75 $10.98 $8.99 $3.39 $8.99 $3.99 $8.99 $2.18 $2.18 $10.98 $1.09 $8.99 $2.39 $9.25 $11.25 $11.75 $2.15 $4.45 $9.25 $11.25 $8.75 $8.99 $8.99 $3.39 $8.99 $10.98 $8.99 $1.69 $8.99 $3.99 $8.75 $4.45 $8.75 $8.75 $2.15 $8.75 $11.25 $2.15 $9.25 $8.75 $8.75 $9.25 $8.49 $8.99 $1.09 $9.25 $2.95 $11.75 $11.75 $9.25 $11.75 $4.45 $9.25 $4.45 $11.75 $8.75 $8.75 $4.45 $8.99 $8.99 $3.99 $8.49 $3.39 $8.99 $1.09 $9.25 $4.45 $8.75 $2.95 $4.45 $2.39 $8.49 $8.99 $8.49 $1.09 $8.99 $3.99 $8.75 $9.25 $4.45 $11.25 $4.45 $8.99 $1.09 $9.25 $2.95 $4.45 $11.75 $4.45 $8.49 $2.39 $10.98 $22.50 $22.50 $11.75 $4.45 $11.25 $4.45 $11.25 $4.45 $11.25 $11.25 $11.75 $9.25 $4.45 $11.48 $17.98 $17.98 $1.69 $17.50 $17.50 $4.45 $8.49 $2.39 $17.50 $17.50 $4.45 $4.45 $11.25 $11.75 $10.98 $8.49 $10.98 $2.18 $2.18 $11.48 $8.49 $2.39 $4.45 $11.25 $11.75 $8.75 $8.49 $2.18 $2.18 $8.49 $3.39 $8.49 $8.99 $10.98 $11.48 $8.49 $1.09 $1.09 $9.25 $8.75 $2.95 $9.25 $4.45 $11.25 $11.48 $8.49 $8.49 $8.99 $2.39 $11.25 $8.75 $2.95 $1.09 $8.99 $8.49 $2.39 $10.98 $1.09 $3.99 $11.25 $8.75 $8.49 $3.39 $8.75 $9.25 $2.15 $11.25 $11.25 $11.25 $4.45 $22.50 $22.50 $4.45 $11.75 $8.75 $17.50 $17.50 $8.75 $9.25 $8.75 $2.15 $9.25 $4.30 $4.30 $8.75 $11.25 $2.15 $8.99 $1.09 $8.99 $3.99 $8.75 $2.95 $2.95 $11.75 $5.90 $5.90 $9.25 $9.25 $11.75 $9.25 $2.95 $17.50 $17.50 $8.75 $9.25 $10.98 $8.99 $1.09 $1.09 $1.09 $8.99 $10.98 $1.09 $8.75 $8.75 $9.25 $9.25 $8.75 $8.75 $8.99 $8.99 $8.99 $1.09 $11.75 $1.25 $8.99 $2.39 $9.25 $2.95 $8.99 $3.99 $8.49 $2.39 $8.49 $8.49 $8.49 $1.69 $8.49 $3.99 $8.99 $1.69 $1.09 $23.78 $23.78 $17.50 $17.50 $2.15 $8.75 $9.25 $9.25 $8.75 $4.45 $8.75 $11.25 $11.25 $1.25 $9.25 $4.45 $11.25 $11.75 $11.75 $6.49 $8.99 $2.39 $8.49 $2.39 $11.25 $8.75 $2.15 $8.99 $1.69 $8.75 $11.25 $2.15 $4.45 $8.75 $8.49 $8.99 $17.50 $17.50 $8.49 $1.09 $1.09 $8.75 $1.25 $2.15 $11.08 $8.49 $8.49 $8.99 $2.39 $8.75 $2.15 $1.50 $11.25 $2.15 $8.49 $8.49 $11.75 $9.25 $11.75 $1.25 $11.25 $8.75 $4.45 $6.49 $9.25 $2.95 $11.25 $4.45 $1.25 $1.25 $8.49 $2.39 $2.18 $2.18 $8.49 $2.18 $2.18 $22.16 $22.16 $17.50 $17.50 $8.75 $2.95 $6.49 $8.99 $3.39 $3.39 $8.99 $8.49 $11.25 $2.15 $11.25 $2.95 $11.25 $1.25 $8.99 $1.09 $8.75 $8.75 $9.25 $2.95 $11.75 $1.50 $8.99 $1.09 $11.25 $1.25 $1.25 $11.25 $11.75 $2.15 $8.99 $1.69 $11.75 $6.49 $8.75 $9.25 $11.25 $4.45 $1.25 $11.25 $4.45 $8.49 $8.99 $8.49 $8.99 $11.25 $1.25 $11.75 $1.25 $11.75 $9.25 $4.45 $11.25 $2.15 $32.94 $32.94 $32.94 $1.25 $11.25 $11.48 $1.69 $1.09 $17.50 $17.50 $4.45 $6.49 $9.25 $8.75 $9.25 $9.25 $8.75 $8.75 $2.15 $2.95 $17.50 $17.50 $10.98 $11.48 $11.48 $3.39 $8.99 $1.69 $8.99 $1.09 $10.98 $3.39 $8.99 $1.09 $9.25 $8.75 $11.25 $4.45 $2.95 $9.25 $22.20 $22.20 $22.20 $8.49 $8.99 $8.75 $8.75 $11.75 $8.75 $11.25 $9.25 $11.25 $11.25 $8.75 $11.25 $2.95 $1.25 $8.49 $1.69 $11.75 $11.25 $8.75 $8.75 $4.45 $8.49 $3.99 $8.49 $3.99 $11.48 $1.69 $1.09 $11.25 $1.50 $10.58 $1.69 $9.25 $11.25 $8.75 $9.25 $11.25 $11.25 $8.75 $11.75 $8.75 $8.75 $8.75 $2.15 $11.25 $11.75 $2.50 $2.50 $4.45 $9.25 $4.45 $11.25 $8.49 $3.99 $9.25 $9.25 $11.25 $9.25 $11.75 $11.25 $1.25 $23.50 $23.50 $1.25 $8.99 $8.49 $7.40 $7.40 $8.75 $1.25 $4.45 $8.75 $2.15 $8.75 $4.45 $7.40 $7.40 $7.40 $8.99 $3.99 $8.99 $1.69 $8.75 $8.75 $8.75 $8.75 $11.25 $11.25 $2.95 $8.75 $18.50 $18.50 $8.49 $3.99 $2.95 $9.25 $9.25 $3.00 $3.00 $1.25 $8.75 $9.25 $4.45 $8.75 $11.25 $4.45 $10.98 $22.16 $22.16 $4.45 $8.75 $9.25 $6.49 $9.25 $11.25 $8.75 $9.25 $2.15 $9.25 $4.45 $9.25 $2.95 $9.25 $8.75 $9.25 $1.25 $1.25 $8.75 $8.75 $9.25 $4.45 $11.75 $11.75 $11.75 $9.25 $9.25 $16.98 $16.98 $2.39 $3.39 $3.39 $9.25 $11.75 $11.25 $2.15 $8.75 $9.25 $4.45 $10.98 $11.25 $9.25 $22.50 $22.50 $9.25 $2.95 $1.50 $11.48 $8.49 $1.69 $8.49 $8.49 $8.49 $6.78 $6.78 $11.75 $4.45 $8.75 $4.45 $11.89 $9.39 $8.75 $2.95 $1.25 $9.25 $8.75 $23.78 $23.78 $8.75 $9.25 $2.15 $2.15 $1.25 $8.49 $3.99 $10.98 $1.09 $8.75 $4.45 $8.75 $11.75 $2.95 $4.45 $9.25 $8.75 $8.49 $3.99 $22.50 $22.50 $11.25 $1.25 $8.75 $8.75 $18.50 $18.50 $6.49 $8.75 $8.75 $4.45 $8.49 $3.99 $8.99 $1.09 $8.49 $2.39 $11.48 $1.69 $2.50 $2.50 $9.25 $1.50 $17.50 $17.50 $2.95 $8.75 $4.45 $11.75 $8.75 $8.49 $1.69 $8.49 $3.99 $8.99 $8.99 $3.99 $8.99 $11.25 $4.45 $1.25 $3.99 $10.98 $7.40 $3.00 $7.40 $4.00 $8.49 $3.99 $9.25 $4.45 $11.25 $1.25 $11.75 $1.25 $11.25 $2.15 $11.25 $4.45 $3.75 $3.75 $3.75 $11.75 $8.99 $2.39 $8.75 $4.45 $1.25 $8.99 $8.49 $2.18 $2.18 $8.49 $2.18 $2.18 $1.09 $8.75 $2.95 $1.25 $1.50 $11.25 $9.25 $2.95 $1.25 $8.49 $3.99 $11.48 $3.99 $8.49 $11.25 $1.25 $8.99 $1.69 $11.25 $1.25 $6.49 $8.75 $9.25 $8.75 $2.95 $8.75 $11.75 $8.69 $8.69 $2.29 $3.99 $8.49 $8.75 $8.75 $1.25 $11.75 $11.25 $11.25 $11.25 $1.25 $9.25 $11.75 $6.49 $3.99 $8.49 $11.25 $2.15 $11.25 $11.89 $8.99 $1.69 $8.99 $8.99 $3.99 $8.99 $9.25 $9.25 $2.15 $7.40 $7.40 $8.75 $8.75 $9.25 $4.45 $11.25 $1.25 $11.75 $11.25 $1.25 $3.99 $8.49 $8.49 $8.49 $8.99 $8.75 $2.15 $1.25 $8.49 $1.09 $1.09 $8.75 $2.95 $1.25 $9.25 $1.25 $2.15 $11.25 $1.25 $4.45 $8.75 $2.50 $2.50 $8.90 $8.90 $8.75 $8.75 $8.75 $11.25 $11.25 $10.98 $3.99 $10.98 $3.99 $1.69 $8.99 $9.25 $8.75 $8.99 $1.09 $9.25 $2.95 $8.75 $9.25 $3.99 $8.49 $8.75 $8.75 $22.50 $22.50 $10.98 $3.27 $3.27 $3.27 $3.99 $8.99 $1.09 $11.08 $8.75 $4.45 $11.08 $3.99 $8.49 $4.30 $4.30 $9.25 $8.75 $11.25 $11.25 $9.25 $8.49 $8.99 $8.49 $8.75 $2.95 $4.45 $9.25 $2.95 $9.25 $8.75 $11.25 $4.45 $16.98 $16.98 $8.49 $2.39 $11.25 $3.75 $3.75 $3.75 $9.25 $4.45 $9.25 $9.25 $4.45 $8.75 $9.25 $8.75 $9.25 $9.25 $9.25 $11.48 $8.99 $22.50 $22.50 $11.75 $11.25 $1.25 $8.75 $2.15 $1.25 $11.25 $8.75 $1.25 $11.25 $1.50 $11.25 $11.25 $9.25 $6.49 $8.90 $8.90 $8.75 $4.45 $11.25 $1.25 $17.50 $17.50 $9.25 $8.75 $11.75 $3.00 $3.00 $8.49 $8.49 $10.98 $8.99 $3.99 $8.75 $4.45 $8.99 $1.69 $11.75 $8.75 $11.25 $4.45 $11.75 $1.25 $11.75 $2.95 $8.99 $8.99 $2.18 $2.18 $17.98 $17.98 $8.99 $8.49 $1.69 $11.75 $11.25 $2.95 $3.75 $3.75 $3.75 $9.25 $11.75 $8.75 $2.15 $1.50 $8.49 $8.49 $3.39 $8.69 $3.89 $8.75 $4.45 $8.75 $11.25 $2.15 $8.75 $8.49 $1.69 $8.49 $8.49 $1.25 $8.75 $11.75 $11.75 $8.99 $1.09 $8.75 $4.45 $8.75 $2.95 $8.75 $2.15 $3.99 $8.49 $8.99 $3.99 $8.49 $1.69 $1.09 $8.99 $1.09 $9.25 $8.75 $8.99 $2.39 $1.25 $1.25 $11.25 $11.25 $9.25 $9.25 $11.25 $1.50 $3.99 $8.49 $11.25 $9.25 $11.25 $17.50 $17.50 $8.75 $8.90 $8.90 $8.75 $8.75 $8.99 $2.39 $11.25 $9.25 $2.15 $11.25 $1.25 $11.75 $1.25 $11.25 $11.75 $1.25 $11.25 $11.25 $8.49 $10.98 $8.75 $1.25 $8.75 $8.49 $8.49 $1.50 $1.50 $8.75 $4.45 $11.25 $1.25 $11.75 $8.49 $2.39 $9.25 $4.45 $9.25 $8.75 $8.99 $1.69 $17.50 $17.50 $2.39 $8.99 $8.99 $11.25 $4.45 $8.75 $4.45 $9.25 $6.49 $10.98 $8.49 $8.49 $1.09 $1.69 $9.25 $4.45 $8.75 $1.25 $2.95 $3.99 $8.49 $11.75 $11.75 $2.15 $11.48 $8.75 $2.15 $1.25 $11.25 $2.15 $1.25 $8.75 $8.75 $6.49 $1.69 $8.99 $8.75 $11.75 $10.98 $1.09 $8.49 $3.39 $8.75 $2.15 $1.25 $11.48 $10.98 $10.98 $8.49 $2.95 $9.25 $9.25 $11.75 $4.45 $11.48 $11.25 $8.75 $4.45 $1.69 $8.99 $8.75 $4.45 $1.50 $11.75 $2.15 $8.99 $2.39 $8.75 $2.95 $1.25 $8.75 $2.15 $1.25 $2.18 $2.18 $2.18 $2.18 $11.48 $8.75 $2.95 $11.75 $11.75 $1.25 $10.58 $8.99 $2.39 $11.75 $4.45 $11.25 $11.25 $17.50 $17.50 $8.75 $8.75 $8.75 $22.50 $22.50 $9.25 $8.75 $4.45 $11.75 $1.25 $11.25 $11.25 $2.95 $8.99 $1.69 $11.25 $4.45 $8.75 $6.49 $8.75 $4.45 $9.25 $4.45 $11.75 $11.75 $4.45 $11.89 $11.75 $11.25 $2.95 $1.50 $4.45 $8.75 $8.99 $1.09 $8.99 $1.09 $3.99 $11.48 $8.49 $9.25 $4.45 $11.48 $9.25 $2.95 $9.25 $8.49 $8.99 $8.99 $8.49 $8.75 $2.95 $4.45 $11.89 $10.58 $8.19 $1.69 $8.75 $2.15 $1.25 $17.50 $17.50 $6.49 $9.25 $2.15 $8.75 $4.45 $8.75 $1.25 $11.48 $11.48 $8.99 $2.18 $2.18 $8.49 $8.99 $2.39 $2.39 $2.18 $2.18 $8.75 $4.45 $11.25 $9.25 $9.25 $11.25 $11.25 $4.45 $2.95 $11.75 $8.49 $8.49 $8.99 $1.69 $9.25 $11.25 $11.75 $9.25 $8.75 $11.75 $8.75 $8.75 $11.25 $11.25 $10.98 $11.25 $4.45 $10.98 $8.49 $8.99 $3.39 $3.99 $8.99 $1.09 $1.09 $2.39 $17.50 $17.50 $4.45 $11.25 $11.25 $4.45 $9.25 $4.45 $8.75 $2.15 $1.25 $11.89 $2.95 $11.75 $1.25 $11.25 $4.45 $11.48 $11.48 $2.95 $9.25 $8.75 $9.25 $2.95 $11.25 $1.25 $11.75 $1.25 $8.99 $2.39 $1.25 $11.25 $1.25 $11.25 $8.49 $3.99 $35.00 $35.00 $35.00 $35.00 $27.75 $27.75 $27.75 $8.75 $11.80 $11.80 $11.80 $11.80 $8.90 $8.90 $5.90 $5.90 $6.49 $10.98 $17.98 $17.98 $2.39 $9.25 $8.75 $2.15 $8.75 $4.45 $8.49 $1.69 $8.19 $8.69 $10.98 $3.99 $11.48 $11.48 $4.45 $8.75 $6.49 $8.75 $8.75 $9.25 $1.25 $4.45 $8.49 $1.69 $9.25 $4.45 $8.99 $1.09 $11.25 $2.95 $11.08 $11.08 $3.89 $10.98 $11.25 $8.75 $11.25 $9.25 $4.30 $4.30 $8.75 $8.49 $3.99 $1.69 $8.99 $8.49 $1.69 $11.75 $11.25 $11.89 $9.25 $2.95 $9.25 $2.95 $8.75 $4.45 $4.45 $8.75 $10.98 $11.48 $8.49 $9.25 $4.45 $11.75 $11.89 $8.99 $8.49 $8.75 $9.25 $8.75 $8.75 $11.75 $11.75 $4.45 $11.25 $11.75 $2.50 $2.50 $8.99 $1.69 $11.75 $2.15 $1.25 $9.25 $8.75 $8.90 $8.90 $9.25 $2.95 $8.75 $11.25 $8.90 $8.90 $11.25 $11.75 $11.48 $1.69 $3.39 $9.25 $2.95 $8.99 $1.69 $8.49 $10.98 $11.25 $2.95 $8.99 $1.69 $8.75 $2.15 $1.25 $8.75 $2.95 $9.25 $2.50 $2.50 $11.25 $1.25 $11.75 $2.50 $2.50 $11.25 $1.50 $8.75 $1.25 $2.95 $11.48 $11.48 $8.75 $8.75 $2.15 $11.75 $1.25 $9.25 $9.25 $6.49 $11.75 $8.49 $8.49 $1.09 $10.98 $8.75 $1.25 $2.15 $11.25 $1.50 $11.25 $11.25 $8.49 $8.49 $8.75 $1.50 $1.25 $1.50 $8.75 $2.50 $2.50 $2.15 $7.40 $7.40 $4.00 $9.25 $9.39 $9.25 $9.25 $9.39 $11.25 $8.90 $8.90 $11.25 $6.00 $6.00 $6.00 $6.00 $11.25 $11.25 $11.25 $22.50 $22.50 $11.48 $1.09 $8.49 $8.49 $17.50 $17.50 $11.25 $1.50 $9.25 $8.75 $3.99 $8.49 $8.75 $8.75 $8.75 $8.75 $8.75 $11.75 $1.50 $11.25 $11.25 $2.95 $8.99 $10.98 $9.25 $8.75 $4.45 $8.49 $1.09 $2.39 $8.75 $8.75 $11.48 $8.99 $8.49 $8.49 $2.39 $10.98 $8.49 $3.99 $11.75 $4.45 $8.75 $2.15 $1.25 $10.98 $8.99 $11.25 $1.50 $8.75 $2.15 $1.25 $8.75 $9.25 $8.75 $11.25 $1.50 $8.75 $1.25 $4.45 $10.98 $8.75 $2.95 $1.25 $8.75 $2.95 $1.25 $8.49 $8.49 $2.39 $11.25 $1.25 $8.75 $8.75 $9.25 $8.75 $11.89 $1.25 $8.75 $2.15 $1.25 $8.99 $1.09 $8.75 $4.45 $26.25 $26.25 $26.25 $8.75 $4.45 $11.75 $2.95 $8.75 $8.75 $11.75 $8.75 $11.25 $11.25 $11.25 $4.45 $1.25 $8.49 $8.49 $8.49 $8.99 $8.99 $2.39 $2.39 $3.99 $8.75 $4.45 $2.15 $9.25 $1.25 $11.25 $11.75 $8.75 $4.45 $11.25 $2.15 $8.75 $4.45 $8.75 $8.75 $1.25 $11.25 $2.15 $8.75 $5.90 $5.90 $11.75 $1.25 $9.25 $3.75 $3.75 $3.75 $8.75 $1.25 $4.45 $11.75 $4.45 $8.75 $23.50 $23.50 $8.75 $2.95 $8.75 $8.75 $11.89 $4.45 $2.95 $1.25 $8.75 $4.45 $2.95 $1.25 $8.75 $2.15 $1.25 $11.75 $2.95 $8.99 $3.39 $9.25 $9.25 $17.50 $17.50 $2.95 $11.89 $1.50 $11.25 $2.95 $9.25 $11.25 $11.25 $2.95 $8.75 $9.25 $4.30 $4.30 $8.75 $8.75 $11.25 $8.75 $4.30 $4.30 $8.75 $1.25 $2.15 $8.49 $8.49 $3.39 $3.39 $10.98 $10.98 $2.39 $11.25 $11.75 $11.75 $1.25 $5.90 $5.90 $8.75 $11.25 $9.25 $4.45 $1.50 $3.39 $8.99 $2.39 $11.25 $2.15 $11.25 $11.75 $11.75 $4.45 $11.75 $4.45 $9.25 $8.75 $8.49 $8.99 $8.49 $8.99 $11.75 $8.75 $8.49 $3.99 $3.89 $11.08 $8.49 $8.99 $8.49 $8.49 $8.49 $11.25 $2.15 $17.50 $17.50 $8.75 $2.95 $8.49 $8.49 $10.98 $1.09 $11.25 $2.15 $2.95 $1.25 $8.75 $9.25 $9.25 $9.25 $2.95 $8.75 $2.15 $1.25 $8.99 $3.99 $11.75 $2.15 $8.99 $3.39 $9.25 $8.75 $11.25 $11.25 $4.45 $8.75 $2.15 $1.25 $11.75 $4.45 $9.25 $2.95 $8.49 $8.49 $11.25 $8.75 $4.45 $11.25 $11.25 $11.25 $11.25 $4.45 $8.49 $1.69 $8.49 $3.39 $8.75 $11.25 $9.25 $8.75 $11.25 $11.25 $11.75 $11.25 $11.75 $11.25 $11.75 $21.96 $21.96 $10.98 $1.69 $11.48 $8.99 $8.49 $1.69 $9.25 $2.15 $1.50 $11.25 $1.50 $8.75 $8.75 $2.95 $8.49 $1.69 $8.75 $2.95 $1.25 $11.25 $2.15 $11.08 $8.49 $8.49 $8.49 $11.75 $1.25 $11.75 $8.75 $8.75 $8.75 $4.45 $11.25 $1.50 $23.50 $23.50 $11.75 $6.49 $8.75 $4.45 $6.49 $8.75 $2.50 $2.50 $2.15 $8.49 $2.39 $8.75 $11.75 $4.45 $8.99 $10.98 $9.25 $2.95 $9.25 $9.25 $11.75 $8.75 $8.75 $8.75 $10.98 $11.25 $9.25 $8.75 $8.75 $2.15 $11.25 $2.15 $4.45 $11.75 $8.49 $2.39 $9.25 $1.25 $1.25 $1.25 $1.25 $8.75 $2.15 $8.49 $1.69 $11.25 $1.50 $8.75 $8.75 $8.49 $3.99 $8.99 $1.09 $11.25 $1.25 $8.49 $2.39 $8.49 $8.75 $9.25 $11.25 $4.45 $11.25 $11.89 $8.99 $8.49 $8.75 $4.45 $8.75 $11.75 $11.75 $8.90 $8.90 $9.39 $2.95 $8.49 $3.99 $8.75 $2.15 $1.25 $21.96 $21.96 $8.49 $1.69 $8.75 $4.45 $8.49 $8.99 $8.49 $3.99 $8.75 $8.75 $2.95 $8.75 $17.50 $17.50 $9.25 $2.95 $8.75 $6.49 $4.30 $4.30 $8.75 $8.75 $2.15 $1.50 $8.49 $8.49 $2.39 $9.25 $4.45 $6.49 $11.75 $4.45 $10.98 $1.69 $9.39 $9.25 $9.25 $2.95 $8.75 $2.15 $1.25 $11.25 $9.25 $8.75 $11.25 $8.75 $11.25 $2.50 $2.50 $2.50 $2.50 $6.00 $6.00 $6.00 $6.00 $8.90 $8.90 $5.90 $5.90 $11.25 $11.25 $8.49 $10.98 $8.75 $2.15 $1.50 $9.25 $1.25 $1.50 $2.15 $1.25 $8.75 $2.95 $8.49 $3.99 $11.25 $4.30 $4.30 $11.75 $2.15 $18.50 $18.50 $8.49 $2.39 $8.75 $4.45 $11.75 $8.99 $3.99 $9.25 $9.25 $1.50 $8.75 $2.95 $6.49 $11.75 $8.49 $8.99 $8.75 $4.45 $6.49 $22.50 $22.50 $9.25 $2.95 $8.49 $1.69 $10.98 $8.75 $4.45 $11.25 $2.95 $8.99 $8.49 $2.39 $11.75 $6.49 $11.25 $11.75 $2.95 $8.99 $1.69 $8.99 $2.18 $2.18 $1.09 $8.99 $8.99 $1.09 $8.99 $8.99 $8.49 $10.98 $1.09 $11.75 $9.25 $11.25 $11.25 $2.15 $11.25 $8.75 $4.45 $2.95 $11.75 $1.50 $8.99 $10.98 $2.39 $8.75 $2.15 $9.25 $1.50 $8.75 $2.15 $3.99 $8.99 $6.49 $8.75 $8.90 $8.90 $8.99 $3.99 $17.50 $17.50 $11.25 $1.25 $10.98 $9.25 $4.45 $1.25 $3.00 $3.00 $11.25 $4.45 $4.45 $2.95 $9.25 $11.25 $2.15 $11.25 $11.25 $4.45 $2.95 $9.25 $11.25 $1.25 $8.75 $2.95 $1.25 $8.75 $4.45 $11.48 $11.48 $8.49 $2.39 $11.25 $11.75 $2.15 $1.50 $2.15 $8.75 $11.25 $8.90 $8.90 $11.25 $11.25 $1.25 $4.45 $9.25 $9.25 $8.75 $9.25 $8.75 $8.75 $9.25 $8.75 $11.75 $11.75 $8.75 $8.75 $8.90 $8.90 $2.95 $10.98 $8.49 $8.49 $10.98 $8.99 $8.99 $11.75 $17.50 $17.50 $11.75 $3.99 $8.49 $10.98 $1.69 $17.50 $17.50 $8.99 $2.39 $8.99 $2.39 $1.25 $8.75 $2.95 $11.75 $11.25 $17.50 $17.50 $8.49 $8.49 $2.39 $11.25 $1.50 $8.75 $3.00 $3.00 $1.25 $8.75 $4.45 $11.75 $11.75 $4.45 $21.96 $21.96 $8.75 $4.45 $8.75 $11.25 $9.25 $8.99 $2.39 $9.25 $8.75 $10.98 $8.49 $3.99 $3.39 $11.75 $1.50 $4.45 $9.25 $8.75 $1.25 $11.75 $8.75 $1.50 $8.75 $8.75 $2.15 $1.50 $8.75 $2.95 $8.75 $8.75 $17.50 $17.50 $8.75 $6.49 $4.45 $11.25 $11.25 $4.30 $4.30 $8.75 $11.25 $4.45 $8.99 $2.39 $9.25 $9.25 $9.25 $4.45 $11.75 $11.25 $2.95 $2.15 $11.25 $11.25 $8.75 $2.15 $1.50 $9.25 $4.45 $10.98 $8.99 $2.18 $2.18 $8.75 $4.45 $1.25 $8.99 $2.39 $4.45 $8.75 $10.98 $11.75 $1.50 $10.98 $8.99 $8.49 $3.99 $8.99 $8.49 $3.99 $8.49 $8.49 $8.99 $11.25 $11.25 $10.98 $10.98 $10.98 $2.39 $3.39 $8.75 $1.25 $2.95 $11.75 $1.50 $10.98 $1.69 $4.45 $8.75 $8.75 $8.75 $8.75 $4.45 $9.25 $8.75 $11.25 $8.75 $3.99 $8.99 $8.49 $11.25 $11.25 $8.75 $4.45 $8.75 $4.45 $1.25 $8.75 $8.75 $1.50 $2.15 $11.75 $11.75 $11.75 $11.75 $11.75 $1.50 $8.75 $9.25 $1.25 $8.75 $2.15 $8.99 $1.09 $4.45 $11.25 $11.75 $2.15 $8.75 $8.75 $1.25 $9.25 $2.15 $11.75 $11.25 $8.75 $11.25 $4.45 $8.49 $1.69 $8.75 $8.75 $8.99 $8.49 $9.25 $11.25 $2.95 $4.45 $11.75 $6.49 $11.48 $8.99 $4.36 $4.36 $4.36 $4.36 $11.48 $8.99 $8.49 $11.48 $8.75 $2.15 $1.50 $8.99 $1.69 $11.25 $1.25 $9.25 $9.25 $8.75 $9.25 $8.90 $8.90 $2.15 $9.25 $10.98 $8.49 $8.75 $9.25 $4.30 $4.30 $9.25 $8.75 $8.75 $2.15 $1.25 $8.75 $1.25 $8.75 $5.90 $5.90 $9.25 $8.75 $9.25 $4.45 $9.25 $11.75 $2.50 $2.50 $9.25 $2.15 $9.25 $1.50 $1.25 $11.25 $2.95 $8.75 $8.75 $8.75 $10.98 $8.75 $8.75 $8.75 $2.15 $1.25 $10.98 $8.75 $2.15 $1.50 $8.75 $2.95 $1.25 $9.25 $9.25 $8.49 $2.39 $8.75 $4.45 $9.25 $8.75 $8.75 $8.75 $9.25 $8.75 $9.25 $8.75 $8.75 $8.75 $8.75 $9.25 $8.75 $9.25 $8.75 $9.25 $8.75 $8.75 $8.75 $9.25 $8.75 $9.25 $8.75 $8.99 $8.99 $8.75 $2.95 $1.25 $11.75 $1.50 $11.25 $11.25 $8.75 $1.50 $2.15 $16.98 $16.98 $11.75 $1.50 $8.75 $4.30 $4.30 $1.50 $8.75 $2.95 $1.25 $1.25 $9.25 $4.45 $11.25 $8.75 $4.45 $8.75 $2.15 $1.25 $10.98 $1.69 $8.75 $1.25 $8.75 $1.25 $11.25 $8.75 $8.75 $8.49 $1.69 $9.25 $11.75 $8.49 $2.39 $9.25 $2.95 $6.49 $8.75 $8.75 $9.25 $8.75 $6.78 $6.78 $17.98 $17.98 $3.39 $11.75 $11.25 $8.75 $4.45 $11.75 $9.25 $8.75 $6.49 $8.99 $2.39 $8.75 $11.25 $11.75 $4.45 $8.75 $2.15 $9.25 $9.25 $9.25 $11.89 $11.75 $11.25 $9.25 $9.25 $8.75 $8.75 $8.49 $1.69 $1.09 $11.25 $1.50 $11.25 $11.25 $11.75 $1.50 $8.49 $8.99 $22.50 $22.50 $8.75 $4.30 $4.30 $8.75 $11.25 $2.15 $11.25 $2.95 $4.45 $11.25 $8.49 $3.39 $2.39 $11.75 $2.15 $11.75 $8.99 $2.39 $8.75 $11.75 $11.89 $1.25 $7.50 $7.50 $7.50 $7.50 $7.50 $11.89 $1.09 $8.49 $2.39 $8.75 $8.75 $8.75 $8.75 $9.25 $11.25 $8.75 $8.90 $8.90 $9.25 $8.75 $8.75 $11.75 $3.00 $3.00 $1.50 $11.25 $11.75 $8.99 $10.98 $4.45 $8.75 $2.15 $9.25 $11.25 $4.45 $1.69 $10.98 $9.25 $11.75 $9.25 $4.45 $10.98 $3.99 $8.49 $1.25 $9.25 $4.45 $10.98 $8.75 $8.75 $11.75 $11.25 $8.49 $11.48 $4.45 $1.25 $11.25 $8.99 $1.09 $2.39 $11.25 $2.15 $8.75 $4.45 $8.49 $1.69 $10.98 $1.69 $9.25 $4.45 $11.25 $8.75 $11.25 $11.75 $11.25 $22.50 $22.50 $8.49 $2.39 $2.50 $2.50 $8.75 $8.75 $9.25 $9.25 $11.25 $8.99 $1.09 $8.99 $1.69 $11.75 $1.25 $21.96 $21.96 $8.75 $2.15 $1.25 $8.75 $11.25 $9.25 $11.25 $8.75 $8.75 $11.25 $2.15 $8.99 $1.09 $1.69 $8.75 $2.15 $1.25 $8.49 $1.09 $1.09 $1.69 $11.48 $8.49 $8.49 $4.78 $4.78 $9.25 $1.25 $1.25 $1.25 $11.25 $11.25 $11.75 $4.45 $11.25 $4.45 $8.99 $1.09 $11.25 $2.15 $11.25 $9.25 $11.75 $11.25 $11.25 $9.25 $2.95 $11.25 $4.45 $8.75 $2.95 $2.95 $11.25 $1.50 $10.98 $16.98 $16.98 $18.50 $18.50 $10.98 $3.99 $1.09 $9.25 $9.25 $8.75 $4.45 $17.50 $17.50 $8.75 $4.45 $8.75 $1.25 $4.45 $9.25 $8.75 $8.75 $17.50 $17.50 $4.45 $9.39 $1.25 $2.95 $11.25 $8.75 $8.75 $11.25 $2.15 $8.90 $8.90 $11.25 $11.89 $10.98 $11.25 $4.45 $11.25 $11.25 $8.49 $10.98 $8.49 $3.39 $9.25 $8.75 $2.95 $3.00 $3.00 $9.39 $11.75 $2.95 $1.50 $11.25 $11.75 $8.75 $2.15 $1.50 $8.49 $3.39 $11.75 $1.25 $17.50 $17.50 $11.25 $1.25 $8.75 $2.95 $1.25 $11.25 $11.75 $13.35 $13.35 $13.35 $11.25 $11.75 $11.25 $11.25 $4.45 $11.25 $8.49 $3.39 $9.25 $2.95 $4.78 $4.78 $2.39 $3.99 $8.99 $8.99 $11.25 $11.25 $8.75 $11.25 $2.95 $4.45 $9.25 $8.75 $4.45 $8.49 $8.49 $10.98 $10.98 $3.99 $11.75 $8.75 $11.75 $4.45 $1.50 $1.25 $8.49 $8.49 $8.75 $8.75 $8.75 $9.25 $8.75 $2.95 $1.25 $11.25 $1.50 $11.25 $4.45 $9.25 $8.75 $8.75 $9.25 $8.75 $4.45 $1.50 $8.75 $8.75 $8.49 $1.69 $8.75 $2.15 $9.25 $2.15 $1.50 $11.25 $11.75 $2.15 $6.49 $9.25 $9.25 $11.25 $11.25 $11.75 $11.75 $11.75 $11.25 $8.75 $2.15 $1.25 $11.75 $9.25 $11.25 $8.75 $5.90 $5.90 $8.75 $4.45 $9.25 $9.25 $4.45 $11.25 $4.45 $11.25 $8.75 $2.15 $11.89 $11.25 $8.75 $2.95 $1.50 $8.75 $4.30 $4.30 $8.75 $11.25 $11.75 $11.75 $2.15 $11.25 $8.99 $1.09 $8.49 $8.49 $8.49 $3.39 $8.99 $10.98 $3.99 $11.75 $2.15 $8.75 $4.45 $2.50 $2.50 $11.48 $1.09 $8.49 $8.49 $16.98 $16.98 $3.99 $10.98 $1.09 $8.75 $2.95 $8.75 $8.75 $2.95 $9.25 $11.25 $2.15 $9.25 $4.45 $4.45 $9.25 $11.75 $11.75 $2.15 $9.25 $8.75 $11.25 $6.49 $8.75 $11.25 $2.95 $10.98 $3.99 $1.50 $9.25 $2.15 $8.75 $11.25 $11.89 $4.45 $1.50 $1.25 $8.75 $8.75 $4.45 $11.25 $11.75 $8.49 $1.09 $1.09 $1.69 $8.99 $3.39 $8.99 $1.69 $8.49 $8.99 $3.27 $3.27 $3.27 $8.99 $8.99 $1.09 $10.98 $1.69 $3.99 $8.49 $1.09 $8.75 $8.75 $11.75 $8.75 $9.25 $8.75 $3.39 $8.99 $8.99 $2.39 $9.25 $8.75 $9.25 $9.25 $8.99 $3.99 $2.39 $8.49 $1.09 $8.49 $8.99 $3.39 $11.25 $1.25 $8.99 $3.99 $8.75 $8.90 $8.90 $6.49 $8.75 $9.25 $11.25 $11.25 $11.25 $1.25 $8.75 $9.25 $4.45 $1.25 $8.75 $1.25 $2.15 $17.98 $17.98 $8.99 $8.75 $2.95 $1.25 $11.75 $1.50 $1.50 $8.75 $8.75 $11.08 $8.99 $1.69 $8.99 $1.69 $10.98 $3.99 $3.39 $11.75 $2.15 $11.75 $2.95 $8.75 $8.75 $11.75 $11.25 $11.75 $11.25 $4.45 $11.25 $1.25 $2.18 $2.18 $2.18 $2.18 $2.39 $8.49 $8.99 $2.39 $11.25 $8.75 $11.75 $11.75 $11.25 $4.45 $2.15 $8.19 $10.58 $4.45 $9.25 $1.09 $8.99 $11.25 $1.50 $8.99 $3.99 $4.45 $11.75 $2.15 $11.25 $8.75 $4.45 $8.75 $9.25 $6.45 $6.45 $6.45 $8.75 $11.25 $11.25 $8.75 $11.75 $21.96 $21.96 $8.99 $5.07 $5.07 $5.07 $8.49 $9.25 $11.25 $4.45 $3.39 $8.49 $8.99 $8.49 $17.50 $17.50 $22.96 $22.96 $8.75 $11.25 $11.89 $11.25 $8.49 $1.69 $1.09 $8.99 $8.99 $9.25 $8.75 $9.25 $2.95 $8.49 $3.99 $8.99 $8.49 $7.17 $7.17 $7.17 $8.49 $8.99 $17.50 $17.50 $9.25 $9.25 $11.25 $1.25 $8.99 $1.09 $8.75 $4.45 $11.25 $2.15 $11.75 $11.25 $11.25 $8.75 $8.75 $4.45 $1.25 $11.75 $11.75 $2.50 $2.50 $8.49 $8.99 $2.18 $2.18 $11.25 $4.45 $11.25 $11.75 $8.49 $8.99 $1.69 $1.09 $8.99 $8.99 $11.25 $6.49 $11.25 $8.75 $4.45 $8.99 $1.69 $11.48 $11.75 $2.50 $2.50 $8.49 $1.09 $1.09 $1.69 $8.49 $2.39 $11.75 $1.25 $8.49 $1.69 $8.49 $1.69 $11.75 $4.45 $8.75 $8.75 $4.45 $8.75 $11.25 $11.25 $8.75 $7.98 $7.98 $8.49 $1.09 $8.49 $3.99 $8.49 $3.99 $8.99 $3.99 $11.25 $4.45 $8.49 $2.39 $8.49 $2.39 $3.99 $8.49 $1.25 $11.25 $4.45 $9.25 $4.45 $1.09 $8.99 $3.99 $11.25 $8.90 $8.90 $9.25 $11.25 $8.75 $11.25 $11.25 $11.25 $11.25 $11.25 $8.99 $8.49 $8.75 $8.75 $4.45 $16.98 $16.98 $11.75 $11.25 $9.25 $4.45 $9.25 $2.95 $8.49 $1.69 $3.75 $3.75 $3.75 $4.45 $9.25 $1.50 $11.25 $11.48 $11.25 $2.15 $8.75 $9.39 $8.49 $3.99 $8.19 $2.29 $11.48 $1.69 $11.48 $3.99 $8.49 $1.69 $9.25 $2.95 $8.49 $1.69 $11.25 $4.45 $9.39 $9.25 $8.75 $8.75 $4.45 $11.89 $4.45 $4.45 $8.75 $8.75 $8.75 $2.15 $8.75 $3.75 $3.75 $3.75 $9.25 $11.25 $4.45 $6.49 $16.98 $16.98 $18.50 $18.50 $2.50 $2.50 $2.95 $3.99 $8.49 $8.19 $11.08 $6.49 $11.75 $2.39 $8.99 $1.09 $11.25 $4.45 $11.25 $8.99 $1.69 $21.96 $21.96 $2.18 $2.18 $8.99 $8.99 $2.39 $8.69 $1.69 $8.90 $8.90 $2.50 $2.50 $8.75 $8.99 $1.09 $8.49 $8.49 $8.75 $4.45 $17.50 $17.50 $8.75 $9.25 $8.49 $2.39 $8.75 $4.45 $11.25 $11.25 $11.75 $8.75 $8.49 $8.49 $8.49 $8.99 $8.75 $4.45 $11.48 $8.75 $1.25 $2.15 $9.25 $4.45 $11.75 $2.15 $11.25 $8.99 $2.39 $8.69 $8.69 $11.75 $2.95 $11.75 $1.50 $9.25 $4.45 $1.50 $11.48 $8.99 $2.39 $11.25 $11.89 $2.15 $1.25 $11.75 $4.45 $8.75 $8.75 $11.25 $4.45 $11.25 $2.15 $4.45 $8.49 $1.09 $3.99 $11.25 $11.25 $8.49 $2.39 $8.99 $2.39 $11.25 $2.15 $8.75 $2.95 $1.25 $8.75 $11.25 $17.50 $17.50 $11.75 $11.75 $11.25 $11.25 $4.45 $2.50 $2.50 $8.75 $8.99 $8.99 $1.69 $8.99 $1.69 $11.25 $1.25 $11.08 $8.69 $8.99 $1.09 $11.25 $11.25 $2.95 $1.25 $8.75 $1.25 $8.75 $8.75 $2.15 $1.25 $8.49 $3.99 $8.49 $2.39 $8.49 $7.17 $7.17 $7.17 $8.75 $4.45 $11.48 $8.75 $8.75 $11.48 $8.75 $9.25 $8.49 $3.99 $1.50 $11.25 $11.25 $8.75 $8.75 $4.45 $9.25 $4.45 $8.75 $8.75 $4.30 $4.30 $2.95 $8.75 $4.50 $4.50 $4.50 $9.25 $11.25 $4.45 $11.25 $11.25 $8.75 $9.25 $8.75 $2.15 $1.25 $8.75 $2.15 $1.25 $8.99 $8.49 $8.75 $8.75 $1.25 $11.75 $4.50 $4.50 $4.50 $8.75 $8.75 $3.99 $3.39 $8.49 $2.39 $8.99 $1.50 $11.25 $11.25 $8.75 $8.75 $2.15 $8.75 $2.15 $1.25 $21.96 $21.96 $8.49 $1.69 $26.07 $26.07 $26.07 $11.75 $1.50 $8.99 $8.99 $11.48 $9.25 $9.25 $8.75 $8.75 $8.75 $9.25 $8.75 $8.75 $2.15 $1.25 $11.89 $8.75 $11.75 $4.45 $18.50 $18.50 $9.25 $9.39 $8.49 $2.39 $8.49 $1.69 $1.09 $8.99 $8.49 $2.18 $2.18 $11.25 $1.25 $8.49 $3.39 $8.49 $3.99 $11.25 $8.75 $8.49 $1.69 $16.98 $16.98 $9.25 $9.25 $11.75 $1.25 $11.25 $8.75 $8.49 $8.49 $8.75 $1.25 $1.25 $1.25 $11.25 $12.98 $12.98 $11.75 $11.75 $4.45 $11.25 $11.75 $10.98 $8.49 $8.49 $2.39 $9.25 $11.25 $8.75 $2.95 $1.50 $11.25 $2.95 $9.25 $2.95 $9.25 $8.75 $11.25 $8.75 $8.75 $4.45 $11.25 $1.25 $8.75 $2.95 $2.50 $2.50 $9.25 $9.25 $9.25 $6.49 $17.50 $17.50 $8.49 $1.69 $8.49 $3.99 $8.75 $2.95 $2.95 $8.49 $8.99 $8.99 $1.09 $8.75 $4.45 $1.25 $11.25 $11.75 $11.25 $9.25 $11.25 $1.50 $11.25 $2.15 $10.98 $8.75 $11.75 $2.95 $11.25 $11.25 $9.25 $8.75 $9.25 $8.75 $8.75 $8.75 $1.25 $11.25 $1.50 $2.15 $8.75 $8.75 $11.25 $8.75 $8.75 $11.25 $4.45 $8.49 $8.49 $8.49 $8.49 $8.99 $1.69 $2.39 $1.09 $1.09 $11.25 $2.95 $35.25 $35.25 $35.25 $8.75 $2.95 $1.25 $11.25 $2.15 $9.25 $4.45 $8.75 $8.75 $8.75 $4.45 $1.25 $11.89 $8.75 $2.15 $1.25 $8.49 $1.09 $1.09 $1.69 $8.69 $8.69 $9.25 $2.95 $10.98 $2.39 $22.50 $22.50 $21.96 $21.96 $10.98 $8.49 $1.69 $11.75 $2.95 $8.75 $11.25 $8.75 $9.25 $8.75 $8.19 $10.58 $8.75 $2.95 $1.50 $2.50 $2.50 $9.25 $4.45 $9.25 $11.25 $8.69 $8.69 $3.89 $8.69 $1.69 $4.45 $9.25 $11.25 $4.45 $2.15 $8.19 $8.69 $4.45 $11.25 $1.25 $11.25 $8.75 $11.89 $11.75 $11.75 $9.25 $8.75 $2.15 $1.50 $11.75 $4.45 $3.99 $8.99 $10.98 $2.39 $1.25 $2.95 $8.75 $2.95 $9.25 $9.25 $9.25 $8.75 $2.15 $9.25 $9.25 $3.39 $8.49 $8.49 $2.39 $10.98 $1.09 $1.09 $8.49 $11.25 $1.25 $8.99 $1.09 $8.75 $2.15 $1.50 $3.99 $8.49 $8.75 $2.15 $1.50 $8.49 $10.98 $2.18 $2.18 $8.75 $2.15 $1.50 $4.45 $9.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $10.50 $10.50 $10.50 $10.50 $10.50 $10.50 $10.50 $6.49 $33.75 $33.75 $33.75 $35.00 $35.00 $35.00 $35.00 $27.75 $27.75 $27.75 $3.00 $3.00 $11.25 $11.75 $10.98 $2.39 $2.50 $2.50 $8.75 $4.45 $16.98 $16.98 $8.75 $6.49 $16.98 $16.98 $17.98 $17.98 $16.98 $16.98 $16.98 $16.98 $8.99 $8.99 $8.49 $9.25 $8.75 $2.95 $11.48 $2.39 $8.99 $2.39 $11.25 $4.45 $8.75 $9.25 $6.49 $26.25 $26.25 $26.25 $8.75 $26.25 $26.25 $26.25 $8.75 $8.75 $11.25 $11.25 $2.15 $1.25 $11.75 $8.75 $2.15 $1.50 $11.25 $2.15 $8.99 $2.39 $11.25 $11.25 $2.15 $11.25 $11.25 $8.75 $4.78 $4.78 $21.96 $21.96 $8.49 $2.39 $9.25 $2.95 $16.98 $16.98 $8.19 $3.89 $8.99 $1.09 $8.99 $3.39 $9.25 $4.45 $10.98 $10.98 $17.50 $17.50 $11.25 $1.25 $8.75 $11.75 $4.45 $11.25 $11.25 $8.99 $1.09 $10.98 $8.49 $1.69 $11.25 $9.25 $16.98 $16.98 $8.75 $4.45 $11.25 $6.49 $11.75 $9.25 $9.25 $8.75 $4.45 $2.50 $2.50 $8.75 $11.75 $8.75 $9.25 $11.75 $9.25 $8.75 $9.25 $8.75 $11.25 $11.75 $9.25 $8.75 $11.75 $8.49 $1.09 $1.09 $8.49 $1.09 $1.69 $11.25 $1.25 $8.75 $2.15 $1.50 $8.49 $1.69 $1.25 $8.75 $2.95 $8.49 $3.99 $8.49 $8.49 $8.75 $11.25 $2.15 $1.50 $11.75 $8.99 $1.09 $10.98 $10.98 $11.25 $1.25 $8.75 $4.45 $4.45 $1.25 $11.89 $8.99 $8.99 $11.25 $4.45 $23.50 $23.50 $8.49 $3.99 $9.25 $4.45 $4.45 $9.25 $8.75 $4.45 $8.75 $8.75 $11.75 $6.49 $17.50 $17.50 $4.45 $8.75 $2.95 $1.50 $8.75 $8.75 $8.75 $4.45 $11.25 $11.25 $11.75 $11.25 $2.95 $11.25 $4.45 $3.00 $3.00 $1.25 $2.95 $9.25 $8.99 $2.39 $6.49 $8.75 $8.90 $8.90 $11.48 $1.09 $10.98 $9.25 $9.25 $11.25 $8.75 $11.75 $11.25 $11.25 $1.25 $9.25 $4.45 $9.25 $6.49 $11.75 $11.75 $8.99 $2.39 $8.49 $8.49 $9.25 $9.25 $1.25 $8.75 $2.95 $11.75 $2.15 $8.49 $8.49 $8.69 $16.38 $16.38 $8.19 $3.89 $2.29 $11.75 $8.75 $8.75 $8.75 $4.45 $8.49 $8.49 $9.25 $8.75 $6.49 $2.95 $11.25 $11.25 $2.15 $9.25 $11.75 $21.96 $21.96 $8.49 $3.39 $1.69 $8.49 $8.75 $4.45 $8.49 $3.99 $11.25 $8.75 $11.25 $2.15 $11.75 $4.45 $11.25 $9.25 $8.75 $18.50 $18.50 $1.50 $8.75 $2.15 $11.48 $2.18 $2.18 $3.99 $11.25 $1.50 $8.99 $2.39 $11.75 $1.50 $11.25 $6.49 $4.45 $11.25 $8.49 $3.99 $2.50 $2.50 $8.75 $9.25 $3.99 $8.99 $8.75 $6.49 $13.52 $13.52 $13.52 $13.52 $13.52 $13.52 $13.52 $13.52 $16.98 $16.98 $16.98 $16.98 $17.98 $17.98 $16.98 $16.98 $8.75 $8.75 $11.25 $11.25 $8.49 $1.09 $1.69 $1.25 $9.25 $2.95 $8.69 $8.19 $8.49 $2.39 $8.49 $2.39 $10.98 $8.99 $8.99 $1.69 $8.49 $8.75 $8.75 $11.25 $4.45 $4.45 $17.50 $17.50 $8.75 $4.45 $8.75 $2.15 $1.50 $1.50 $8.99 $1.09 $8.75 $4.45 $8.75 $8.75 $11.25 $4.30 $4.30 $8.49 $1.69 $1.09 $1.09 $8.75 $2.15 $1.50 $8.99 $8.49 $3.99 $8.75 $2.15 $1.25 $9.25 $2.95 $11.25 $4.45 $9.25 $2.95 $3.99 $8.99 $8.49 $8.75 $8.75 $11.25 $9.25 $8.75 $4.45 $11.25 $1.25 $9.25 $11.25 $4.45 $2.95 $10.98 $8.75 $8.75 $18.50 $18.50 $9.25 $9.25 $5.00 $5.00 $5.00 $5.00 $8.75 $2.95 $16.98 $16.98 $11.25 $2.95 $8.75 $2.15 $1.50 $8.49 $2.39 $9.25 $2.15 $1.25 $8.19 $8.69 $8.19 $8.19 $8.75 $2.95 $1.25 $9.25 $2.95 $11.25 $8.75 $11.25 $11.25 $8.99 $1.09 $9.25 $9.25 $4.45 $8.49 $3.99 $2.39 $1.09 $8.99 $8.49 $8.75 $8.75 $11.25 $11.75 $4.45 $2.50 $2.50 $8.75 $8.49 $3.39 $8.75 $9.25 $4.45 $1.25 $11.25 $2.15 $4.45 $2.50 $2.50 $8.99 $3.99 $8.75 $2.15 $11.75 $11.75 $1.25 $8.75 $9.39 $11.25 $9.25 $9.25 $2.95 $9.25 $4.45 $1.25 $9.25 $8.75 $11.75 $1.50 $8.75 $4.45 $8.99 $1.09 $9.25 $2.95 $8.99 $1.69 $8.69 $1.69 $11.25 $4.45 $8.75 $8.75 $4.45 $11.25 $8.75 $2.95 $1.50 $8.19 $8.69 $1.09 $1.69 $8.49 $8.75 $2.95 $1.25 $8.49 $3.99 $10.98 $3.39 $11.25 $11.25 $2.15 $18.50 $18.50 $8.49 $8.49 $11.25 $1.50 $8.49 $2.39 $8.99 $2.39 $11.75 $4.45 $17.50 $17.50 $9.25 $9.25 $8.75 $4.45 $3.75 $3.75 $3.75 $8.75 $4.45 $11.75 $2.95 $1.25 $4.45 $8.75 $1.25 $1.50 $9.25 $11.25 $11.25 $11.25 $11.25 $9.25 $11.25 $4.45 $8.75 $9.25 $8.75 $8.75 $4.45 $8.75 $1.25 $2.15 $8.75 $8.75 $4.30 $4.30 $8.75 $1.25 $2.95 $9.25 $2.95 $4.45 $11.25 $11.25 $9.25 $9.25 $4.50 $4.50 $4.50 $11.75 $1.25 $11.75 $11.75 $1.25 $1.25 $9.25 $4.45 $11.25 $11.75 $17.50 $17.50 $2.15 $1.25 $11.25 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $11.25 $4.45 $4.45 $2.95 $11.25 $2.15 $1.25 $1.50 $8.75 $11.25 $2.95 $11.25 $1.25 $2.15 $11.25 $9.25 $6.49 $1.25 $8.75 $2.15 $8.75 $6.49 $11.25 $1.50 $8.75 $4.45 $8.75 $4.45 $9.25 $9.25 $1.25 $1.25 $8.75 $4.50 $4.50 $4.50 $11.25 $1.25 $1.50 $9.25 $2.15 $11.25 $4.45 $11.25 $4.45 $8.75 $4.45 $9.25 $4.45 $1.25 $11.25 $4.45 $8.75 $4.45 $8.75 $2.15 $8.75 $4.45 $8.75 $11.75 $1.50 $11.25 $4.45 $8.75 $2.15 $1.50 $8.75 $4.45 $8.75 $11.75 $8.75 $8.75 $11.25 $11.25 $1.50 $8.75 $2.15 $11.75 $2.15 $9.25 $2.95 $18.50 $18.50 $1.25 $4.45 $8.50 $8.50 $8.50 $8.50 $11.25 $11.89 $1.25 $9.39 $4.45 $8.75 $2.15 $1.50 $11.75 $8.75 $9.25 $9.25 $4.45 $1.25 $11.25 $9.25 $2.95 $8.75 $8.75 $2.15 $8.75 $11.25 $11.25 $11.25 $11.75 $11.25 $2.15 $11.25 $2.15 $8.99 $8.99 $8.75 $9.25 $9.25 $11.25 $8.75 $4.45 $8.75 $1.25 $4.45 $11.25 $1.25 $8.75 $8.75 $11.25 $8.75 $1.25 $1.25 $1.25 $9.25 $11.75 $2.15 $8.75 $4.45 $8.75 $4.45 $11.25 $9.25 $8.75 $9.25 $4.45 $11.75 $4.45 $1.25 $4.45 $11.75 $9.25 $11.25 $2.15 $23.50 $23.50 $9.25 $2.15 $18.50 $18.50 $8.75 $5.90 $5.90 $11.89 $4.45 $8.75 $4.45 $9.25 $2.95 $11.25 $2.95 $11.25 $11.25 $11.25 $2.95 $11.75 $9.25 $9.25 $2.95 $11.25 $2.15 $9.25 $8.90 $8.90 $8.75 $11.25 $11.25 $11.25 $8.75 $2.15 $1.25 $11.25 $1.25 $8.75 $4.45 $1.25 $11.25 $11.25 $11.75 $1.25 $11.25 $11.25 $11.25 $8.75 $8.75 $18.50 $18.50 $11.75 $1.25 $4.45 $9.25 $6.49 $4.45 $8.75 $11.25 $6.49 $11.75 $8.75 $9.25 $11.25 $2.15 $8.75 $4.45 $11.25 $4.45 $8.75 $9.25 $4.45 $1.25 $1.25 $8.75 $11.25 $11.75 $2.15 $11.75 $4.45 $11.75 $1.25 $11.75 $11.75 $11.25 $4.30 $4.30 $9.39 $9.39 $8.75 $1.25 $9.25 $4.45 $11.25 $1.25 $8.75 $1.25 $2.95 $11.25 $4.45 $8.75 $1.50 $4.45 $4.45 $9.25 $8.75 $2.95 $1.25 $11.25 $2.95 $8.75 $8.75 $8.75 $8.75 $4.30 $4.30 $9.25 $9.39 $4.45 $9.25 $1.25 $22.50 $22.50 $4.45 $2.95 $2.15 $23.50 $23.50 $11.75 $2.15 $1.25 $9.25 $4.45 $11.25 $11.75 $17.50 $17.50 $8.75 $11.75 $11.25 $8.75 $4.45 $11.75 $1.50 $8.75 $8.75 $11.75 $9.25 $11.25 $4.45 $11.75 $9.25 $4.45 $11.25 $8.75 $8.75 $2.15 $1.50 $8.75 $4.45 $9.25 $8.75 $1.50 $1.25 $1.25 $1.25 $8.75 $2.95 $11.25 $11.25 $1.50 $11.75 $11.25 $2.15 $9.25 $8.75 $11.75 $2.95 $1.50 $8.75 $1.50 $1.25 $8.75 $11.75 $11.25 $11.25 $11.75 $11.25 $11.75 $8.75 $17.80 $17.80 $17.80 $17.80 $5.00 $5.00 $5.00 $5.00 $5.00 $5.00 $5.00 $5.00 $8.75 $2.95 $1.25 $11.25 $1.25 $2.15 $11.25 $2.50 $2.50 $9.25 $1.25 $11.25 $2.95 $11.75 $2.15 $11.25 $1.50 $8.99 $1.99 $11.49 $8.75 $4.45 $1.25 $8.75 $4.45 $1.25 $1.50 $11.75 $8.75 $8.75 $11.25 $6.49 $11.75 $8.75 $2.15 $1.25 $6.49 $8.75 $4.45 $8.75 $4.45 $8.75 $11.25 $4.45 $6.49 $9.25 $8.75 $1.25 $4.45 $11.25 $8.75 $1.50 $8.75 $1.50 $1.25 $9.25 $9.39 $4.45 $9.25 $8.75 $4.45 $1.25 $11.25 $11.75 $8.75 $11.25 $9.25 $8.75 $11.25 $2.50 $2.50 $17.50 $17.50 $9.25 $4.45 $11.25 $1.25 $8.75 $4.45 $1.50 $8.75 $1.50 $1.25 $9.39 $8.75 $8.75 $4.45 $11.25 $1.25 $9.25 $4.45 $11.25 $8.75 $3.00 $3.00 $8.75 $2.15 $1.25 $11.25 $11.25 $4.45 $11.25 $11.25 $8.75 $11.75 $11.75 $11.75 $8.75 $4.45 $1.25 $1.50 $8.75 $4.45 $1.25 $9.25 $9.25 $8.75 $4.45 $1.25 $11.75 $11.25 $1.25 $11.75 $11.25 $9.25 $2.15 $1.50 $8.75 $4.45 $11.75 $11.75 $11.25 $8.75 $8.75 ' to numeric\n\n\n\n\n\nStep 17. How many different items are sold?\n\nchipo.item_name.value_counts().count()\n\n50"
  },
  {
    "objectID": "labs/Labexercises/01World-Food-Facts-Exercises-with-solutions.html",
    "href": "labs/Labexercises/01World-Food-Facts-Exercises-with-solutions.html",
    "title": "Exercise 1",
    "section": "",
    "text": "Step 1. Go to https://www.kaggle.com/openfoodfacts/world-food-facts/data\n\n\nStep 2. Download the dataset to your computer and unzip it.\n\nimport pandas as pd\nimport numpy as np\n\n\n\nStep 3. Use the tsv file and assign it to a dataframe called food\n\nfood = pd.read_csv('E:\\Yang Fan\\Lab 1\\en.openfoodfacts.org.products.tsv', sep='\\t')\n\n\n\nStep 4. See the first 5 entries\n\nfood.head()\n\n\n\n\n\n\n\n\ncode\nurl\ncreator\ncreated_t\ncreated_datetime\nlast_modified_t\nlast_modified_datetime\nproduct_name\ngeneric_name\nquantity\n...\nfruits-vegetables-nuts_100g\nfruits-vegetables-nuts-estimate_100g\ncollagen-meat-protein-ratio_100g\ncocoa_100g\nchlorophyl_100g\ncarbon-footprint_100g\nnutrition-score-fr_100g\nnutrition-score-uk_100g\nglycemic-index_100g\nwater-hardness_100g\n\n\n\n\n0\n3087\nhttp://world-en.openfoodfacts.org/product/0000...\nopenfoodfacts-contributors\n1474103866\n2016-09-17T09:17:46Z\n1474103893\n2016-09-17T09:18:13Z\nFarine de blé noir\nNaN\n1kg\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n4530\nhttp://world-en.openfoodfacts.org/product/0000...\nusda-ndb-import\n1489069957\n2017-03-09T14:32:37Z\n1489069957\n2017-03-09T14:32:37Z\nBanana Chips Sweetened (Whole)\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n14.0\n14.0\nNaN\nNaN\n\n\n2\n4559\nhttp://world-en.openfoodfacts.org/product/0000...\nusda-ndb-import\n1489069957\n2017-03-09T14:32:37Z\n1489069957\n2017-03-09T14:32:37Z\nPeanuts\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\n\n\n3\n16087\nhttp://world-en.openfoodfacts.org/product/0000...\nusda-ndb-import\n1489055731\n2017-03-09T10:35:31Z\n1489055731\n2017-03-09T10:35:31Z\nOrganic Salted Nut Mix\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n12.0\n12.0\nNaN\nNaN\n\n\n4\n16094\nhttp://world-en.openfoodfacts.org/product/0000...\nusda-ndb-import\n1489055653\n2017-03-09T10:34:13Z\n1489055653\n2017-03-09T10:34:13Z\nOrganic Polenta\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 163 columns\n\n\n\n\n\nStep 5. What is the number of observations in the dataset?\n\nfood.shape\n\n(356027, 163)\n\n\n\nfood.shape[0]\n\n356027\n\n\n\n\nStep 6. What is the number of columns in the dataset?\n\nprint(food.shape) \nprint(food.shape[1]) \n\n#OR\n\nfood.info() \n\n(356027, 163)\n163\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 356027 entries, 0 to 356026\nColumns: 163 entries, code to water-hardness_100g\ndtypes: float64(107), object(56)\nmemory usage: 442.8+ MB\n\n\n\n\nStep 7. Print the name of all the columns.\n\nfood.columns\n\nIndex(['code', 'url', 'creator', 'created_t', 'created_datetime',\n       'last_modified_t', 'last_modified_datetime', 'product_name',\n       'generic_name', 'quantity',\n       ...\n       'fruits-vegetables-nuts_100g', 'fruits-vegetables-nuts-estimate_100g',\n       'collagen-meat-protein-ratio_100g', 'cocoa_100g', 'chlorophyl_100g',\n       'carbon-footprint_100g', 'nutrition-score-fr_100g',\n       'nutrition-score-uk_100g', 'glycemic-index_100g',\n       'water-hardness_100g'],\n      dtype='object', length=163)\n\n\n\n\nStep 8. What is the name of 105th column?\n\nfood.columns[104]\n\n'-glucose_100g'\n\n\n\n\nStep 9. What is the type of the observations of the 105th column?\n\nfood.dtypes['-glucose_100g']\n\ndtype('float64')\n\n\n\n\nStep 10. How is the dataset indexed?\n\nfood.index\n\nRangeIndex(start=0, stop=356027, step=1)\n\n\n\n\nStep 11. What is the product name of the 19th observation?\n\nfood.values[18][7]\n\n'Lotus Organic Brown Jasmine Rice'"
  },
  {
    "objectID": "labs/Labexercises/02Euro12-Exercises-with-solutions.html",
    "href": "labs/Labexercises/02Euro12-Exercises-with-solutions.html",
    "title": "Ex2 - Filtering and Sorting Data",
    "section": "",
    "text": "This time we are going to pull data directly from the internet.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\n\n\n\nStep 2. Import the dataset from this address.\n\n\nStep 3. Assign it to a variable called euro12.\n\neuro12 = pd.read_csv('https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/02_Filtering_%26_Sorting/Euro12/Euro_2012_stats_TEAM.csv', sep=',')\neuro12\n\n\n\n\n\n\n\n\nTeam\nGoals\nShots on target\nShots off target\nShooting Accuracy\n% Goals-to-shots\nTotal shots (inc. Blocked)\nHit Woodwork\nPenalty goals\nPenalties not scored\n...\nSaves made\nSaves-to-shots ratio\nFouls Won\nFouls Conceded\nOffsides\nYellow Cards\nRed Cards\nSubs on\nSubs off\nPlayers Used\n\n\n\n\n0\nCroatia\n4\n13\n12\n51.9%\n16.0%\n32\n0\n0\n0\n...\n13\n81.3%\n41\n62\n2\n9\n0\n9\n9\n16\n\n\n1\nCzech Republic\n4\n13\n18\n41.9%\n12.9%\n39\n0\n0\n0\n...\n9\n60.1%\n53\n73\n8\n7\n0\n11\n11\n19\n\n\n2\nDenmark\n4\n10\n10\n50.0%\n20.0%\n27\n1\n0\n0\n...\n10\n66.7%\n25\n38\n8\n4\n0\n7\n7\n15\n\n\n3\nEngland\n5\n11\n18\n50.0%\n17.2%\n40\n0\n0\n0\n...\n22\n88.1%\n43\n45\n6\n5\n0\n11\n11\n16\n\n\n4\nFrance\n3\n22\n24\n37.9%\n6.5%\n65\n1\n0\n0\n...\n6\n54.6%\n36\n51\n5\n6\n0\n11\n11\n19\n\n\n5\nGermany\n10\n32\n32\n47.8%\n15.6%\n80\n2\n1\n0\n...\n10\n62.6%\n63\n49\n12\n4\n0\n15\n15\n17\n\n\n6\nGreece\n5\n8\n18\n30.7%\n19.2%\n32\n1\n1\n1\n...\n13\n65.1%\n67\n48\n12\n9\n1\n12\n12\n20\n\n\n7\nItaly\n6\n34\n45\n43.0%\n7.5%\n110\n2\n0\n0\n...\n20\n74.1%\n101\n89\n16\n16\n0\n18\n18\n19\n\n\n8\nNetherlands\n2\n12\n36\n25.0%\n4.1%\n60\n2\n0\n0\n...\n12\n70.6%\n35\n30\n3\n5\n0\n7\n7\n15\n\n\n9\nPoland\n2\n15\n23\n39.4%\n5.2%\n48\n0\n0\n0\n...\n6\n66.7%\n48\n56\n3\n7\n1\n7\n7\n17\n\n\n10\nPortugal\n6\n22\n42\n34.3%\n9.3%\n82\n6\n0\n0\n...\n10\n71.5%\n73\n90\n10\n12\n0\n14\n14\n16\n\n\n11\nRepublic of Ireland\n1\n7\n12\n36.8%\n5.2%\n28\n0\n0\n0\n...\n17\n65.4%\n43\n51\n11\n6\n1\n10\n10\n17\n\n\n12\nRussia\n5\n9\n31\n22.5%\n12.5%\n59\n2\n0\n0\n...\n10\n77.0%\n34\n43\n4\n6\n0\n7\n7\n16\n\n\n13\nSpain\n12\n42\n33\n55.9%\n16.0%\n100\n0\n1\n0\n...\n15\n93.8%\n102\n83\n19\n11\n0\n17\n17\n18\n\n\n14\nSweden\n5\n17\n19\n47.2%\n13.8%\n39\n3\n0\n0\n...\n8\n61.6%\n35\n51\n7\n7\n0\n9\n9\n18\n\n\n15\nUkraine\n2\n7\n26\n21.2%\n6.0%\n38\n0\n0\n0\n...\n13\n76.5%\n48\n31\n4\n5\n0\n9\n9\n18\n\n\n\n\n16 rows × 35 columns\n\n\n\n\n\nStep 4. Select only the Goal column.\n\neuro12.Goals\n\n0      4\n1      4\n2      4\n3      5\n4      3\n5     10\n6      5\n7      6\n8      2\n9      2\n10     6\n11     1\n12     5\n13    12\n14     5\n15     2\nName: Goals, dtype: int64\n\n\n\n\nStep 5. How many team participated in the Euro2012?\n\neuro12.shape[0]\n\n16\n\n\n\n\nStep 6. What is the number of columns in the dataset?\n\neuro12.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 16 entries, 0 to 15\nData columns (total 35 columns):\n #   Column                      Non-Null Count  Dtype  \n---  ------                      --------------  -----  \n 0   Team                        16 non-null     object \n 1   Goals                       16 non-null     int64  \n 2   Shots on target             16 non-null     int64  \n 3   Shots off target            16 non-null     int64  \n 4   Shooting Accuracy           16 non-null     object \n 5   % Goals-to-shots            16 non-null     object \n 6   Total shots (inc. Blocked)  16 non-null     int64  \n 7   Hit Woodwork                16 non-null     int64  \n 8   Penalty goals               16 non-null     int64  \n 9   Penalties not scored        16 non-null     int64  \n 10  Headed goals                16 non-null     int64  \n 11  Passes                      16 non-null     int64  \n 12  Passes completed            16 non-null     int64  \n 13  Passing Accuracy            16 non-null     object \n 14  Touches                     16 non-null     int64  \n 15  Crosses                     16 non-null     int64  \n 16  Dribbles                    16 non-null     int64  \n 17  Corners Taken               16 non-null     int64  \n 18  Tackles                     16 non-null     int64  \n 19  Clearances                  16 non-null     int64  \n 20  Interceptions               16 non-null     int64  \n 21  Clearances off line         15 non-null     float64\n 22  Clean Sheets                16 non-null     int64  \n 23  Blocks                      16 non-null     int64  \n 24  Goals conceded              16 non-null     int64  \n 25  Saves made                  16 non-null     int64  \n 26  Saves-to-shots ratio        16 non-null     object \n 27  Fouls Won                   16 non-null     int64  \n 28  Fouls Conceded              16 non-null     int64  \n 29  Offsides                    16 non-null     int64  \n 30  Yellow Cards                16 non-null     int64  \n 31  Red Cards                   16 non-null     int64  \n 32  Subs on                     16 non-null     int64  \n 33  Subs off                    16 non-null     int64  \n 34  Players Used                16 non-null     int64  \ndtypes: float64(1), int64(29), object(5)\nmemory usage: 4.5+ KB\n\n\n\n\nStep 7. View only the columns Team, Yellow Cards and Red Cards and assign them to a dataframe called discipline\n\ndiscipline = euro12[['Team', 'Yellow Cards', 'Red Cards']]\ndiscipline\n\n\n\n\n\n\n\n\nTeam\nYellow Cards\nRed Cards\n\n\n\n\n0\nCroatia\n9\n0\n\n\n1\nCzech Republic\n7\n0\n\n\n2\nDenmark\n4\n0\n\n\n3\nEngland\n5\n0\n\n\n4\nFrance\n6\n0\n\n\n5\nGermany\n4\n0\n\n\n6\nGreece\n9\n1\n\n\n7\nItaly\n16\n0\n\n\n8\nNetherlands\n5\n0\n\n\n9\nPoland\n7\n1\n\n\n10\nPortugal\n12\n0\n\n\n11\nRepublic of Ireland\n6\n1\n\n\n12\nRussia\n6\n0\n\n\n13\nSpain\n11\n0\n\n\n14\nSweden\n7\n0\n\n\n15\nUkraine\n5\n0\n\n\n\n\n\n\n\n\n\nStep 8. Sort the teams by Red Cards, then to Yellow Cards\n\ndiscipline.sort_values(['Red Cards', 'Yellow Cards'], ascending = False)\n\n\n\n\n\n\n\n\nTeam\nYellow Cards\nRed Cards\n\n\n\n\n6\nGreece\n9\n1\n\n\n9\nPoland\n7\n1\n\n\n11\nRepublic of Ireland\n6\n1\n\n\n7\nItaly\n16\n0\n\n\n10\nPortugal\n12\n0\n\n\n13\nSpain\n11\n0\n\n\n0\nCroatia\n9\n0\n\n\n1\nCzech Republic\n7\n0\n\n\n14\nSweden\n7\n0\n\n\n4\nFrance\n6\n0\n\n\n12\nRussia\n6\n0\n\n\n3\nEngland\n5\n0\n\n\n8\nNetherlands\n5\n0\n\n\n15\nUkraine\n5\n0\n\n\n2\nDenmark\n4\n0\n\n\n5\nGermany\n4\n0\n\n\n\n\n\n\n\n\n\nStep 9. Calculate the mean Yellow Cards given per Team\n\n\nround(discipline['Yellow Cards'].mean())\n\n7\n\n\n\n\nStep 10. Filter teams that scored more than 6 goals\n\neuro12[euro12.Goals &gt; 6]\n\n\n\n\n\n\n\n\nTeam\nGoals\nShots on target\nShots off target\nShooting Accuracy\n% Goals-to-shots\nTotal shots (inc. Blocked)\nHit Woodwork\nPenalty goals\nPenalties not scored\n...\nSaves made\nSaves-to-shots ratio\nFouls Won\nFouls Conceded\nOffsides\nYellow Cards\nRed Cards\nSubs on\nSubs off\nPlayers Used\n\n\n\n\n5\nGermany\n10\n32\n32\n47.8%\n15.6%\n80\n2\n1\n0\n...\n10\n62.6%\n63\n49\n12\n4\n0\n15\n15\n17\n\n\n13\nSpain\n12\n42\n33\n55.9%\n16.0%\n100\n0\n1\n0\n...\n15\n93.8%\n102\n83\n19\n11\n0\n17\n17\n18\n\n\n\n\n2 rows × 35 columns\n\n\n\n\n\nStep 11. Select the teams that start with G\n\neuro12[euro12.Team.str.startswith('G')]\n\n\n\n\n\n\n\n\nTeam\nGoals\nShots on target\nShots off target\nShooting Accuracy\n% Goals-to-shots\nTotal shots (inc. Blocked)\nHit Woodwork\nPenalty goals\nPenalties not scored\n...\nSaves made\nSaves-to-shots ratio\nFouls Won\nFouls Conceded\nOffsides\nYellow Cards\nRed Cards\nSubs on\nSubs off\nPlayers Used\n\n\n\n\n5\nGermany\n10\n32\n32\n47.8%\n15.6%\n80\n2\n1\n0\n...\n10\n62.6%\n63\n49\n12\n4\n0\n15\n15\n17\n\n\n6\nGreece\n5\n8\n18\n30.7%\n19.2%\n32\n1\n1\n1\n...\n13\n65.1%\n67\n48\n12\n9\n1\n12\n12\n20\n\n\n\n\n2 rows × 35 columns\n\n\n\n\n\nStep 12. Select the first 7 columns\n\neuro12.iloc[: , 0:7]\n\n\n\n\n\n\n\n\nTeam\nGoals\nShots on target\nShots off target\nShooting Accuracy\n% Goals-to-shots\nTotal shots (inc. Blocked)\n\n\n\n\n0\nCroatia\n4\n13\n12\n51.9%\n16.0%\n32\n\n\n1\nCzech Republic\n4\n13\n18\n41.9%\n12.9%\n39\n\n\n2\nDenmark\n4\n10\n10\n50.0%\n20.0%\n27\n\n\n3\nEngland\n5\n11\n18\n50.0%\n17.2%\n40\n\n\n4\nFrance\n3\n22\n24\n37.9%\n6.5%\n65\n\n\n5\nGermany\n10\n32\n32\n47.8%\n15.6%\n80\n\n\n6\nGreece\n5\n8\n18\n30.7%\n19.2%\n32\n\n\n7\nItaly\n6\n34\n45\n43.0%\n7.5%\n110\n\n\n8\nNetherlands\n2\n12\n36\n25.0%\n4.1%\n60\n\n\n9\nPoland\n2\n15\n23\n39.4%\n5.2%\n48\n\n\n10\nPortugal\n6\n22\n42\n34.3%\n9.3%\n82\n\n\n11\nRepublic of Ireland\n1\n7\n12\n36.8%\n5.2%\n28\n\n\n12\nRussia\n5\n9\n31\n22.5%\n12.5%\n59\n\n\n13\nSpain\n12\n42\n33\n55.9%\n16.0%\n100\n\n\n14\nSweden\n5\n17\n19\n47.2%\n13.8%\n39\n\n\n15\nUkraine\n2\n7\n26\n21.2%\n6.0%\n38\n\n\n\n\n\n\n\n\n\nStep 13. Select all columns except the last 3.\n\neuro12.iloc[: , :-3]\n\n\n\n\n\n\n\n\nTeam\nGoals\nShots on target\nShots off target\nShooting Accuracy\n% Goals-to-shots\nTotal shots (inc. Blocked)\nHit Woodwork\nPenalty goals\nPenalties not scored\n...\nClean Sheets\nBlocks\nGoals conceded\nSaves made\nSaves-to-shots ratio\nFouls Won\nFouls Conceded\nOffsides\nYellow Cards\nRed Cards\n\n\n\n\n0\nCroatia\n4\n13\n12\n51.9%\n16.0%\n32\n0\n0\n0\n...\n0\n10\n3\n13\n81.3%\n41\n62\n2\n9\n0\n\n\n1\nCzech Republic\n4\n13\n18\n41.9%\n12.9%\n39\n0\n0\n0\n...\n1\n10\n6\n9\n60.1%\n53\n73\n8\n7\n0\n\n\n2\nDenmark\n4\n10\n10\n50.0%\n20.0%\n27\n1\n0\n0\n...\n1\n10\n5\n10\n66.7%\n25\n38\n8\n4\n0\n\n\n3\nEngland\n5\n11\n18\n50.0%\n17.2%\n40\n0\n0\n0\n...\n2\n29\n3\n22\n88.1%\n43\n45\n6\n5\n0\n\n\n4\nFrance\n3\n22\n24\n37.9%\n6.5%\n65\n1\n0\n0\n...\n1\n7\n5\n6\n54.6%\n36\n51\n5\n6\n0\n\n\n5\nGermany\n10\n32\n32\n47.8%\n15.6%\n80\n2\n1\n0\n...\n1\n11\n6\n10\n62.6%\n63\n49\n12\n4\n0\n\n\n6\nGreece\n5\n8\n18\n30.7%\n19.2%\n32\n1\n1\n1\n...\n1\n23\n7\n13\n65.1%\n67\n48\n12\n9\n1\n\n\n7\nItaly\n6\n34\n45\n43.0%\n7.5%\n110\n2\n0\n0\n...\n2\n18\n7\n20\n74.1%\n101\n89\n16\n16\n0\n\n\n8\nNetherlands\n2\n12\n36\n25.0%\n4.1%\n60\n2\n0\n0\n...\n0\n9\n5\n12\n70.6%\n35\n30\n3\n5\n0\n\n\n9\nPoland\n2\n15\n23\n39.4%\n5.2%\n48\n0\n0\n0\n...\n0\n8\n3\n6\n66.7%\n48\n56\n3\n7\n1\n\n\n10\nPortugal\n6\n22\n42\n34.3%\n9.3%\n82\n6\n0\n0\n...\n2\n11\n4\n10\n71.5%\n73\n90\n10\n12\n0\n\n\n11\nRepublic of Ireland\n1\n7\n12\n36.8%\n5.2%\n28\n0\n0\n0\n...\n0\n23\n9\n17\n65.4%\n43\n51\n11\n6\n1\n\n\n12\nRussia\n5\n9\n31\n22.5%\n12.5%\n59\n2\n0\n0\n...\n0\n8\n3\n10\n77.0%\n34\n43\n4\n6\n0\n\n\n13\nSpain\n12\n42\n33\n55.9%\n16.0%\n100\n0\n1\n0\n...\n5\n8\n1\n15\n93.8%\n102\n83\n19\n11\n0\n\n\n14\nSweden\n5\n17\n19\n47.2%\n13.8%\n39\n3\n0\n0\n...\n1\n12\n5\n8\n61.6%\n35\n51\n7\n7\n0\n\n\n15\nUkraine\n2\n7\n26\n21.2%\n6.0%\n38\n0\n0\n0\n...\n0\n4\n4\n13\n76.5%\n48\n31\n4\n5\n0\n\n\n\n\n16 rows × 32 columns\n\n\n\n\n\nStep 14. Present only the Shooting Accuracy from England, Italy and Russia\n\neuro12.loc[euro12.Team.isin(['England', 'Italy', 'Russia']), ['Team','Shooting Accuracy']]\n\n\n\n\n\n\n\n\nTeam\nShooting Accuracy\n\n\n\n\n3\nEngland\n50.0%\n\n\n7\nItaly\n43.0%\n\n\n12\nRussia\n22.5%"
  },
  {
    "objectID": "labs/Labexercises/03Scores-Exercises-with-solutions.html",
    "href": "labs/Labexercises/03Scores-Exercises-with-solutions.html",
    "title": "Scores",
    "section": "",
    "text": "Introduction:\nThis time you will create the data.\nExercise based on Chris Albon work, the credits belong to him.\n\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n%matplotlib inline\n\n\n\nStep 2. Create the DataFrame that should look like the one below.\n\nraw_data = {'first_name': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'], \n            'last_name': ['Miller', 'Jacobson', 'Ali', 'Milner', 'Cooze'], \n            'female': [0, 1, 1, 0, 1],\n            'age': [42, 52, 36, 24, 73], \n            'preTestScore': [4, 24, 31, 2, 3],\n            'postTestScore': [25, 94, 57, 62, 70]}\n\ndf = pd.DataFrame(raw_data, columns = ['first_name', 'last_name', 'age', 'female', 'preTestScore', 'postTestScore'])\n\ndf\n\n\n\n\n\n\n\n\nfirst_name\nlast_name\nage\nfemale\npreTestScore\npostTestScore\n\n\n\n\n0\nJason\nMiller\n42\n0\n4\n25\n\n\n1\nMolly\nJacobson\n52\n1\n24\n94\n\n\n2\nTina\nAli\n36\n1\n31\n57\n\n\n3\nJake\nMilner\n24\n0\n2\n62\n\n\n4\nAmy\nCooze\n73\n1\n3\n70\n\n\n\n\n\n\n\n\n\nStep 3. Create a Scatterplot of preTestScore and postTestScore, with the size of each point determined by age\n\nHint: Don’t forget to place the labels\n\nplt.scatter(df.preTestScore, df.postTestScore, s=df.age)\n\n#set labels and titles\nplt.title(\"preTestScore x postTestScore\")\nplt.xlabel('preTestScore')\nplt.ylabel('preTestScore')\n\nText(0, 0.5, 'preTestScore')\n\n\n\n\n\nStep 4. Create a Scatterplot of preTestScore and postTestScore.\n\n\nThis time the size should be 4.5 times the postTestScore and the color determined by sex\n\nplt.scatter(df.preTestScore, df.postTestScore, s= df.postTestScore * 4.5, c = df.female)\n\n#set labels and titles\nplt.title(\"preTestScore x postTestScore\")\nplt.xlabel('preTestScore')\nplt.ylabel('preTestScore')\n\nText(46.972222222222214, 0.5, 'preTestScore')\n\n\n\n\nBONUS: Create your own question and answer it."
  },
  {
    "objectID": "labs/Practice/Practice3.1.html",
    "href": "labs/Practice/Practice3.1.html",
    "title": "",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nimport textwrap\n\n\npd.read_csv(\n    \"https://vincentarelbundock.github.io/Rdatasets/csv/dplyr/storms.csv\", nrows=10\n)\n\n\n\n\n\n\n\n\nrownames\nname\nyear\nmonth\nday\nhour\nlat\nlong\nstatus\ncategory\nwind\npressure\ntropicalstorm_force_diameter\nhurricane_force_diameter\n\n\n\n\n0\n1\nAmy\n1975\n6\n27\n0\n27.5\n-79.0\ntropical depression\nNaN\n25\n1013\nNaN\nNaN\n\n\n1\n2\nAmy\n1975\n6\n27\n6\n28.5\n-79.0\ntropical depression\nNaN\n25\n1013\nNaN\nNaN\n\n\n2\n3\nAmy\n1975\n6\n27\n12\n29.5\n-79.0\ntropical depression\nNaN\n25\n1013\nNaN\nNaN\n\n\n3\n4\nAmy\n1975\n6\n27\n18\n30.5\n-79.0\ntropical depression\nNaN\n25\n1013\nNaN\nNaN\n\n\n4\n5\nAmy\n1975\n6\n28\n0\n31.5\n-78.8\ntropical depression\nNaN\n25\n1012\nNaN\nNaN\n\n\n5\n6\nAmy\n1975\n6\n28\n6\n32.4\n-78.7\ntropical depression\nNaN\n25\n1012\nNaN\nNaN\n\n\n6\n7\nAmy\n1975\n6\n28\n12\n33.3\n-78.0\ntropical depression\nNaN\n25\n1011\nNaN\nNaN\n\n\n7\n8\nAmy\n1975\n6\n28\n18\n34.0\n-77.0\ntropical depression\nNaN\n30\n1006\nNaN\nNaN\n\n\n8\n9\nAmy\n1975\n6\n29\n0\n34.4\n-75.8\ntropical storm\nNaN\n35\n1004\nNaN\nNaN\n\n\n9\n10\nAmy\n1975\n6\n29\n6\n34.0\n-74.8\ntropical storm\nNaN\n40\n1002\nNaN\nNaN\n\n\n\n\n\n\n\n\n#\nimport requests\n\nurl = \"https://api.ons.gov.uk/timeseries/JP9Z/dataset/UNEM/data\"\n\nresponse = requests.get(url)\n\nprint(f\"请求状态码: {response.status_code}\")\nprint(f\"返回的原始内容: {response.text}\")\n#The interface API is no longer available, so I can't get the data at all!\n\n请求状态码: 404\n返回的原始内容: This API is being decommissioned as part of a suite of work to improve the digital products and services we offer. It is planned to be fully retired on 25/11/2024. If you have any queries please contact us at apiservice@ons.gov.uk.\n\n\n\nFor the above, I made changes to the code. Using the FRED API key, query series_id=“LMJVTTUVGBM647S”, which is the series number of the UK job vacancies (in thousands).167ccd5ae95c579ae64d6b1295c563feThis is Jie Xu’s FRED API KEY\n\nimport requests\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# 替换为你的 FRED API key\napi_key = \"167ccd5ae95c579ae64d6b1295c563fe\"\nurl = f\"https://api.stlouisfed.org/fred/series/observations?series_id=LMJVTTUVGBM647S&api_key={api_key}&file_type=json\"\n\nresponse = requests.get(url)\ndata = response.json()['observations']\n\n# 提取日期和职位空缺数量，过滤掉非数字数据\ndates = [item['date'] for item in data]\nvalues = []\nfor item in data:\n    try:\n        values.append(float(item['value']))\n    except ValueError:\n        values.append(None)  # 使用 None 表示无效数据\n\n# 创建 DataFrame 并去除无效数据\ndf = pd.DataFrame({'Date': dates, 'Vacancies': values})\ndf.dropna(inplace=True)\n\n# 绘制折线图\nplt.figure(figsize=(10, 5))\nplt.plot(df['Date'], df['Vacancies'], label='UK Job Vacancies (Service Industries)', color='blue')\nplt.xlabel('Date')\nplt.ylabel('Vacancies (Thousands)')\nplt.title('UK Job Vacancies (Service Industries)')\nplt.xticks(rotation=45)\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport pandas_datareader.data as web\n\ndf_u = web.DataReader(\"LRHUTTTTGBM156S\", \"fred\")\n\ndf_u.plot(title=\"UK unemployment (percent)\", legend=False, ylim=(2, 6), lw=3.0)\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport requests\n\nurl = \"https://databank.worldbank.org/source/millennium-development-goals/Series/EN.ATM.CO2E.KT\"\nhtml = requests.get(url).content\ndf = pd.read_html(html)[0]\nprint(df.columns)\n\nIndex(['Indicator', 'Rule', 'Weighted Indicator', '%'], dtype='object')\n\n\nFirst, I went to Getting Data-Coding for Economists based on the code, but could not get the data. Then, I went to https://datacatalog.worldbank.org/indicator/b66c366b-bdce-eb11-bacc-000d3a596ff0/CO2-emissions–metric-tons-per-capita. this site to get the Data and the page keeps reporting an error: An error occurs. This could be due to\nOur network is unusually busy The server is currently unavailable Please try again later or contact data@worldbank.org\nError ID: 9bcb1bcd7bd91946aaddafba37719f72. So I can only use excel sheet.\n\nimport pandas as pd\n\n# 读取Excel文件，假设数据在第一个工作表\ndf = pd.read_excel('data\\EN.ATM.CO2E.PC.xlsx')\n\n# 重命名列（如果需要），确保列名与你的要求一致，这里假设原始列名分别为'Country Name'、'Year'、'EN.ATM.CO2E.KT'\ndf.rename(columns={'Country Name': 'country', 'Year': 'year', 'EN.ATM.CO2E.KT': 'EN.ATM.CO2E.PC'}, inplace=True)\n\n# 选择需要的列并按照人均二氧化碳排放量排序\nselected_data = df[['country', 'year', 'EN.ATM.CO2E.PC']].sort_values('EN.ATM.CO2E.PC')\n\n# 显示处理后的数据表格\nprint(selected_data)\n\n                   country  year  EN.ATM.CO2E.PC\n0                    India  2017        1.704927\n1     East Asia\\n& Pacific  2017        5.960076\n2  Europe &\\nCentral\\nAsia  2017        6.746232\n3                    China  2017        7.226160\n4           United\\nStates  2017       14.823245\n\n\n\nimport seaborn as sns\n\nfig, ax = plt.subplots()\nsns.barplot(x=\"country\", y=\"EN.ATM.CO2E.PC\", data=df.reset_index(), ax=ax)\nax.set_title(r\"CO$_2$ (metric tons per capita)\", loc=\"right\")\nplt.suptitle(\"The USA leads the world on per-capita emissions\", y=1.01)\nfor key, spine in ax.spines.items():\n    spine.set_visible(False)\nax.set_ylabel(\"\")\nax.set_xlabel(\"\")\nax.yaxis.tick_right()\nplt.show()\n\n\n\n\n\n\n\n\nFor this link: https://stats.oecd.org/SDMX-JSON/data/PDB_LV/GBR+FRA+CAN+ITA+DEU+JPN+USA.T_GDPEMP.CPC/all?startTime=2010 can no longer be found.404 - File or directory not found. The resource you are looking for may have been deleted, renamed, or is temporarily unavailable.\n\nurl = \"http://aeturrell.com/research\"\npage = requests.get(url)\npage.text[:300]\n\n'&lt;!DOCTYPE html&gt;\\n&lt;html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\" xml:lang=\"en\"&gt;&lt;head&gt;\\n\\n&lt;meta charset=\"utf-8\"&gt;\\n&lt;meta name=\"generator\" content=\"quarto-1.5.56\"&gt;\\n\\n&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, user-scalable=yes\"&gt;\\n\\n&lt;meta name=\"author\" content=\"Arthur Turrell\"&gt;\\n'\n\n\n\nsoup = BeautifulSoup(page.text, \"html.parser\")\nprint(soup.prettify()[60000:60500])\n\n       &lt;/div&gt;\n          &lt;div class=\"project-category\"&gt;\n           &lt;a href=\"#category=gender pay gap\"&gt;\n            gender pay gap\n           &lt;/a&gt;\n          &lt;/div&gt;\n          &lt;div class=\"project-category\"&gt;\n           &lt;a href=\"#category=labour\"&gt;\n            labour\n           &lt;/a&gt;\n          &lt;/div&gt;\n          &lt;div class=\"project-category\"&gt;\n           &lt;a href=\"#category=text analysis\"&gt;\n            text analysis\n           &lt;/a&gt;\n          &lt;/div&gt;\n         &lt;/div&gt;\n         &lt;div class=\"project-details-listing\n\n\n\n# Get all paragraphs\nall_paras = soup.find_all(\"p\")\n# Just show one of the paras\nall_paras[1]\n\n&lt;p&gt;Botta, Federico, Robin Lovelace, Laura Gilbert, and Arthur Turrell. \"Packaging code and data for reproducible research: A case study of journey time statistics.\" &lt;i&gt;Environment and Planning B: Urban Analytics and City Science&lt;/i&gt; (2024): 23998083241267331. doi: &lt;a href=\"https://doi.org/10.1177/23998083241267331\"&gt;&lt;code&gt;10.1177/23998083241267331&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;\n\n\n\nall_paras[1].text\n\n'Botta, Federico, Robin Lovelace, Laura Gilbert, and Arthur Turrell. \"Packaging code and data for reproducible research: A case study of journey time statistics.\" Environment and Planning B: Urban Analytics and City Science (2024): 23998083241267331. doi: 10.1177/23998083241267331'\n\n\n\nprojects = soup.find_all(\"div\", class_=\"project-content listing-pub-info\")\nprojects = [x.text.strip() for x in projects]\nprojects[:4]\n\n['Botta, Federico, Robin Lovelace, Laura Gilbert, and Arthur Turrell. \"Packaging code and data for reproducible research: A case study of journey time statistics.\" Environment and Planning B: Urban Analytics and City Science (2024): 23998083241267331. doi: 10.1177/23998083241267331',\n 'Kalamara, Eleni, Arthur Turrell, Chris Redl, George Kapetanios, and Sujit Kapadia. \"Making text count: economic forecasting using newspaper text.\" Journal of Applied Econometrics 37, no. 5 (2022): 896-919. doi: 10.1002/jae.2907',\n 'Turrell, A., Speigner, B., Copple, D., Djumalieva, J. and Thurgood, J., 2021. Is the UK’s productivity puzzle mostly driven by occupational mismatch? An analysis using big data on job vacancies. Labour Economics, 71, p.102013. doi: 10.1016/j.labeco.2021.102013',\n 'Haldane, Andrew G., and Arthur E. Turrell. \"Drawing on different disciplines: macroeconomic agent-based models.\" Journal of Evolutionary Economics 29 (2019): 39-66. doi: 10.1007/s00191-018-0557-5']\n\n\n\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef scraper(url):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # 检查请求是否成功，若不成功则抛出异常\n        soup = BeautifulSoup(response.text, 'html.parser')\n        return soup.title.string if soup.title else \"No title found\"\n    except requests.RequestException as e:\n        print(f\"Error fetching page {url}: {e}\")\n        return None\n\nstart, stop = 0, 50\nroot_url = \"www.codingforeconomists.com/page=\"\ninfo_on_pages = []\nfor i in range(start, stop):\n    url = root_url + str(i)\n    info = scraper(url)\n    info_on_pages.append(info)\n\nprint(info_on_pages)\n\nError fetching page www.codingforeconomists.com/page=0: Invalid URL 'www.codingforeconomists.com/page=0': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=0?\nError fetching page www.codingforeconomists.com/page=1: Invalid URL 'www.codingforeconomists.com/page=1': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=1?\nError fetching page www.codingforeconomists.com/page=2: Invalid URL 'www.codingforeconomists.com/page=2': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=2?\nError fetching page www.codingforeconomists.com/page=3: Invalid URL 'www.codingforeconomists.com/page=3': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=3?\nError fetching page www.codingforeconomists.com/page=4: Invalid URL 'www.codingforeconomists.com/page=4': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=4?\nError fetching page www.codingforeconomists.com/page=5: Invalid URL 'www.codingforeconomists.com/page=5': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=5?\nError fetching page www.codingforeconomists.com/page=6: Invalid URL 'www.codingforeconomists.com/page=6': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=6?\nError fetching page www.codingforeconomists.com/page=7: Invalid URL 'www.codingforeconomists.com/page=7': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=7?\nError fetching page www.codingforeconomists.com/page=8: Invalid URL 'www.codingforeconomists.com/page=8': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=8?\nError fetching page www.codingforeconomists.com/page=9: Invalid URL 'www.codingforeconomists.com/page=9': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=9?\nError fetching page www.codingforeconomists.com/page=10: Invalid URL 'www.codingforeconomists.com/page=10': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=10?\nError fetching page www.codingforeconomists.com/page=11: Invalid URL 'www.codingforeconomists.com/page=11': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=11?\nError fetching page www.codingforeconomists.com/page=12: Invalid URL 'www.codingforeconomists.com/page=12': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=12?\nError fetching page www.codingforeconomists.com/page=13: Invalid URL 'www.codingforeconomists.com/page=13': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=13?\nError fetching page www.codingforeconomists.com/page=14: Invalid URL 'www.codingforeconomists.com/page=14': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=14?\nError fetching page www.codingforeconomists.com/page=15: Invalid URL 'www.codingforeconomists.com/page=15': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=15?\nError fetching page www.codingforeconomists.com/page=16: Invalid URL 'www.codingforeconomists.com/page=16': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=16?\nError fetching page www.codingforeconomists.com/page=17: Invalid URL 'www.codingforeconomists.com/page=17': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=17?\nError fetching page www.codingforeconomists.com/page=18: Invalid URL 'www.codingforeconomists.com/page=18': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=18?\nError fetching page www.codingforeconomists.com/page=19: Invalid URL 'www.codingforeconomists.com/page=19': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=19?\nError fetching page www.codingforeconomists.com/page=20: Invalid URL 'www.codingforeconomists.com/page=20': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=20?\nError fetching page www.codingforeconomists.com/page=21: Invalid URL 'www.codingforeconomists.com/page=21': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=21?\nError fetching page www.codingforeconomists.com/page=22: Invalid URL 'www.codingforeconomists.com/page=22': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=22?\nError fetching page www.codingforeconomists.com/page=23: Invalid URL 'www.codingforeconomists.com/page=23': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=23?\nError fetching page www.codingforeconomists.com/page=24: Invalid URL 'www.codingforeconomists.com/page=24': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=24?\nError fetching page www.codingforeconomists.com/page=25: Invalid URL 'www.codingforeconomists.com/page=25': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=25?\nError fetching page www.codingforeconomists.com/page=26: Invalid URL 'www.codingforeconomists.com/page=26': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=26?\nError fetching page www.codingforeconomists.com/page=27: Invalid URL 'www.codingforeconomists.com/page=27': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=27?\nError fetching page www.codingforeconomists.com/page=28: Invalid URL 'www.codingforeconomists.com/page=28': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=28?\nError fetching page www.codingforeconomists.com/page=29: Invalid URL 'www.codingforeconomists.com/page=29': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=29?\nError fetching page www.codingforeconomists.com/page=30: Invalid URL 'www.codingforeconomists.com/page=30': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=30?\nError fetching page www.codingforeconomists.com/page=31: Invalid URL 'www.codingforeconomists.com/page=31': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=31?\nError fetching page www.codingforeconomists.com/page=32: Invalid URL 'www.codingforeconomists.com/page=32': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=32?\nError fetching page www.codingforeconomists.com/page=33: Invalid URL 'www.codingforeconomists.com/page=33': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=33?\nError fetching page www.codingforeconomists.com/page=34: Invalid URL 'www.codingforeconomists.com/page=34': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=34?\nError fetching page www.codingforeconomists.com/page=35: Invalid URL 'www.codingforeconomists.com/page=35': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=35?\nError fetching page www.codingforeconomists.com/page=36: Invalid URL 'www.codingforeconomists.com/page=36': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=36?\nError fetching page www.codingforeconomists.com/page=37: Invalid URL 'www.codingforeconomists.com/page=37': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=37?\nError fetching page www.codingforeconomists.com/page=38: Invalid URL 'www.codingforeconomists.com/page=38': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=38?\nError fetching page www.codingforeconomists.com/page=39: Invalid URL 'www.codingforeconomists.com/page=39': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=39?\nError fetching page www.codingforeconomists.com/page=40: Invalid URL 'www.codingforeconomists.com/page=40': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=40?\nError fetching page www.codingforeconomists.com/page=41: Invalid URL 'www.codingforeconomists.com/page=41': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=41?\nError fetching page www.codingforeconomists.com/page=42: Invalid URL 'www.codingforeconomists.com/page=42': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=42?\nError fetching page www.codingforeconomists.com/page=43: Invalid URL 'www.codingforeconomists.com/page=43': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=43?\nError fetching page www.codingforeconomists.com/page=44: Invalid URL 'www.codingforeconomists.com/page=44': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=44?\nError fetching page www.codingforeconomists.com/page=45: Invalid URL 'www.codingforeconomists.com/page=45': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=45?\nError fetching page www.codingforeconomists.com/page=46: Invalid URL 'www.codingforeconomists.com/page=46': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=46?\nError fetching page www.codingforeconomists.com/page=47: Invalid URL 'www.codingforeconomists.com/page=47': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=47?\nError fetching page www.codingforeconomists.com/page=48: Invalid URL 'www.codingforeconomists.com/page=48': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=48?\nError fetching page www.codingforeconomists.com/page=49: Invalid URL 'www.codingforeconomists.com/page=49': No scheme supplied. Perhaps you meant https://www.codingforeconomists.com/page=49?\n[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n\n\n\ndf_list = pd.read_html(\n    \"https://simple.wikipedia.org/wiki/FIFA_World_Cup\", match=\"Sweden\"\n)\n# Retrieve first and only entry from list of dataframes\ndf = df_list[0]\ndf.head()\n\n\n\n\n\n\n\n\nYears\nHosts\nWinners\nScore\nRunner's-up\nThird place\nScore.1\nFourth place\n\n\n\n\n0\n1930 Details\nUruguay\nUruguay\n4 - 2\nArgentina\nUnited States\n[note 1]\nYugoslavia\n\n\n1\n1934 Details\nItaly\nItaly\n2 - 1\nCzechoslovakia\nGermany\n3 - 2\nAustria\n\n\n2\n1938 Details\nFrance\nItaly\n4 - 2\nHungary\nBrazil\n4 - 2\nSweden\n\n\n3\n1950 Details\nBrazil\nUruguay\n2 - 1\nBrazil\nSweden\n[note 2]\nSpain\n\n\n4\n1954 Details\nSwitzerland\nWest Germany\n3 - 2\nHungary\nAustria\n3 - 1\nUruguay\n\n\n\n\n\n\n\npdf part\n\nimport PyPDF2\nfrom pathlib import Path\n\ndef read_pdf_text(file_path):\n    text = \"\"\n    with open(file_path, 'rb') as file:\n        reader = PyPDF2.PdfReader(file)\n        for page in reader.pages:\n            text += page.extract_text()\n    return text\n\npdf_file_path = Path(\"D:\\360MoveData\\Users\\75903\\Desktop\\SuYibo-Homework\\data\\pdf_with_table.pdf\")\npdf_text = read_pdf_text(pdf_file_path)\nprint(pdf_text[:220])\n\n3 \n 2 Quantifying Fuel -Saving Opportunit ies from Specific Driving \nBehavior Changes  \n2.1 Savings from Improving Individual Driving  Profiles  \n2.1.1  Drive Profile Subsample from Real -World Travel Survey  \nThe interi\n\n\n\nimport tabula\nimport os\n\n# 设置 PDF 文件路径\npdf_path = os.path.join('data', 'pdf_with_table.pdf')\n\n# 使用 tabula 读取 PDF 中的表格数据\ntables = tabula.read_pdf(pdf_path, pages='all')\n\n# 打印第一个表格的数据（你可以根据实际需求修改处理方式）\nif len(tables) &gt; 0:\n    print(tables[0])\n\n  Unnamed: 0 Unnamed: 1 Unnamed: 2 Unnamed: 3 Percent Fuel Savings Unnamed: 4\n0      Cycle         KI   Distance        NaN                  NaN        NaN\n1       Name     (1/km)       (mi)   Improved  Decreased Eliminate  Decreased\n2        NaN        NaN        NaN      Speed          Accel Stops       Idle\n3     2012_2       3.30        1.3       5.9%           9.5% 29.2%      17.4%\n4     2145_1       0.68       11.2       2.4%            0.1% 9.5%       2.7%\n5     4234_1       0.59       58.7       8.5%            1.3% 8.5%       3.3%\n6     2032_2       0.17       57.8      21.7%            0.3% 2.7%       1.2%\n7     4171_1       0.07      173.9      58.1%            1.6% 2.1%       0.5%"
  },
  {
    "objectID": "labs/Practice/Practice3.3.html",
    "href": "labs/Practice/Practice3.3.html",
    "title": "",
    "section": "",
    "text": "import requests\n \n# 定义请求的 URL 和 headers\nurl = \"https://movie.douban.com/top250\"\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n}\n \n# 发送 GET 请求\nresponse = requests.get(url, headers=headers)\nresponse.encoding = 'utf-8'  # 设置编码方式\nhtml_content = response.text  # 获取网页的 HTML 内容\nprint(\"网页内容加载成功！\")\n\n网页内容加载成功！\n\n\n\nfrom bs4 import BeautifulSoup\n \n# 使用 Beautiful Soup 解析 HTML\nsoup = BeautifulSoup(html_content, 'html.parser')\n \n# 提取电影名称、描述、评分和评价人数\nmovies = []\nfor item in soup.find_all('div', class_='item'):\n    title = item.find('span', class_='title').get_text()  # 电影名称\n    description = item.find('span', class_='inq')  # 电影描述\n    rating = item.find('span', class_='rating_num').get_text()  # 评分\n    votes = item.find('div', class_='star').find_all('span')[3].get_text()  # 评价人数\n    \n    # 如果没有描述，将其置为空字符串\n    if description:\n        description = description.get_text()\n    else:\n        description = ''\n    \n    movie = {\n        \"title\": title,\n        \"description\": description,\n        \"rating\": rating,\n        \"votes\": votes.replace('人评价', '').strip()\n    }\n    movies.append(movie)\n \nprint(\"数据提取成功！\")\n\n数据提取成功！\n\n\n\nimport csv\n \n# 将数据保存到 CSV 文件\nwith open('D:\\360MoveData\\Users\\75903\\Desktop\\SuYibo-Homework\\data\\douban_top250.csv', 'w', newline='', encoding='utf-8') as csvfile:\n    fieldnames = ['title', 'description', 'rating', 'votes']\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n \n    writer.writeheader()  # 写入表头\n    for movie in movies:\n        writer.writerow(movie)  # 写入每一行数据\n \nprint(\"数据已成功保存到 D:\\360MoveData\\Users\\75903\\Desktop\\SuYibo-Homework\\data\\douban_top250.csv\")\n\n数据已成功保存到 data\\douban_top250.csv\n\n\n\nimport pandas as pd\n\ndf = pd.read_csv('D:\\360MoveData\\Users\\75903\\Desktop\\SuYibo-Homework\\data\\douban_top250.csv')\n# show\nprint(df)\n\n        title                      description  rating    votes\n0      肖申克的救赎                          希望让人自由。     9.7  3086743\n1        霸王别姬                            风华绝代。     9.6  2278421\n2        阿甘正传                        一部美国近现代史。     9.5  2297780\n3       泰坦尼克号                       失去的才是永恒的。      9.5  2338435\n4        千与千寻                  最好的宫崎骏，最好的久石让。      9.4  2386813\n5        美丽人生                           最美的谎言。     9.5  1404250\n6     这个杀手不太冷                  怪蜀黍和小萝莉不得不说的故事。     9.4  2434726\n7        星际穿越            爱是一种力量，让我们超越时空感知它的存在。     9.4  2010081\n8        盗梦空间                  诺兰给了我们一场无法盗取的梦。     9.4  2197475\n9       楚门的世界            如果再也不能见到你，祝你早安，午安，晚安。     9.4  1868113\n10     辛德勒的名单                  拯救一个人，就是拯救整个世界。     9.5  1187176\n11    忠犬八公的故事                    永远都不能忘记你所爱的人。     9.4  1469596\n12      海上钢琴师        每个人都要走一条自己坚定了的路，就算是粉身碎骨。      9.3  1780067\n13    三傻大闹宝莱坞                   英俊版憨豆，高情商版谢耳朵。     9.2  1967345\n14     放牛班的春天              天籁一般的童声，是最接近上帝的存在。      9.3  1395330\n15     机器人总动员                         小瓦力，大人生。     9.3  1400496\n16      疯狂动物城  迪士尼给我们营造的乌托邦就是这样，永远善良勇敢，永远出乎意料。     9.2  2096015\n17        无间道                   香港电影史上永不过时的杰作。     9.3  1467474\n18       控方证人                       比利·怀德满分作品。     9.6   640164\n19  大话西游之大圣娶亲                            一生所爱。     9.2  1621517\n20         熔炉     我们一路奋战不是为了改变世界，而是为了不让世界改变我们。     9.3   986933\n21         教父            千万不要记恨你的对手，这样会让你失去理智。     9.3  1037883\n22       触不可及                       满满温情的高雅喜剧。     9.3  1208935\n23      寻梦环游记              死亡不是真的逝去，遗忘才是永恒的消亡。     9.1  1823049\n24     当幸福来敲门                          平民励志片。      9.2  1606700"
  },
  {
    "objectID": "labs/Practice/analysis_dataset.html",
    "href": "labs/Practice/analysis_dataset.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pathlib import Path\nimport pingouin as pg\nfrom lets_plot import *\n\nLetsPlot.setup_html(no_js=True)\n\n\n# Create a dictionary with the data in\ndata = {\n    \"Copenhagen\": [14.1, 14.1, 13.7, 12.9, 12.3, 11.7, 10.8, 10.6, 9.8, 5.3],\n    \"Dniprop\": [11.0, 12.6, 12.1, 11.2, 11.3, 10.5, 9.5, 10.3, 9.0, 8.7],\n    \"Minsk\": [12.8, 12.3, 12.6, 12.3, 11.8, 9.9, 9.9, 8.4, 8.3, 6.9],\n}\n\ndf = pd.DataFrame.from_dict(data)\ndf.head()\n\n\n\n\n\n\n\n\nCopenhagen\nDniprop\nMinsk\n\n\n\n\n0\n14.1\n11.0\n12.8\n\n\n1\n14.1\n12.6\n12.3\n\n\n2\n13.7\n12.1\n12.6\n\n\n3\n12.9\n11.2\n12.3\n\n\n4\n12.3\n11.3\n11.8\n\n\n\n\n\n\n\n\n# Plot the data\nfig, ax = plt.subplots()\ndf.plot(ax=ax)\nax.set_title(\"Average contribution to public goods game: without punishment\")\nax.set_ylabel(\"Average contribution\")\nax.set_xlabel(\"Round\")\n\nText(0.5, 0, 'Round')\n\n\n\n\n\n\n\n\n\n\ndata_np = pd.read_excel(\n    \"D:\\360MoveData\\Users\\75903\\Desktop\\SuYibo-Homework\\Practice2 SuYibo/Public-goods-experimental-data.xlsx\",\n    usecols=\"A:Q\",\n    header=1,\n    index_col=\"Period\",\n)\ndata_n = data_np.iloc[:10, :].copy()\ndata_p = data_np.iloc[14:24, :].copy()\n\n\ntest_data = {\n    \"City A\": [14.1, 14.1, 13.7],\n    \"City B\": [11.0, 12.6, 12.1],\n}\n\n# Original dataframe\ntest_df = pd.DataFrame.from_dict(test_data)\n# A copy of the dataframe\ntest_copy = test_df.copy()\n# A pointer to the dataframe\ntest_pointer = test_df\n\n\ntest_pointer.iloc[1, 1] = 99\n\n\nprint(\"test_df=\")\nprint(f\"{test_df}\\n\")\nprint(\"test_copy=\")\nprint(f\"{test_copy}\\n\")\n\ntest_df=\n   City A  City B\n0    14.1    11.0\n1    14.1    99.0\n2    13.7    12.1\n\ntest_copy=\n   City A  City B\n0    14.1    11.0\n1    14.1    12.6\n2    13.7    12.1\n\n\n\n\ndata_n.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 10 entries, 1 to 10\nData columns (total 16 columns):\n #   Column           Non-Null Count  Dtype \n---  ------           --------------  ----- \n 0   Copenhagen       10 non-null     object\n 1   Dnipropetrovs’k  10 non-null     object\n 2   Minsk            10 non-null     object\n 3   St. Gallen       10 non-null     object\n 4   Muscat           10 non-null     object\n 5   Samara           10 non-null     object\n 6   Zurich           10 non-null     object\n 7   Boston           10 non-null     object\n 8   Bonn             10 non-null     object\n 9   Chengdu          10 non-null     object\n 10  Seoul            10 non-null     object\n 11  Riyadh           10 non-null     object\n 12  Nottingham       10 non-null     object\n 13  Athens           10 non-null     object\n 14  Istanbul         10 non-null     object\n 15  Melbourne        10 non-null     object\ndtypes: object(16)\nmemory usage: 1.3+ KB\n\n\n\ndata_n = data_n.astype(\"double\")\ndata_p = data_p.astype(\"double\")\n\n\nmean_n_c = data_n.mean(axis=1)\nmean_p_c = data_p.agg(np.mean, axis=1)\n\n\nfig, ax = plt.subplots()\nmean_n_c.plot(ax=ax, label=\"Without punishment\")\nmean_p_c.plot(ax=ax, label=\"With punishment\")\nax.set_title(\"Average contribution to public goods game\")\nax.set_ylabel(\"Average contribution\")\nax.legend()\n\n\n\n\n\n\n\n\n\npartial_names_list = [\"F. Kennedy\", \"Lennon\", \"Maynard Keynes\", \"Wayne\"]\n[\"John \" + name for name in partial_names_list]\n\n['John F. Kennedy', 'John Lennon', 'John Maynard Keynes', 'John Wayne']\n\n\n\n# Create new dataframe with bars in\ncompare_grps = pd.DataFrame(\n    [mean_n_c.loc[[1, 10]], mean_p_c.loc[[1, 10]]],\n    index=[\"Without punishment\", \"With punishment\"],\n)\n# Rename columns to have \"round\" in them\ncompare_grps.columns = [\"Round \" + str(i) for i in compare_grps.columns]\n# flip cols and index round ready for plotting (.T is transpose)\ncompare_grps = compare_grps.T\n# Make a bar chart\ncompare_grps.plot.bar(rot=0)\n\n\n\n\n\n\n\n\n\nn_c = data_n.agg([\"std\", \"var\", \"mean\"], 1)\nn_c\n\n\n\n\n\n\n\n\nstd\nvar\nmean\n\n\nPeriod\n\n\n\n\n\n\n\n1\n2.020724\n4.083325\n10.578313\n\n\n2\n2.238129\n5.009220\n10.628398\n\n\n3\n2.329569\n5.426891\n10.407079\n\n\n4\n2.068213\n4.277504\n9.813033\n\n\n5\n2.108329\n4.445049\n9.305433\n\n\n6\n2.240881\n5.021549\n8.454844\n\n\n7\n2.136614\n4.565117\n7.837568\n\n\n8\n2.349442\n5.519880\n7.376388\n\n\n9\n2.413845\n5.826645\n6.392985\n\n\n10\n2.187126\n4.783520\n4.383769\n\n\n\n\n\n\n\n\np_c = data_p.agg([\"std\", \"var\", \"mean\"], 1)\n\n\nfig, ax = plt.subplots()\nn_c[\"mean\"].plot(ax=ax, label=\"mean\")\n# mean + 2 sd\n(n_c[\"mean\"] + 2 * n_c[\"std\"]).plot(ax=ax, ylim=(0, None), color=\"red\", label=\"±2 s.d.\")\n# mean - 2 sd\n(n_c[\"mean\"] - 2 * n_c[\"std\"]).plot(ax=ax, ylim=(0, None), color=\"red\", label=\"\")\nfor i in range(len(data_n.columns)):\n    ax.scatter(x=data_n.index, y=data_n.iloc[:, i], color=\"k\", alpha=0.3)\nax.legend()\nax.set_ylabel(\"Average contribution\")\nax.set_title(\"Contribution to public goods game without punishment\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\np_c[\"mean\"].plot(ax=ax, label=\"mean\")\n# mean + 2 sd\n(p_c[\"mean\"] + 2 * p_c[\"std\"]).plot(ax=ax, ylim=(0, None), color=\"red\", label=\"±2 s.d.\")\n# mean - 2 sd\n(p_c[\"mean\"] - 2 * p_c[\"std\"]).plot(ax=ax, ylim=(0, None), color=\"red\", label=\"\")\nfor i in range(len(data_p.columns)):\n    ax.scatter(x=data_p.index, y=data_p.iloc[:, i], color=\"k\", alpha=0.3)\nax.legend()\nax.set_ylabel(\"Average contribution\")\nax.set_title(\"Contribution to public goods game without punishment\")\nplt.show()\n\n\n\n\n\n\n\n\n\ndata_p.apply(lambda x: x.max() - x.min(), axis=1)\n\nPeriod\n1     10.199675\n2     12.185065\n3     12.689935\n4     12.625000\n5     12.140375\n6     12.827541\n7     13.098931\n8     13.482621\n9     13.496754\n10    11.307360\ndtype: float64\n\n\n\n# A lambda function accepting three inputs, a,b and c and calculating the sum of the squares\ntest_function = lambda a, b, c: a**2 + b**2 + c**2\n\n# Now we apply the function by handing over (in parenthesis) the following inputs: a=3, b=4 and c=5\ntest_function(3, 4, 5)\n\n50\n\n\n\nrange_function = lambda x: x.max() - x.min()\nrange_p = data_p.apply(range_function, axis=1)\nrange_n = data_n.apply(range_function, axis=1)\n\n\nfig, ax = plt.subplots()\nrange_p.plot(ax=ax, label=\"With punishment\")\nrange_n.plot(ax=ax, label=\"Without punishment\")\nax.set_ylim(0, None)\nax.legend()\nax.set_title(\"Range of contributions to the public goods game\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfuncs_to_apply = [range_function, \"max\", \"min\", \"std\", \"mean\"]\nsumm_p = data_p.apply(funcs_to_apply, axis=1).rename(columns={\"&lt;lambda&gt;\": \"range\"})\nsumm_n = data_n.apply(funcs_to_apply, axis=1).rename(columns={\"&lt;lambda&gt;\": \"range\"})\n\n\nsumm_n.loc[[1, 10], :].round(2)\n\n\n\n\n\n\n\n\nrange\nmax\nmin\nstd\nmean\n\n\nPeriod\n\n\n\n\n\n\n\n\n\n1\n6.14\n14.10\n7.96\n2.02\n10.58\n\n\n10\n7.38\n8.68\n1.30\n2.19\n4.38\n\n\n\n\n\n\n\n\nsumm_p.loc[[1, 10], :].round(2)\n\n\n\n\n\n\n\n\nrange\nmax\nmin\nstd\nmean\n\n\nPeriod\n\n\n\n\n\n\n\n\n\n1\n10.20\n16.02\n5.82\n3.21\n10.64\n\n\n10\n11.31\n17.51\n6.20\n3.90\n12.87\n\n\n\n\n\n\n\n\npg.ttest(x=data_n.iloc[0, :], y=data_p.iloc[0, :])\n\n\n\n\n\n\n\n\nT\ndof\nalternative\np-val\nCI95%\ncohen-d\nBF10\npower\n\n\n\n\nT-test\n-0.063782\n30\ntwo-sided\n0.949567\n[-2.0, 1.87]\n0.02255\n0.337\n0.050437\n\n\n\n\n\n\n\n\npg.ttest(x=data_n.iloc[0, :], y=data_p.iloc[0, :], paired=True)\n\n\n\n\n\n\n\n\nT\ndof\nalternative\np-val\nCI95%\ncohen-d\nBF10\npower\n\n\n\n\nT-test\n-0.149959\n15\ntwo-sided\n0.882795\n[-0.92, 0.8]\n0.02255\n0.258\n0.05082"
  },
  {
    "objectID": "labs.html",
    "href": "labs.html",
    "title": "01Chipotle-Exercises-with-solutions",
    "section": "",
    "text": "01Chipotle-Exercises-with-solutions\n\n\nEx2 - Getting and Knowing your Data\nThis time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\nimport numpy as np\n\n\n\nStep 2. Import the dataset from this address.\n\nchipo = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv', sep= '\\t')\n\n\n\nStep 3. Assign it to a variable called chipo.\n\nchipo = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv', sep= '\\t')\n\n\n\nStep 4. See the first 10 entries\n\nchipo.head(10)\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nchoice_description\nitem_price\n\n\n\n\n0\n1\n1\nChips and Fresh Tomato Salsa\nNaN\n$2.39\n\n\n1\n1\n1\nIzze\n[Clementine]\n$3.39\n\n\n2\n1\n1\nNantucket Nectar\n[Apple]\n$3.39\n\n\n3\n1\n1\nChips and Tomatillo-Green Chili Salsa\nNaN\n$2.39\n\n\n4\n2\n2\nChicken Bowl\n[Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n$16.98\n\n\n5\n3\n1\nChicken Bowl\n[Fresh Tomato Salsa (Mild), [Rice, Cheese, Sou...\n$10.98\n\n\n6\n3\n1\nSide of Chips\nNaN\n$1.69\n\n\n7\n4\n1\nSteak Burrito\n[Tomatillo Red Chili Salsa, [Fajita Vegetables...\n$11.75\n\n\n8\n4\n1\nSteak Soft Tacos\n[Tomatillo Green Chili Salsa, [Pinto Beans, Ch...\n$9.25\n\n\n9\n5\n1\nSteak Burrito\n[Fresh Tomato Salsa, [Rice, Black Beans, Pinto...\n$9.25\n\n\n\n\n\n\n\n\n\nStep 5. What is the number of observations in the dataset?\n\n# Solution 1\n\nchipo.shape\n\n(4622, 5)\n\n\n\n# Solution 2\n\nchipo.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4622 entries, 0 to 4621\nData columns (total 5 columns):\n #   Column              Non-Null Count  Dtype \n---  ------              --------------  ----- \n 0   order_id            4622 non-null   int64 \n 1   quantity            4622 non-null   int64 \n 2   item_name           4622 non-null   object\n 3   choice_description  3376 non-null   object\n 4   item_price          4622 non-null   object\ndtypes: int64(2), object(3)\nmemory usage: 180.7+ KB\n\n\n\n\nStep 6. What is the number of columns in the dataset?\n\nchipo.shape[1]\n\n5\n\n\n\n\nStep 7. Print the name of all the columns.\n\nchipo.columns\n\nIndex(['order_id', 'quantity', 'item_name', 'choice_description',\n       'item_price'],\n      dtype='object')\n\n\n\n\nStep 8. How is the dataset indexed?\n\nchipo.index\n\nRangeIndex(start=0, stop=4622, step=1)\n\n\n\n\nStep 9. Which was the most-ordered item?\n\nchipo.groupby(by=\"item_name\").sum().sort_values('quantity',ascending=False).head(1)\n\n\n\n\n\n\n\n\norder_id\nquantity\nchoice_description\nitem_price\n\n\nitem_name\n\n\n\n\n\n\n\n\nChicken Bowl\n713926\n761\n[Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n$16.98 $10.98 $11.25 $8.75 $8.49 $11.25 $8.75 ...\n\n\n\n\n\n\n\n\n\nStep 10. For the most-ordered item, how many items were ordered?\n\nchipo.groupby(by=\"item_name\").sum().sort_values('quantity',ascending=False).head(1)\n\n\n\n\n\n\n\n\norder_id\nquantity\nchoice_description\nitem_price\n\n\nitem_name\n\n\n\n\n\n\n\n\nChicken Bowl\n713926\n761\n[Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n$16.98 $10.98 $11.25 $8.75 $8.49 $11.25 $8.75 ...\n\n\n\n\n\n\n\n\n\nStep 11. What was the most ordered item in the choice_description column?\n\nchipo.groupby(by=\"choice_description\").sum().sort_values('quantity',ascending=False).head(1)\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nitem_price\n\n\nchoice_description\n\n\n\n\n\n\n\n\n[Diet Coke]\n123455\n159\nCanned SodaCanned SodaCanned Soda6 Pack Soft D...\n$2.18 $1.09 $1.09 $6.49 $2.18 $1.25 $1.09 $6.4...\n\n\n\n\n\n\n\n\n\nStep 12. How many items were orderd in total?\n\nchipo.item_name.count()\n\n4622\n\n\n\n\nStep 13. Turn the item price into a float\n\nStep 13.a. Check the item price type\n\nchipo.item_price.dtype\n\ndtype('O')\n\n\n\n\nStep 13.b. Create a lambda function and change the type of item price\n\ndollarizer = lambda x: float(x[1:-1])\nchipo.item_price = chipo.item_price.apply(dollarizer)\n\n\n\nStep 13.c. Check the item price type\n\nchipo.item_price.dtype\n\ndtype('float64')\n\n\n\n\n\nStep 14. How much was the revenue for the period in the dataset?\n\nrevenue =  (chipo.item_price * chipo.quantity).sum()\nprint('Revenue is : $ '+ str(revenue))\n\nRevenue is : $ 39237.02\n\n\n\n\nStep 15. How many orders were made in the period?\n\nchipo.order_id.value_counts().count()\n\n1834\n\n\n\n\nStep 16. What is the average revenue amount per order?\n\n# Solution 1\nchipo['revenue'] = chipo['quantity'] * chipo['item_price']\norder_grouped = chipo.groupby(by=['order_id']).sum()\norder_grouped['revenue'].mean()\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[8], line 4\n      2 chipo['revenue'] = chipo['quantity'] * chipo['item_price']\n      3 order_grouped = chipo.groupby(by=['order_id']).sum()\n----&gt; 4 order_grouped['revenue'].mean()\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:6549, in Series.mean(self, axis, skipna, numeric_only, **kwargs)\n   6541 @doc(make_doc(\"mean\", ndim=1))\n   6542 def mean(\n   6543     self,\n   (...)\n   6547     **kwargs,\n   6548 ):\n-&gt; 6549     return NDFrame.mean(self, axis, skipna, numeric_only, **kwargs)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:12420, in NDFrame.mean(self, axis, skipna, numeric_only, **kwargs)\n  12413 def mean(\n  12414     self,\n  12415     axis: Axis | None = 0,\n   (...)\n  12418     **kwargs,\n  12419 ) -&gt; Series | float:\n&gt; 12420     return self._stat_function(\n  12421         \"mean\", nanops.nanmean, axis, skipna, numeric_only, **kwargs\n  12422     )\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:12377, in NDFrame._stat_function(self, name, func, axis, skipna, numeric_only, **kwargs)\n  12373 nv.validate_func(name, (), kwargs)\n  12375 validate_bool_kwarg(skipna, \"skipna\", none_allowed=False)\n&gt; 12377 return self._reduce(\n  12378     func, name=name, axis=axis, skipna=skipna, numeric_only=numeric_only\n  12379 )\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:6457, in Series._reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\n   6452     # GH#47500 - change to TypeError to match other methods\n   6453     raise TypeError(\n   6454         f\"Series.{name} does not allow {kwd_name}={numeric_only} \"\n   6455         \"with non-numeric dtypes.\"\n   6456     )\n-&gt; 6457 return op(delegate, skipna=skipna, **kwds)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:147, in bottleneck_switch.__call__.&lt;locals&gt;.f(values, axis, skipna, **kwds)\n    145         result = alt(values, axis=axis, skipna=skipna, **kwds)\n    146 else:\n--&gt; 147     result = alt(values, axis=axis, skipna=skipna, **kwds)\n    149 return result\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:404, in _datetimelike_compat.&lt;locals&gt;.new_func(values, axis, skipna, mask, **kwargs)\n    401 if datetimelike and mask is None:\n    402     mask = isna(values)\n--&gt; 404 result = func(values, axis=axis, skipna=skipna, mask=mask, **kwargs)\n    406 if datetimelike:\n    407     result = _wrap_results(result, orig_values.dtype, fill_value=iNaT)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:720, in nanmean(values, axis, skipna, mask)\n    718 count = _get_counts(values.shape, mask, axis, dtype=dtype_count)\n    719 the_sum = values.sum(axis, dtype=dtype_sum)\n--&gt; 720 the_sum = _ensure_numeric(the_sum)\n    722 if axis is not None and getattr(the_sum, \"ndim\", False):\n    723     count = cast(np.ndarray, count)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:1701, in _ensure_numeric(x)\n   1698 elif not (is_float(x) or is_integer(x) or is_complex(x)):\n   1699     if isinstance(x, str):\n   1700         # GH#44008, GH#36703 avoid casting e.g. strings to numeric\n-&gt; 1701         raise TypeError(f\"Could not convert string '{x}' to numeric\")\n   1702     try:\n   1703         x = float(x)\n\nTypeError: Could not convert string '$2.39 $3.39 $3.39 $2.39 $16.98 $16.98 $10.98 $1.69 $11.75 $9.25 $9.25 $4.45 $8.75 $8.75 $11.25 $4.45 $2.39 $8.49 $8.49 $2.18 $2.18 $8.75 $4.45 $8.99 $3.39 $10.98 $3.39 $2.39 $8.49 $8.99 $1.09 $8.49 $2.39 $8.99 $1.69 $8.99 $1.09 $8.75 $8.75 $4.45 $2.95 $11.75 $2.15 $4.45 $11.25 $11.75 $8.75 $10.98 $8.99 $3.39 $8.99 $3.99 $8.99 $2.18 $2.18 $10.98 $1.09 $8.99 $2.39 $9.25 $11.25 $11.75 $2.15 $4.45 $9.25 $11.25 $8.75 $8.99 $8.99 $3.39 $8.99 $10.98 $8.99 $1.69 $8.99 $3.99 $8.75 $4.45 $8.75 $8.75 $2.15 $8.75 $11.25 $2.15 $9.25 $8.75 $8.75 $9.25 $8.49 $8.99 $1.09 $9.25 $2.95 $11.75 $11.75 $9.25 $11.75 $4.45 $9.25 $4.45 $11.75 $8.75 $8.75 $4.45 $8.99 $8.99 $3.99 $8.49 $3.39 $8.99 $1.09 $9.25 $4.45 $8.75 $2.95 $4.45 $2.39 $8.49 $8.99 $8.49 $1.09 $8.99 $3.99 $8.75 $9.25 $4.45 $11.25 $4.45 $8.99 $1.09 $9.25 $2.95 $4.45 $11.75 $4.45 $8.49 $2.39 $10.98 $22.50 $22.50 $11.75 $4.45 $11.25 $4.45 $11.25 $4.45 $11.25 $11.25 $11.75 $9.25 $4.45 $11.48 $17.98 $17.98 $1.69 $17.50 $17.50 $4.45 $8.49 $2.39 $17.50 $17.50 $4.45 $4.45 $11.25 $11.75 $10.98 $8.49 $10.98 $2.18 $2.18 $11.48 $8.49 $2.39 $4.45 $11.25 $11.75 $8.75 $8.49 $2.18 $2.18 $8.49 $3.39 $8.49 $8.99 $10.98 $11.48 $8.49 $1.09 $1.09 $9.25 $8.75 $2.95 $9.25 $4.45 $11.25 $11.48 $8.49 $8.49 $8.99 $2.39 $11.25 $8.75 $2.95 $1.09 $8.99 $8.49 $2.39 $10.98 $1.09 $3.99 $11.25 $8.75 $8.49 $3.39 $8.75 $9.25 $2.15 $11.25 $11.25 $11.25 $4.45 $22.50 $22.50 $4.45 $11.75 $8.75 $17.50 $17.50 $8.75 $9.25 $8.75 $2.15 $9.25 $4.30 $4.30 $8.75 $11.25 $2.15 $8.99 $1.09 $8.99 $3.99 $8.75 $2.95 $2.95 $11.75 $5.90 $5.90 $9.25 $9.25 $11.75 $9.25 $2.95 $17.50 $17.50 $8.75 $9.25 $10.98 $8.99 $1.09 $1.09 $1.09 $8.99 $10.98 $1.09 $8.75 $8.75 $9.25 $9.25 $8.75 $8.75 $8.99 $8.99 $8.99 $1.09 $11.75 $1.25 $8.99 $2.39 $9.25 $2.95 $8.99 $3.99 $8.49 $2.39 $8.49 $8.49 $8.49 $1.69 $8.49 $3.99 $8.99 $1.69 $1.09 $23.78 $23.78 $17.50 $17.50 $2.15 $8.75 $9.25 $9.25 $8.75 $4.45 $8.75 $11.25 $11.25 $1.25 $9.25 $4.45 $11.25 $11.75 $11.75 $6.49 $8.99 $2.39 $8.49 $2.39 $11.25 $8.75 $2.15 $8.99 $1.69 $8.75 $11.25 $2.15 $4.45 $8.75 $8.49 $8.99 $17.50 $17.50 $8.49 $1.09 $1.09 $8.75 $1.25 $2.15 $11.08 $8.49 $8.49 $8.99 $2.39 $8.75 $2.15 $1.50 $11.25 $2.15 $8.49 $8.49 $11.75 $9.25 $11.75 $1.25 $11.25 $8.75 $4.45 $6.49 $9.25 $2.95 $11.25 $4.45 $1.25 $1.25 $8.49 $2.39 $2.18 $2.18 $8.49 $2.18 $2.18 $22.16 $22.16 $17.50 $17.50 $8.75 $2.95 $6.49 $8.99 $3.39 $3.39 $8.99 $8.49 $11.25 $2.15 $11.25 $2.95 $11.25 $1.25 $8.99 $1.09 $8.75 $8.75 $9.25 $2.95 $11.75 $1.50 $8.99 $1.09 $11.25 $1.25 $1.25 $11.25 $11.75 $2.15 $8.99 $1.69 $11.75 $6.49 $8.75 $9.25 $11.25 $4.45 $1.25 $11.25 $4.45 $8.49 $8.99 $8.49 $8.99 $11.25 $1.25 $11.75 $1.25 $11.75 $9.25 $4.45 $11.25 $2.15 $32.94 $32.94 $32.94 $1.25 $11.25 $11.48 $1.69 $1.09 $17.50 $17.50 $4.45 $6.49 $9.25 $8.75 $9.25 $9.25 $8.75 $8.75 $2.15 $2.95 $17.50 $17.50 $10.98 $11.48 $11.48 $3.39 $8.99 $1.69 $8.99 $1.09 $10.98 $3.39 $8.99 $1.09 $9.25 $8.75 $11.25 $4.45 $2.95 $9.25 $22.20 $22.20 $22.20 $8.49 $8.99 $8.75 $8.75 $11.75 $8.75 $11.25 $9.25 $11.25 $11.25 $8.75 $11.25 $2.95 $1.25 $8.49 $1.69 $11.75 $11.25 $8.75 $8.75 $4.45 $8.49 $3.99 $8.49 $3.99 $11.48 $1.69 $1.09 $11.25 $1.50 $10.58 $1.69 $9.25 $11.25 $8.75 $9.25 $11.25 $11.25 $8.75 $11.75 $8.75 $8.75 $8.75 $2.15 $11.25 $11.75 $2.50 $2.50 $4.45 $9.25 $4.45 $11.25 $8.49 $3.99 $9.25 $9.25 $11.25 $9.25 $11.75 $11.25 $1.25 $23.50 $23.50 $1.25 $8.99 $8.49 $7.40 $7.40 $8.75 $1.25 $4.45 $8.75 $2.15 $8.75 $4.45 $7.40 $7.40 $7.40 $8.99 $3.99 $8.99 $1.69 $8.75 $8.75 $8.75 $8.75 $11.25 $11.25 $2.95 $8.75 $18.50 $18.50 $8.49 $3.99 $2.95 $9.25 $9.25 $3.00 $3.00 $1.25 $8.75 $9.25 $4.45 $8.75 $11.25 $4.45 $10.98 $22.16 $22.16 $4.45 $8.75 $9.25 $6.49 $9.25 $11.25 $8.75 $9.25 $2.15 $9.25 $4.45 $9.25 $2.95 $9.25 $8.75 $9.25 $1.25 $1.25 $8.75 $8.75 $9.25 $4.45 $11.75 $11.75 $11.75 $9.25 $9.25 $16.98 $16.98 $2.39 $3.39 $3.39 $9.25 $11.75 $11.25 $2.15 $8.75 $9.25 $4.45 $10.98 $11.25 $9.25 $22.50 $22.50 $9.25 $2.95 $1.50 $11.48 $8.49 $1.69 $8.49 $8.49 $8.49 $6.78 $6.78 $11.75 $4.45 $8.75 $4.45 $11.89 $9.39 $8.75 $2.95 $1.25 $9.25 $8.75 $23.78 $23.78 $8.75 $9.25 $2.15 $2.15 $1.25 $8.49 $3.99 $10.98 $1.09 $8.75 $4.45 $8.75 $11.75 $2.95 $4.45 $9.25 $8.75 $8.49 $3.99 $22.50 $22.50 $11.25 $1.25 $8.75 $8.75 $18.50 $18.50 $6.49 $8.75 $8.75 $4.45 $8.49 $3.99 $8.99 $1.09 $8.49 $2.39 $11.48 $1.69 $2.50 $2.50 $9.25 $1.50 $17.50 $17.50 $2.95 $8.75 $4.45 $11.75 $8.75 $8.49 $1.69 $8.49 $3.99 $8.99 $8.99 $3.99 $8.99 $11.25 $4.45 $1.25 $3.99 $10.98 $7.40 $3.00 $7.40 $4.00 $8.49 $3.99 $9.25 $4.45 $11.25 $1.25 $11.75 $1.25 $11.25 $2.15 $11.25 $4.45 $3.75 $3.75 $3.75 $11.75 $8.99 $2.39 $8.75 $4.45 $1.25 $8.99 $8.49 $2.18 $2.18 $8.49 $2.18 $2.18 $1.09 $8.75 $2.95 $1.25 $1.50 $11.25 $9.25 $2.95 $1.25 $8.49 $3.99 $11.48 $3.99 $8.49 $11.25 $1.25 $8.99 $1.69 $11.25 $1.25 $6.49 $8.75 $9.25 $8.75 $2.95 $8.75 $11.75 $8.69 $8.69 $2.29 $3.99 $8.49 $8.75 $8.75 $1.25 $11.75 $11.25 $11.25 $11.25 $1.25 $9.25 $11.75 $6.49 $3.99 $8.49 $11.25 $2.15 $11.25 $11.89 $8.99 $1.69 $8.99 $8.99 $3.99 $8.99 $9.25 $9.25 $2.15 $7.40 $7.40 $8.75 $8.75 $9.25 $4.45 $11.25 $1.25 $11.75 $11.25 $1.25 $3.99 $8.49 $8.49 $8.49 $8.99 $8.75 $2.15 $1.25 $8.49 $1.09 $1.09 $8.75 $2.95 $1.25 $9.25 $1.25 $2.15 $11.25 $1.25 $4.45 $8.75 $2.50 $2.50 $8.90 $8.90 $8.75 $8.75 $8.75 $11.25 $11.25 $10.98 $3.99 $10.98 $3.99 $1.69 $8.99 $9.25 $8.75 $8.99 $1.09 $9.25 $2.95 $8.75 $9.25 $3.99 $8.49 $8.75 $8.75 $22.50 $22.50 $10.98 $3.27 $3.27 $3.27 $3.99 $8.99 $1.09 $11.08 $8.75 $4.45 $11.08 $3.99 $8.49 $4.30 $4.30 $9.25 $8.75 $11.25 $11.25 $9.25 $8.49 $8.99 $8.49 $8.75 $2.95 $4.45 $9.25 $2.95 $9.25 $8.75 $11.25 $4.45 $16.98 $16.98 $8.49 $2.39 $11.25 $3.75 $3.75 $3.75 $9.25 $4.45 $9.25 $9.25 $4.45 $8.75 $9.25 $8.75 $9.25 $9.25 $9.25 $11.48 $8.99 $22.50 $22.50 $11.75 $11.25 $1.25 $8.75 $2.15 $1.25 $11.25 $8.75 $1.25 $11.25 $1.50 $11.25 $11.25 $9.25 $6.49 $8.90 $8.90 $8.75 $4.45 $11.25 $1.25 $17.50 $17.50 $9.25 $8.75 $11.75 $3.00 $3.00 $8.49 $8.49 $10.98 $8.99 $3.99 $8.75 $4.45 $8.99 $1.69 $11.75 $8.75 $11.25 $4.45 $11.75 $1.25 $11.75 $2.95 $8.99 $8.99 $2.18 $2.18 $17.98 $17.98 $8.99 $8.49 $1.69 $11.75 $11.25 $2.95 $3.75 $3.75 $3.75 $9.25 $11.75 $8.75 $2.15 $1.50 $8.49 $8.49 $3.39 $8.69 $3.89 $8.75 $4.45 $8.75 $11.25 $2.15 $8.75 $8.49 $1.69 $8.49 $8.49 $1.25 $8.75 $11.75 $11.75 $8.99 $1.09 $8.75 $4.45 $8.75 $2.95 $8.75 $2.15 $3.99 $8.49 $8.99 $3.99 $8.49 $1.69 $1.09 $8.99 $1.09 $9.25 $8.75 $8.99 $2.39 $1.25 $1.25 $11.25 $11.25 $9.25 $9.25 $11.25 $1.50 $3.99 $8.49 $11.25 $9.25 $11.25 $17.50 $17.50 $8.75 $8.90 $8.90 $8.75 $8.75 $8.99 $2.39 $11.25 $9.25 $2.15 $11.25 $1.25 $11.75 $1.25 $11.25 $11.75 $1.25 $11.25 $11.25 $8.49 $10.98 $8.75 $1.25 $8.75 $8.49 $8.49 $1.50 $1.50 $8.75 $4.45 $11.25 $1.25 $11.75 $8.49 $2.39 $9.25 $4.45 $9.25 $8.75 $8.99 $1.69 $17.50 $17.50 $2.39 $8.99 $8.99 $11.25 $4.45 $8.75 $4.45 $9.25 $6.49 $10.98 $8.49 $8.49 $1.09 $1.69 $9.25 $4.45 $8.75 $1.25 $2.95 $3.99 $8.49 $11.75 $11.75 $2.15 $11.48 $8.75 $2.15 $1.25 $11.25 $2.15 $1.25 $8.75 $8.75 $6.49 $1.69 $8.99 $8.75 $11.75 $10.98 $1.09 $8.49 $3.39 $8.75 $2.15 $1.25 $11.48 $10.98 $10.98 $8.49 $2.95 $9.25 $9.25 $11.75 $4.45 $11.48 $11.25 $8.75 $4.45 $1.69 $8.99 $8.75 $4.45 $1.50 $11.75 $2.15 $8.99 $2.39 $8.75 $2.95 $1.25 $8.75 $2.15 $1.25 $2.18 $2.18 $2.18 $2.18 $11.48 $8.75 $2.95 $11.75 $11.75 $1.25 $10.58 $8.99 $2.39 $11.75 $4.45 $11.25 $11.25 $17.50 $17.50 $8.75 $8.75 $8.75 $22.50 $22.50 $9.25 $8.75 $4.45 $11.75 $1.25 $11.25 $11.25 $2.95 $8.99 $1.69 $11.25 $4.45 $8.75 $6.49 $8.75 $4.45 $9.25 $4.45 $11.75 $11.75 $4.45 $11.89 $11.75 $11.25 $2.95 $1.50 $4.45 $8.75 $8.99 $1.09 $8.99 $1.09 $3.99 $11.48 $8.49 $9.25 $4.45 $11.48 $9.25 $2.95 $9.25 $8.49 $8.99 $8.99 $8.49 $8.75 $2.95 $4.45 $11.89 $10.58 $8.19 $1.69 $8.75 $2.15 $1.25 $17.50 $17.50 $6.49 $9.25 $2.15 $8.75 $4.45 $8.75 $1.25 $11.48 $11.48 $8.99 $2.18 $2.18 $8.49 $8.99 $2.39 $2.39 $2.18 $2.18 $8.75 $4.45 $11.25 $9.25 $9.25 $11.25 $11.25 $4.45 $2.95 $11.75 $8.49 $8.49 $8.99 $1.69 $9.25 $11.25 $11.75 $9.25 $8.75 $11.75 $8.75 $8.75 $11.25 $11.25 $10.98 $11.25 $4.45 $10.98 $8.49 $8.99 $3.39 $3.99 $8.99 $1.09 $1.09 $2.39 $17.50 $17.50 $4.45 $11.25 $11.25 $4.45 $9.25 $4.45 $8.75 $2.15 $1.25 $11.89 $2.95 $11.75 $1.25 $11.25 $4.45 $11.48 $11.48 $2.95 $9.25 $8.75 $9.25 $2.95 $11.25 $1.25 $11.75 $1.25 $8.99 $2.39 $1.25 $11.25 $1.25 $11.25 $8.49 $3.99 $35.00 $35.00 $35.00 $35.00 $27.75 $27.75 $27.75 $8.75 $11.80 $11.80 $11.80 $11.80 $8.90 $8.90 $5.90 $5.90 $6.49 $10.98 $17.98 $17.98 $2.39 $9.25 $8.75 $2.15 $8.75 $4.45 $8.49 $1.69 $8.19 $8.69 $10.98 $3.99 $11.48 $11.48 $4.45 $8.75 $6.49 $8.75 $8.75 $9.25 $1.25 $4.45 $8.49 $1.69 $9.25 $4.45 $8.99 $1.09 $11.25 $2.95 $11.08 $11.08 $3.89 $10.98 $11.25 $8.75 $11.25 $9.25 $4.30 $4.30 $8.75 $8.49 $3.99 $1.69 $8.99 $8.49 $1.69 $11.75 $11.25 $11.89 $9.25 $2.95 $9.25 $2.95 $8.75 $4.45 $4.45 $8.75 $10.98 $11.48 $8.49 $9.25 $4.45 $11.75 $11.89 $8.99 $8.49 $8.75 $9.25 $8.75 $8.75 $11.75 $11.75 $4.45 $11.25 $11.75 $2.50 $2.50 $8.99 $1.69 $11.75 $2.15 $1.25 $9.25 $8.75 $8.90 $8.90 $9.25 $2.95 $8.75 $11.25 $8.90 $8.90 $11.25 $11.75 $11.48 $1.69 $3.39 $9.25 $2.95 $8.99 $1.69 $8.49 $10.98 $11.25 $2.95 $8.99 $1.69 $8.75 $2.15 $1.25 $8.75 $2.95 $9.25 $2.50 $2.50 $11.25 $1.25 $11.75 $2.50 $2.50 $11.25 $1.50 $8.75 $1.25 $2.95 $11.48 $11.48 $8.75 $8.75 $2.15 $11.75 $1.25 $9.25 $9.25 $6.49 $11.75 $8.49 $8.49 $1.09 $10.98 $8.75 $1.25 $2.15 $11.25 $1.50 $11.25 $11.25 $8.49 $8.49 $8.75 $1.50 $1.25 $1.50 $8.75 $2.50 $2.50 $2.15 $7.40 $7.40 $4.00 $9.25 $9.39 $9.25 $9.25 $9.39 $11.25 $8.90 $8.90 $11.25 $6.00 $6.00 $6.00 $6.00 $11.25 $11.25 $11.25 $22.50 $22.50 $11.48 $1.09 $8.49 $8.49 $17.50 $17.50 $11.25 $1.50 $9.25 $8.75 $3.99 $8.49 $8.75 $8.75 $8.75 $8.75 $8.75 $11.75 $1.50 $11.25 $11.25 $2.95 $8.99 $10.98 $9.25 $8.75 $4.45 $8.49 $1.09 $2.39 $8.75 $8.75 $11.48 $8.99 $8.49 $8.49 $2.39 $10.98 $8.49 $3.99 $11.75 $4.45 $8.75 $2.15 $1.25 $10.98 $8.99 $11.25 $1.50 $8.75 $2.15 $1.25 $8.75 $9.25 $8.75 $11.25 $1.50 $8.75 $1.25 $4.45 $10.98 $8.75 $2.95 $1.25 $8.75 $2.95 $1.25 $8.49 $8.49 $2.39 $11.25 $1.25 $8.75 $8.75 $9.25 $8.75 $11.89 $1.25 $8.75 $2.15 $1.25 $8.99 $1.09 $8.75 $4.45 $26.25 $26.25 $26.25 $8.75 $4.45 $11.75 $2.95 $8.75 $8.75 $11.75 $8.75 $11.25 $11.25 $11.25 $4.45 $1.25 $8.49 $8.49 $8.49 $8.99 $8.99 $2.39 $2.39 $3.99 $8.75 $4.45 $2.15 $9.25 $1.25 $11.25 $11.75 $8.75 $4.45 $11.25 $2.15 $8.75 $4.45 $8.75 $8.75 $1.25 $11.25 $2.15 $8.75 $5.90 $5.90 $11.75 $1.25 $9.25 $3.75 $3.75 $3.75 $8.75 $1.25 $4.45 $11.75 $4.45 $8.75 $23.50 $23.50 $8.75 $2.95 $8.75 $8.75 $11.89 $4.45 $2.95 $1.25 $8.75 $4.45 $2.95 $1.25 $8.75 $2.15 $1.25 $11.75 $2.95 $8.99 $3.39 $9.25 $9.25 $17.50 $17.50 $2.95 $11.89 $1.50 $11.25 $2.95 $9.25 $11.25 $11.25 $2.95 $8.75 $9.25 $4.30 $4.30 $8.75 $8.75 $11.25 $8.75 $4.30 $4.30 $8.75 $1.25 $2.15 $8.49 $8.49 $3.39 $3.39 $10.98 $10.98 $2.39 $11.25 $11.75 $11.75 $1.25 $5.90 $5.90 $8.75 $11.25 $9.25 $4.45 $1.50 $3.39 $8.99 $2.39 $11.25 $2.15 $11.25 $11.75 $11.75 $4.45 $11.75 $4.45 $9.25 $8.75 $8.49 $8.99 $8.49 $8.99 $11.75 $8.75 $8.49 $3.99 $3.89 $11.08 $8.49 $8.99 $8.49 $8.49 $8.49 $11.25 $2.15 $17.50 $17.50 $8.75 $2.95 $8.49 $8.49 $10.98 $1.09 $11.25 $2.15 $2.95 $1.25 $8.75 $9.25 $9.25 $9.25 $2.95 $8.75 $2.15 $1.25 $8.99 $3.99 $11.75 $2.15 $8.99 $3.39 $9.25 $8.75 $11.25 $11.25 $4.45 $8.75 $2.15 $1.25 $11.75 $4.45 $9.25 $2.95 $8.49 $8.49 $11.25 $8.75 $4.45 $11.25 $11.25 $11.25 $11.25 $4.45 $8.49 $1.69 $8.49 $3.39 $8.75 $11.25 $9.25 $8.75 $11.25 $11.25 $11.75 $11.25 $11.75 $11.25 $11.75 $21.96 $21.96 $10.98 $1.69 $11.48 $8.99 $8.49 $1.69 $9.25 $2.15 $1.50 $11.25 $1.50 $8.75 $8.75 $2.95 $8.49 $1.69 $8.75 $2.95 $1.25 $11.25 $2.15 $11.08 $8.49 $8.49 $8.49 $11.75 $1.25 $11.75 $8.75 $8.75 $8.75 $4.45 $11.25 $1.50 $23.50 $23.50 $11.75 $6.49 $8.75 $4.45 $6.49 $8.75 $2.50 $2.50 $2.15 $8.49 $2.39 $8.75 $11.75 $4.45 $8.99 $10.98 $9.25 $2.95 $9.25 $9.25 $11.75 $8.75 $8.75 $8.75 $10.98 $11.25 $9.25 $8.75 $8.75 $2.15 $11.25 $2.15 $4.45 $11.75 $8.49 $2.39 $9.25 $1.25 $1.25 $1.25 $1.25 $8.75 $2.15 $8.49 $1.69 $11.25 $1.50 $8.75 $8.75 $8.49 $3.99 $8.99 $1.09 $11.25 $1.25 $8.49 $2.39 $8.49 $8.75 $9.25 $11.25 $4.45 $11.25 $11.89 $8.99 $8.49 $8.75 $4.45 $8.75 $11.75 $11.75 $8.90 $8.90 $9.39 $2.95 $8.49 $3.99 $8.75 $2.15 $1.25 $21.96 $21.96 $8.49 $1.69 $8.75 $4.45 $8.49 $8.99 $8.49 $3.99 $8.75 $8.75 $2.95 $8.75 $17.50 $17.50 $9.25 $2.95 $8.75 $6.49 $4.30 $4.30 $8.75 $8.75 $2.15 $1.50 $8.49 $8.49 $2.39 $9.25 $4.45 $6.49 $11.75 $4.45 $10.98 $1.69 $9.39 $9.25 $9.25 $2.95 $8.75 $2.15 $1.25 $11.25 $9.25 $8.75 $11.25 $8.75 $11.25 $2.50 $2.50 $2.50 $2.50 $6.00 $6.00 $6.00 $6.00 $8.90 $8.90 $5.90 $5.90 $11.25 $11.25 $8.49 $10.98 $8.75 $2.15 $1.50 $9.25 $1.25 $1.50 $2.15 $1.25 $8.75 $2.95 $8.49 $3.99 $11.25 $4.30 $4.30 $11.75 $2.15 $18.50 $18.50 $8.49 $2.39 $8.75 $4.45 $11.75 $8.99 $3.99 $9.25 $9.25 $1.50 $8.75 $2.95 $6.49 $11.75 $8.49 $8.99 $8.75 $4.45 $6.49 $22.50 $22.50 $9.25 $2.95 $8.49 $1.69 $10.98 $8.75 $4.45 $11.25 $2.95 $8.99 $8.49 $2.39 $11.75 $6.49 $11.25 $11.75 $2.95 $8.99 $1.69 $8.99 $2.18 $2.18 $1.09 $8.99 $8.99 $1.09 $8.99 $8.99 $8.49 $10.98 $1.09 $11.75 $9.25 $11.25 $11.25 $2.15 $11.25 $8.75 $4.45 $2.95 $11.75 $1.50 $8.99 $10.98 $2.39 $8.75 $2.15 $9.25 $1.50 $8.75 $2.15 $3.99 $8.99 $6.49 $8.75 $8.90 $8.90 $8.99 $3.99 $17.50 $17.50 $11.25 $1.25 $10.98 $9.25 $4.45 $1.25 $3.00 $3.00 $11.25 $4.45 $4.45 $2.95 $9.25 $11.25 $2.15 $11.25 $11.25 $4.45 $2.95 $9.25 $11.25 $1.25 $8.75 $2.95 $1.25 $8.75 $4.45 $11.48 $11.48 $8.49 $2.39 $11.25 $11.75 $2.15 $1.50 $2.15 $8.75 $11.25 $8.90 $8.90 $11.25 $11.25 $1.25 $4.45 $9.25 $9.25 $8.75 $9.25 $8.75 $8.75 $9.25 $8.75 $11.75 $11.75 $8.75 $8.75 $8.90 $8.90 $2.95 $10.98 $8.49 $8.49 $10.98 $8.99 $8.99 $11.75 $17.50 $17.50 $11.75 $3.99 $8.49 $10.98 $1.69 $17.50 $17.50 $8.99 $2.39 $8.99 $2.39 $1.25 $8.75 $2.95 $11.75 $11.25 $17.50 $17.50 $8.49 $8.49 $2.39 $11.25 $1.50 $8.75 $3.00 $3.00 $1.25 $8.75 $4.45 $11.75 $11.75 $4.45 $21.96 $21.96 $8.75 $4.45 $8.75 $11.25 $9.25 $8.99 $2.39 $9.25 $8.75 $10.98 $8.49 $3.99 $3.39 $11.75 $1.50 $4.45 $9.25 $8.75 $1.25 $11.75 $8.75 $1.50 $8.75 $8.75 $2.15 $1.50 $8.75 $2.95 $8.75 $8.75 $17.50 $17.50 $8.75 $6.49 $4.45 $11.25 $11.25 $4.30 $4.30 $8.75 $11.25 $4.45 $8.99 $2.39 $9.25 $9.25 $9.25 $4.45 $11.75 $11.25 $2.95 $2.15 $11.25 $11.25 $8.75 $2.15 $1.50 $9.25 $4.45 $10.98 $8.99 $2.18 $2.18 $8.75 $4.45 $1.25 $8.99 $2.39 $4.45 $8.75 $10.98 $11.75 $1.50 $10.98 $8.99 $8.49 $3.99 $8.99 $8.49 $3.99 $8.49 $8.49 $8.99 $11.25 $11.25 $10.98 $10.98 $10.98 $2.39 $3.39 $8.75 $1.25 $2.95 $11.75 $1.50 $10.98 $1.69 $4.45 $8.75 $8.75 $8.75 $8.75 $4.45 $9.25 $8.75 $11.25 $8.75 $3.99 $8.99 $8.49 $11.25 $11.25 $8.75 $4.45 $8.75 $4.45 $1.25 $8.75 $8.75 $1.50 $2.15 $11.75 $11.75 $11.75 $11.75 $11.75 $1.50 $8.75 $9.25 $1.25 $8.75 $2.15 $8.99 $1.09 $4.45 $11.25 $11.75 $2.15 $8.75 $8.75 $1.25 $9.25 $2.15 $11.75 $11.25 $8.75 $11.25 $4.45 $8.49 $1.69 $8.75 $8.75 $8.99 $8.49 $9.25 $11.25 $2.95 $4.45 $11.75 $6.49 $11.48 $8.99 $4.36 $4.36 $4.36 $4.36 $11.48 $8.99 $8.49 $11.48 $8.75 $2.15 $1.50 $8.99 $1.69 $11.25 $1.25 $9.25 $9.25 $8.75 $9.25 $8.90 $8.90 $2.15 $9.25 $10.98 $8.49 $8.75 $9.25 $4.30 $4.30 $9.25 $8.75 $8.75 $2.15 $1.25 $8.75 $1.25 $8.75 $5.90 $5.90 $9.25 $8.75 $9.25 $4.45 $9.25 $11.75 $2.50 $2.50 $9.25 $2.15 $9.25 $1.50 $1.25 $11.25 $2.95 $8.75 $8.75 $8.75 $10.98 $8.75 $8.75 $8.75 $2.15 $1.25 $10.98 $8.75 $2.15 $1.50 $8.75 $2.95 $1.25 $9.25 $9.25 $8.49 $2.39 $8.75 $4.45 $9.25 $8.75 $8.75 $8.75 $9.25 $8.75 $9.25 $8.75 $8.75 $8.75 $8.75 $9.25 $8.75 $9.25 $8.75 $9.25 $8.75 $8.75 $8.75 $9.25 $8.75 $9.25 $8.75 $8.99 $8.99 $8.75 $2.95 $1.25 $11.75 $1.50 $11.25 $11.25 $8.75 $1.50 $2.15 $16.98 $16.98 $11.75 $1.50 $8.75 $4.30 $4.30 $1.50 $8.75 $2.95 $1.25 $1.25 $9.25 $4.45 $11.25 $8.75 $4.45 $8.75 $2.15 $1.25 $10.98 $1.69 $8.75 $1.25 $8.75 $1.25 $11.25 $8.75 $8.75 $8.49 $1.69 $9.25 $11.75 $8.49 $2.39 $9.25 $2.95 $6.49 $8.75 $8.75 $9.25 $8.75 $6.78 $6.78 $17.98 $17.98 $3.39 $11.75 $11.25 $8.75 $4.45 $11.75 $9.25 $8.75 $6.49 $8.99 $2.39 $8.75 $11.25 $11.75 $4.45 $8.75 $2.15 $9.25 $9.25 $9.25 $11.89 $11.75 $11.25 $9.25 $9.25 $8.75 $8.75 $8.49 $1.69 $1.09 $11.25 $1.50 $11.25 $11.25 $11.75 $1.50 $8.49 $8.99 $22.50 $22.50 $8.75 $4.30 $4.30 $8.75 $11.25 $2.15 $11.25 $2.95 $4.45 $11.25 $8.49 $3.39 $2.39 $11.75 $2.15 $11.75 $8.99 $2.39 $8.75 $11.75 $11.89 $1.25 $7.50 $7.50 $7.50 $7.50 $7.50 $11.89 $1.09 $8.49 $2.39 $8.75 $8.75 $8.75 $8.75 $9.25 $11.25 $8.75 $8.90 $8.90 $9.25 $8.75 $8.75 $11.75 $3.00 $3.00 $1.50 $11.25 $11.75 $8.99 $10.98 $4.45 $8.75 $2.15 $9.25 $11.25 $4.45 $1.69 $10.98 $9.25 $11.75 $9.25 $4.45 $10.98 $3.99 $8.49 $1.25 $9.25 $4.45 $10.98 $8.75 $8.75 $11.75 $11.25 $8.49 $11.48 $4.45 $1.25 $11.25 $8.99 $1.09 $2.39 $11.25 $2.15 $8.75 $4.45 $8.49 $1.69 $10.98 $1.69 $9.25 $4.45 $11.25 $8.75 $11.25 $11.75 $11.25 $22.50 $22.50 $8.49 $2.39 $2.50 $2.50 $8.75 $8.75 $9.25 $9.25 $11.25 $8.99 $1.09 $8.99 $1.69 $11.75 $1.25 $21.96 $21.96 $8.75 $2.15 $1.25 $8.75 $11.25 $9.25 $11.25 $8.75 $8.75 $11.25 $2.15 $8.99 $1.09 $1.69 $8.75 $2.15 $1.25 $8.49 $1.09 $1.09 $1.69 $11.48 $8.49 $8.49 $4.78 $4.78 $9.25 $1.25 $1.25 $1.25 $11.25 $11.25 $11.75 $4.45 $11.25 $4.45 $8.99 $1.09 $11.25 $2.15 $11.25 $9.25 $11.75 $11.25 $11.25 $9.25 $2.95 $11.25 $4.45 $8.75 $2.95 $2.95 $11.25 $1.50 $10.98 $16.98 $16.98 $18.50 $18.50 $10.98 $3.99 $1.09 $9.25 $9.25 $8.75 $4.45 $17.50 $17.50 $8.75 $4.45 $8.75 $1.25 $4.45 $9.25 $8.75 $8.75 $17.50 $17.50 $4.45 $9.39 $1.25 $2.95 $11.25 $8.75 $8.75 $11.25 $2.15 $8.90 $8.90 $11.25 $11.89 $10.98 $11.25 $4.45 $11.25 $11.25 $8.49 $10.98 $8.49 $3.39 $9.25 $8.75 $2.95 $3.00 $3.00 $9.39 $11.75 $2.95 $1.50 $11.25 $11.75 $8.75 $2.15 $1.50 $8.49 $3.39 $11.75 $1.25 $17.50 $17.50 $11.25 $1.25 $8.75 $2.95 $1.25 $11.25 $11.75 $13.35 $13.35 $13.35 $11.25 $11.75 $11.25 $11.25 $4.45 $11.25 $8.49 $3.39 $9.25 $2.95 $4.78 $4.78 $2.39 $3.99 $8.99 $8.99 $11.25 $11.25 $8.75 $11.25 $2.95 $4.45 $9.25 $8.75 $4.45 $8.49 $8.49 $10.98 $10.98 $3.99 $11.75 $8.75 $11.75 $4.45 $1.50 $1.25 $8.49 $8.49 $8.75 $8.75 $8.75 $9.25 $8.75 $2.95 $1.25 $11.25 $1.50 $11.25 $4.45 $9.25 $8.75 $8.75 $9.25 $8.75 $4.45 $1.50 $8.75 $8.75 $8.49 $1.69 $8.75 $2.15 $9.25 $2.15 $1.50 $11.25 $11.75 $2.15 $6.49 $9.25 $9.25 $11.25 $11.25 $11.75 $11.75 $11.75 $11.25 $8.75 $2.15 $1.25 $11.75 $9.25 $11.25 $8.75 $5.90 $5.90 $8.75 $4.45 $9.25 $9.25 $4.45 $11.25 $4.45 $11.25 $8.75 $2.15 $11.89 $11.25 $8.75 $2.95 $1.50 $8.75 $4.30 $4.30 $8.75 $11.25 $11.75 $11.75 $2.15 $11.25 $8.99 $1.09 $8.49 $8.49 $8.49 $3.39 $8.99 $10.98 $3.99 $11.75 $2.15 $8.75 $4.45 $2.50 $2.50 $11.48 $1.09 $8.49 $8.49 $16.98 $16.98 $3.99 $10.98 $1.09 $8.75 $2.95 $8.75 $8.75 $2.95 $9.25 $11.25 $2.15 $9.25 $4.45 $4.45 $9.25 $11.75 $11.75 $2.15 $9.25 $8.75 $11.25 $6.49 $8.75 $11.25 $2.95 $10.98 $3.99 $1.50 $9.25 $2.15 $8.75 $11.25 $11.89 $4.45 $1.50 $1.25 $8.75 $8.75 $4.45 $11.25 $11.75 $8.49 $1.09 $1.09 $1.69 $8.99 $3.39 $8.99 $1.69 $8.49 $8.99 $3.27 $3.27 $3.27 $8.99 $8.99 $1.09 $10.98 $1.69 $3.99 $8.49 $1.09 $8.75 $8.75 $11.75 $8.75 $9.25 $8.75 $3.39 $8.99 $8.99 $2.39 $9.25 $8.75 $9.25 $9.25 $8.99 $3.99 $2.39 $8.49 $1.09 $8.49 $8.99 $3.39 $11.25 $1.25 $8.99 $3.99 $8.75 $8.90 $8.90 $6.49 $8.75 $9.25 $11.25 $11.25 $11.25 $1.25 $8.75 $9.25 $4.45 $1.25 $8.75 $1.25 $2.15 $17.98 $17.98 $8.99 $8.75 $2.95 $1.25 $11.75 $1.50 $1.50 $8.75 $8.75 $11.08 $8.99 $1.69 $8.99 $1.69 $10.98 $3.99 $3.39 $11.75 $2.15 $11.75 $2.95 $8.75 $8.75 $11.75 $11.25 $11.75 $11.25 $4.45 $11.25 $1.25 $2.18 $2.18 $2.18 $2.18 $2.39 $8.49 $8.99 $2.39 $11.25 $8.75 $11.75 $11.75 $11.25 $4.45 $2.15 $8.19 $10.58 $4.45 $9.25 $1.09 $8.99 $11.25 $1.50 $8.99 $3.99 $4.45 $11.75 $2.15 $11.25 $8.75 $4.45 $8.75 $9.25 $6.45 $6.45 $6.45 $8.75 $11.25 $11.25 $8.75 $11.75 $21.96 $21.96 $8.99 $5.07 $5.07 $5.07 $8.49 $9.25 $11.25 $4.45 $3.39 $8.49 $8.99 $8.49 $17.50 $17.50 $22.96 $22.96 $8.75 $11.25 $11.89 $11.25 $8.49 $1.69 $1.09 $8.99 $8.99 $9.25 $8.75 $9.25 $2.95 $8.49 $3.99 $8.99 $8.49 $7.17 $7.17 $7.17 $8.49 $8.99 $17.50 $17.50 $9.25 $9.25 $11.25 $1.25 $8.99 $1.09 $8.75 $4.45 $11.25 $2.15 $11.75 $11.25 $11.25 $8.75 $8.75 $4.45 $1.25 $11.75 $11.75 $2.50 $2.50 $8.49 $8.99 $2.18 $2.18 $11.25 $4.45 $11.25 $11.75 $8.49 $8.99 $1.69 $1.09 $8.99 $8.99 $11.25 $6.49 $11.25 $8.75 $4.45 $8.99 $1.69 $11.48 $11.75 $2.50 $2.50 $8.49 $1.09 $1.09 $1.69 $8.49 $2.39 $11.75 $1.25 $8.49 $1.69 $8.49 $1.69 $11.75 $4.45 $8.75 $8.75 $4.45 $8.75 $11.25 $11.25 $8.75 $7.98 $7.98 $8.49 $1.09 $8.49 $3.99 $8.49 $3.99 $8.99 $3.99 $11.25 $4.45 $8.49 $2.39 $8.49 $2.39 $3.99 $8.49 $1.25 $11.25 $4.45 $9.25 $4.45 $1.09 $8.99 $3.99 $11.25 $8.90 $8.90 $9.25 $11.25 $8.75 $11.25 $11.25 $11.25 $11.25 $11.25 $8.99 $8.49 $8.75 $8.75 $4.45 $16.98 $16.98 $11.75 $11.25 $9.25 $4.45 $9.25 $2.95 $8.49 $1.69 $3.75 $3.75 $3.75 $4.45 $9.25 $1.50 $11.25 $11.48 $11.25 $2.15 $8.75 $9.39 $8.49 $3.99 $8.19 $2.29 $11.48 $1.69 $11.48 $3.99 $8.49 $1.69 $9.25 $2.95 $8.49 $1.69 $11.25 $4.45 $9.39 $9.25 $8.75 $8.75 $4.45 $11.89 $4.45 $4.45 $8.75 $8.75 $8.75 $2.15 $8.75 $3.75 $3.75 $3.75 $9.25 $11.25 $4.45 $6.49 $16.98 $16.98 $18.50 $18.50 $2.50 $2.50 $2.95 $3.99 $8.49 $8.19 $11.08 $6.49 $11.75 $2.39 $8.99 $1.09 $11.25 $4.45 $11.25 $8.99 $1.69 $21.96 $21.96 $2.18 $2.18 $8.99 $8.99 $2.39 $8.69 $1.69 $8.90 $8.90 $2.50 $2.50 $8.75 $8.99 $1.09 $8.49 $8.49 $8.75 $4.45 $17.50 $17.50 $8.75 $9.25 $8.49 $2.39 $8.75 $4.45 $11.25 $11.25 $11.75 $8.75 $8.49 $8.49 $8.49 $8.99 $8.75 $4.45 $11.48 $8.75 $1.25 $2.15 $9.25 $4.45 $11.75 $2.15 $11.25 $8.99 $2.39 $8.69 $8.69 $11.75 $2.95 $11.75 $1.50 $9.25 $4.45 $1.50 $11.48 $8.99 $2.39 $11.25 $11.89 $2.15 $1.25 $11.75 $4.45 $8.75 $8.75 $11.25 $4.45 $11.25 $2.15 $4.45 $8.49 $1.09 $3.99 $11.25 $11.25 $8.49 $2.39 $8.99 $2.39 $11.25 $2.15 $8.75 $2.95 $1.25 $8.75 $11.25 $17.50 $17.50 $11.75 $11.75 $11.25 $11.25 $4.45 $2.50 $2.50 $8.75 $8.99 $8.99 $1.69 $8.99 $1.69 $11.25 $1.25 $11.08 $8.69 $8.99 $1.09 $11.25 $11.25 $2.95 $1.25 $8.75 $1.25 $8.75 $8.75 $2.15 $1.25 $8.49 $3.99 $8.49 $2.39 $8.49 $7.17 $7.17 $7.17 $8.75 $4.45 $11.48 $8.75 $8.75 $11.48 $8.75 $9.25 $8.49 $3.99 $1.50 $11.25 $11.25 $8.75 $8.75 $4.45 $9.25 $4.45 $8.75 $8.75 $4.30 $4.30 $2.95 $8.75 $4.50 $4.50 $4.50 $9.25 $11.25 $4.45 $11.25 $11.25 $8.75 $9.25 $8.75 $2.15 $1.25 $8.75 $2.15 $1.25 $8.99 $8.49 $8.75 $8.75 $1.25 $11.75 $4.50 $4.50 $4.50 $8.75 $8.75 $3.99 $3.39 $8.49 $2.39 $8.99 $1.50 $11.25 $11.25 $8.75 $8.75 $2.15 $8.75 $2.15 $1.25 $21.96 $21.96 $8.49 $1.69 $26.07 $26.07 $26.07 $11.75 $1.50 $8.99 $8.99 $11.48 $9.25 $9.25 $8.75 $8.75 $8.75 $9.25 $8.75 $8.75 $2.15 $1.25 $11.89 $8.75 $11.75 $4.45 $18.50 $18.50 $9.25 $9.39 $8.49 $2.39 $8.49 $1.69 $1.09 $8.99 $8.49 $2.18 $2.18 $11.25 $1.25 $8.49 $3.39 $8.49 $3.99 $11.25 $8.75 $8.49 $1.69 $16.98 $16.98 $9.25 $9.25 $11.75 $1.25 $11.25 $8.75 $8.49 $8.49 $8.75 $1.25 $1.25 $1.25 $11.25 $12.98 $12.98 $11.75 $11.75 $4.45 $11.25 $11.75 $10.98 $8.49 $8.49 $2.39 $9.25 $11.25 $8.75 $2.95 $1.50 $11.25 $2.95 $9.25 $2.95 $9.25 $8.75 $11.25 $8.75 $8.75 $4.45 $11.25 $1.25 $8.75 $2.95 $2.50 $2.50 $9.25 $9.25 $9.25 $6.49 $17.50 $17.50 $8.49 $1.69 $8.49 $3.99 $8.75 $2.95 $2.95 $8.49 $8.99 $8.99 $1.09 $8.75 $4.45 $1.25 $11.25 $11.75 $11.25 $9.25 $11.25 $1.50 $11.25 $2.15 $10.98 $8.75 $11.75 $2.95 $11.25 $11.25 $9.25 $8.75 $9.25 $8.75 $8.75 $8.75 $1.25 $11.25 $1.50 $2.15 $8.75 $8.75 $11.25 $8.75 $8.75 $11.25 $4.45 $8.49 $8.49 $8.49 $8.49 $8.99 $1.69 $2.39 $1.09 $1.09 $11.25 $2.95 $35.25 $35.25 $35.25 $8.75 $2.95 $1.25 $11.25 $2.15 $9.25 $4.45 $8.75 $8.75 $8.75 $4.45 $1.25 $11.89 $8.75 $2.15 $1.25 $8.49 $1.09 $1.09 $1.69 $8.69 $8.69 $9.25 $2.95 $10.98 $2.39 $22.50 $22.50 $21.96 $21.96 $10.98 $8.49 $1.69 $11.75 $2.95 $8.75 $11.25 $8.75 $9.25 $8.75 $8.19 $10.58 $8.75 $2.95 $1.50 $2.50 $2.50 $9.25 $4.45 $9.25 $11.25 $8.69 $8.69 $3.89 $8.69 $1.69 $4.45 $9.25 $11.25 $4.45 $2.15 $8.19 $8.69 $4.45 $11.25 $1.25 $11.25 $8.75 $11.89 $11.75 $11.75 $9.25 $8.75 $2.15 $1.50 $11.75 $4.45 $3.99 $8.99 $10.98 $2.39 $1.25 $2.95 $8.75 $2.95 $9.25 $9.25 $9.25 $8.75 $2.15 $9.25 $9.25 $3.39 $8.49 $8.49 $2.39 $10.98 $1.09 $1.09 $8.49 $11.25 $1.25 $8.99 $1.09 $8.75 $2.15 $1.50 $3.99 $8.49 $8.75 $2.15 $1.50 $8.49 $10.98 $2.18 $2.18 $8.75 $2.15 $1.50 $4.45 $9.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $10.50 $10.50 $10.50 $10.50 $10.50 $10.50 $10.50 $6.49 $33.75 $33.75 $33.75 $35.00 $35.00 $35.00 $35.00 $27.75 $27.75 $27.75 $3.00 $3.00 $11.25 $11.75 $10.98 $2.39 $2.50 $2.50 $8.75 $4.45 $16.98 $16.98 $8.75 $6.49 $16.98 $16.98 $17.98 $17.98 $16.98 $16.98 $16.98 $16.98 $8.99 $8.99 $8.49 $9.25 $8.75 $2.95 $11.48 $2.39 $8.99 $2.39 $11.25 $4.45 $8.75 $9.25 $6.49 $26.25 $26.25 $26.25 $8.75 $26.25 $26.25 $26.25 $8.75 $8.75 $11.25 $11.25 $2.15 $1.25 $11.75 $8.75 $2.15 $1.50 $11.25 $2.15 $8.99 $2.39 $11.25 $11.25 $2.15 $11.25 $11.25 $8.75 $4.78 $4.78 $21.96 $21.96 $8.49 $2.39 $9.25 $2.95 $16.98 $16.98 $8.19 $3.89 $8.99 $1.09 $8.99 $3.39 $9.25 $4.45 $10.98 $10.98 $17.50 $17.50 $11.25 $1.25 $8.75 $11.75 $4.45 $11.25 $11.25 $8.99 $1.09 $10.98 $8.49 $1.69 $11.25 $9.25 $16.98 $16.98 $8.75 $4.45 $11.25 $6.49 $11.75 $9.25 $9.25 $8.75 $4.45 $2.50 $2.50 $8.75 $11.75 $8.75 $9.25 $11.75 $9.25 $8.75 $9.25 $8.75 $11.25 $11.75 $9.25 $8.75 $11.75 $8.49 $1.09 $1.09 $8.49 $1.09 $1.69 $11.25 $1.25 $8.75 $2.15 $1.50 $8.49 $1.69 $1.25 $8.75 $2.95 $8.49 $3.99 $8.49 $8.49 $8.75 $11.25 $2.15 $1.50 $11.75 $8.99 $1.09 $10.98 $10.98 $11.25 $1.25 $8.75 $4.45 $4.45 $1.25 $11.89 $8.99 $8.99 $11.25 $4.45 $23.50 $23.50 $8.49 $3.99 $9.25 $4.45 $4.45 $9.25 $8.75 $4.45 $8.75 $8.75 $11.75 $6.49 $17.50 $17.50 $4.45 $8.75 $2.95 $1.50 $8.75 $8.75 $8.75 $4.45 $11.25 $11.25 $11.75 $11.25 $2.95 $11.25 $4.45 $3.00 $3.00 $1.25 $2.95 $9.25 $8.99 $2.39 $6.49 $8.75 $8.90 $8.90 $11.48 $1.09 $10.98 $9.25 $9.25 $11.25 $8.75 $11.75 $11.25 $11.25 $1.25 $9.25 $4.45 $9.25 $6.49 $11.75 $11.75 $8.99 $2.39 $8.49 $8.49 $9.25 $9.25 $1.25 $8.75 $2.95 $11.75 $2.15 $8.49 $8.49 $8.69 $16.38 $16.38 $8.19 $3.89 $2.29 $11.75 $8.75 $8.75 $8.75 $4.45 $8.49 $8.49 $9.25 $8.75 $6.49 $2.95 $11.25 $11.25 $2.15 $9.25 $11.75 $21.96 $21.96 $8.49 $3.39 $1.69 $8.49 $8.75 $4.45 $8.49 $3.99 $11.25 $8.75 $11.25 $2.15 $11.75 $4.45 $11.25 $9.25 $8.75 $18.50 $18.50 $1.50 $8.75 $2.15 $11.48 $2.18 $2.18 $3.99 $11.25 $1.50 $8.99 $2.39 $11.75 $1.50 $11.25 $6.49 $4.45 $11.25 $8.49 $3.99 $2.50 $2.50 $8.75 $9.25 $3.99 $8.99 $8.75 $6.49 $13.52 $13.52 $13.52 $13.52 $13.52 $13.52 $13.52 $13.52 $16.98 $16.98 $16.98 $16.98 $17.98 $17.98 $16.98 $16.98 $8.75 $8.75 $11.25 $11.25 $8.49 $1.09 $1.69 $1.25 $9.25 $2.95 $8.69 $8.19 $8.49 $2.39 $8.49 $2.39 $10.98 $8.99 $8.99 $1.69 $8.49 $8.75 $8.75 $11.25 $4.45 $4.45 $17.50 $17.50 $8.75 $4.45 $8.75 $2.15 $1.50 $1.50 $8.99 $1.09 $8.75 $4.45 $8.75 $8.75 $11.25 $4.30 $4.30 $8.49 $1.69 $1.09 $1.09 $8.75 $2.15 $1.50 $8.99 $8.49 $3.99 $8.75 $2.15 $1.25 $9.25 $2.95 $11.25 $4.45 $9.25 $2.95 $3.99 $8.99 $8.49 $8.75 $8.75 $11.25 $9.25 $8.75 $4.45 $11.25 $1.25 $9.25 $11.25 $4.45 $2.95 $10.98 $8.75 $8.75 $18.50 $18.50 $9.25 $9.25 $5.00 $5.00 $5.00 $5.00 $8.75 $2.95 $16.98 $16.98 $11.25 $2.95 $8.75 $2.15 $1.50 $8.49 $2.39 $9.25 $2.15 $1.25 $8.19 $8.69 $8.19 $8.19 $8.75 $2.95 $1.25 $9.25 $2.95 $11.25 $8.75 $11.25 $11.25 $8.99 $1.09 $9.25 $9.25 $4.45 $8.49 $3.99 $2.39 $1.09 $8.99 $8.49 $8.75 $8.75 $11.25 $11.75 $4.45 $2.50 $2.50 $8.75 $8.49 $3.39 $8.75 $9.25 $4.45 $1.25 $11.25 $2.15 $4.45 $2.50 $2.50 $8.99 $3.99 $8.75 $2.15 $11.75 $11.75 $1.25 $8.75 $9.39 $11.25 $9.25 $9.25 $2.95 $9.25 $4.45 $1.25 $9.25 $8.75 $11.75 $1.50 $8.75 $4.45 $8.99 $1.09 $9.25 $2.95 $8.99 $1.69 $8.69 $1.69 $11.25 $4.45 $8.75 $8.75 $4.45 $11.25 $8.75 $2.95 $1.50 $8.19 $8.69 $1.09 $1.69 $8.49 $8.75 $2.95 $1.25 $8.49 $3.99 $10.98 $3.39 $11.25 $11.25 $2.15 $18.50 $18.50 $8.49 $8.49 $11.25 $1.50 $8.49 $2.39 $8.99 $2.39 $11.75 $4.45 $17.50 $17.50 $9.25 $9.25 $8.75 $4.45 $3.75 $3.75 $3.75 $8.75 $4.45 $11.75 $2.95 $1.25 $4.45 $8.75 $1.25 $1.50 $9.25 $11.25 $11.25 $11.25 $11.25 $9.25 $11.25 $4.45 $8.75 $9.25 $8.75 $8.75 $4.45 $8.75 $1.25 $2.15 $8.75 $8.75 $4.30 $4.30 $8.75 $1.25 $2.95 $9.25 $2.95 $4.45 $11.25 $11.25 $9.25 $9.25 $4.50 $4.50 $4.50 $11.75 $1.25 $11.75 $11.75 $1.25 $1.25 $9.25 $4.45 $11.25 $11.75 $17.50 $17.50 $2.15 $1.25 $11.25 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $11.25 $4.45 $4.45 $2.95 $11.25 $2.15 $1.25 $1.50 $8.75 $11.25 $2.95 $11.25 $1.25 $2.15 $11.25 $9.25 $6.49 $1.25 $8.75 $2.15 $8.75 $6.49 $11.25 $1.50 $8.75 $4.45 $8.75 $4.45 $9.25 $9.25 $1.25 $1.25 $8.75 $4.50 $4.50 $4.50 $11.25 $1.25 $1.50 $9.25 $2.15 $11.25 $4.45 $11.25 $4.45 $8.75 $4.45 $9.25 $4.45 $1.25 $11.25 $4.45 $8.75 $4.45 $8.75 $2.15 $8.75 $4.45 $8.75 $11.75 $1.50 $11.25 $4.45 $8.75 $2.15 $1.50 $8.75 $4.45 $8.75 $11.75 $8.75 $8.75 $11.25 $11.25 $1.50 $8.75 $2.15 $11.75 $2.15 $9.25 $2.95 $18.50 $18.50 $1.25 $4.45 $8.50 $8.50 $8.50 $8.50 $11.25 $11.89 $1.25 $9.39 $4.45 $8.75 $2.15 $1.50 $11.75 $8.75 $9.25 $9.25 $4.45 $1.25 $11.25 $9.25 $2.95 $8.75 $8.75 $2.15 $8.75 $11.25 $11.25 $11.25 $11.75 $11.25 $2.15 $11.25 $2.15 $8.99 $8.99 $8.75 $9.25 $9.25 $11.25 $8.75 $4.45 $8.75 $1.25 $4.45 $11.25 $1.25 $8.75 $8.75 $11.25 $8.75 $1.25 $1.25 $1.25 $9.25 $11.75 $2.15 $8.75 $4.45 $8.75 $4.45 $11.25 $9.25 $8.75 $9.25 $4.45 $11.75 $4.45 $1.25 $4.45 $11.75 $9.25 $11.25 $2.15 $23.50 $23.50 $9.25 $2.15 $18.50 $18.50 $8.75 $5.90 $5.90 $11.89 $4.45 $8.75 $4.45 $9.25 $2.95 $11.25 $2.95 $11.25 $11.25 $11.25 $2.95 $11.75 $9.25 $9.25 $2.95 $11.25 $2.15 $9.25 $8.90 $8.90 $8.75 $11.25 $11.25 $11.25 $8.75 $2.15 $1.25 $11.25 $1.25 $8.75 $4.45 $1.25 $11.25 $11.25 $11.75 $1.25 $11.25 $11.25 $11.25 $8.75 $8.75 $18.50 $18.50 $11.75 $1.25 $4.45 $9.25 $6.49 $4.45 $8.75 $11.25 $6.49 $11.75 $8.75 $9.25 $11.25 $2.15 $8.75 $4.45 $11.25 $4.45 $8.75 $9.25 $4.45 $1.25 $1.25 $8.75 $11.25 $11.75 $2.15 $11.75 $4.45 $11.75 $1.25 $11.75 $11.75 $11.25 $4.30 $4.30 $9.39 $9.39 $8.75 $1.25 $9.25 $4.45 $11.25 $1.25 $8.75 $1.25 $2.95 $11.25 $4.45 $8.75 $1.50 $4.45 $4.45 $9.25 $8.75 $2.95 $1.25 $11.25 $2.95 $8.75 $8.75 $8.75 $8.75 $4.30 $4.30 $9.25 $9.39 $4.45 $9.25 $1.25 $22.50 $22.50 $4.45 $2.95 $2.15 $23.50 $23.50 $11.75 $2.15 $1.25 $9.25 $4.45 $11.25 $11.75 $17.50 $17.50 $8.75 $11.75 $11.25 $8.75 $4.45 $11.75 $1.50 $8.75 $8.75 $11.75 $9.25 $11.25 $4.45 $11.75 $9.25 $4.45 $11.25 $8.75 $8.75 $2.15 $1.50 $8.75 $4.45 $9.25 $8.75 $1.50 $1.25 $1.25 $1.25 $8.75 $2.95 $11.25 $11.25 $1.50 $11.75 $11.25 $2.15 $9.25 $8.75 $11.75 $2.95 $1.50 $8.75 $1.50 $1.25 $8.75 $11.75 $11.25 $11.25 $11.75 $11.25 $11.75 $8.75 $17.80 $17.80 $17.80 $17.80 $5.00 $5.00 $5.00 $5.00 $5.00 $5.00 $5.00 $5.00 $8.75 $2.95 $1.25 $11.25 $1.25 $2.15 $11.25 $2.50 $2.50 $9.25 $1.25 $11.25 $2.95 $11.75 $2.15 $11.25 $1.50 $8.99 $1.99 $11.49 $8.75 $4.45 $1.25 $8.75 $4.45 $1.25 $1.50 $11.75 $8.75 $8.75 $11.25 $6.49 $11.75 $8.75 $2.15 $1.25 $6.49 $8.75 $4.45 $8.75 $4.45 $8.75 $11.25 $4.45 $6.49 $9.25 $8.75 $1.25 $4.45 $11.25 $8.75 $1.50 $8.75 $1.50 $1.25 $9.25 $9.39 $4.45 $9.25 $8.75 $4.45 $1.25 $11.25 $11.75 $8.75 $11.25 $9.25 $8.75 $11.25 $2.50 $2.50 $17.50 $17.50 $9.25 $4.45 $11.25 $1.25 $8.75 $4.45 $1.50 $8.75 $1.50 $1.25 $9.39 $8.75 $8.75 $4.45 $11.25 $1.25 $9.25 $4.45 $11.25 $8.75 $3.00 $3.00 $8.75 $2.15 $1.25 $11.25 $11.25 $4.45 $11.25 $11.25 $8.75 $11.75 $11.75 $11.75 $8.75 $4.45 $1.25 $1.50 $8.75 $4.45 $1.25 $9.25 $9.25 $8.75 $4.45 $1.25 $11.75 $11.25 $1.25 $11.75 $11.25 $9.25 $2.15 $1.50 $8.75 $4.45 $11.75 $11.75 $11.25 $8.75 $8.75 ' to numeric\n\n\n\n\n# Solution 2\n\nchipo.groupby(by=['order_id']).sum()['revenue'].mean()\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[9], line 3\n      1 # Solution 2\n----&gt; 3 chipo.groupby(by=['order_id']).sum()['revenue'].mean()\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:6549, in Series.mean(self, axis, skipna, numeric_only, **kwargs)\n   6541 @doc(make_doc(\"mean\", ndim=1))\n   6542 def mean(\n   6543     self,\n   (...)\n   6547     **kwargs,\n   6548 ):\n-&gt; 6549     return NDFrame.mean(self, axis, skipna, numeric_only, **kwargs)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:12420, in NDFrame.mean(self, axis, skipna, numeric_only, **kwargs)\n  12413 def mean(\n  12414     self,\n  12415     axis: Axis | None = 0,\n   (...)\n  12418     **kwargs,\n  12419 ) -&gt; Series | float:\n&gt; 12420     return self._stat_function(\n  12421         \"mean\", nanops.nanmean, axis, skipna, numeric_only, **kwargs\n  12422     )\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:12377, in NDFrame._stat_function(self, name, func, axis, skipna, numeric_only, **kwargs)\n  12373 nv.validate_func(name, (), kwargs)\n  12375 validate_bool_kwarg(skipna, \"skipna\", none_allowed=False)\n&gt; 12377 return self._reduce(\n  12378     func, name=name, axis=axis, skipna=skipna, numeric_only=numeric_only\n  12379 )\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:6457, in Series._reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\n   6452     # GH#47500 - change to TypeError to match other methods\n   6453     raise TypeError(\n   6454         f\"Series.{name} does not allow {kwd_name}={numeric_only} \"\n   6455         \"with non-numeric dtypes.\"\n   6456     )\n-&gt; 6457 return op(delegate, skipna=skipna, **kwds)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:147, in bottleneck_switch.__call__.&lt;locals&gt;.f(values, axis, skipna, **kwds)\n    145         result = alt(values, axis=axis, skipna=skipna, **kwds)\n    146 else:\n--&gt; 147     result = alt(values, axis=axis, skipna=skipna, **kwds)\n    149 return result\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:404, in _datetimelike_compat.&lt;locals&gt;.new_func(values, axis, skipna, mask, **kwargs)\n    401 if datetimelike and mask is None:\n    402     mask = isna(values)\n--&gt; 404 result = func(values, axis=axis, skipna=skipna, mask=mask, **kwargs)\n    406 if datetimelike:\n    407     result = _wrap_results(result, orig_values.dtype, fill_value=iNaT)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:720, in nanmean(values, axis, skipna, mask)\n    718 count = _get_counts(values.shape, mask, axis, dtype=dtype_count)\n    719 the_sum = values.sum(axis, dtype=dtype_sum)\n--&gt; 720 the_sum = _ensure_numeric(the_sum)\n    722 if axis is not None and getattr(the_sum, \"ndim\", False):\n    723     count = cast(np.ndarray, count)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:1701, in _ensure_numeric(x)\n   1698 elif not (is_float(x) or is_integer(x) or is_complex(x)):\n   1699     if isinstance(x, str):\n   1700         # GH#44008, GH#36703 avoid casting e.g. strings to numeric\n-&gt; 1701         raise TypeError(f\"Could not convert string '{x}' to numeric\")\n   1702     try:\n   1703         x = float(x)\n\nTypeError: Could not convert string '$2.39 $3.39 $3.39 $2.39 $16.98 $16.98 $10.98 $1.69 $11.75 $9.25 $9.25 $4.45 $8.75 $8.75 $11.25 $4.45 $2.39 $8.49 $8.49 $2.18 $2.18 $8.75 $4.45 $8.99 $3.39 $10.98 $3.39 $2.39 $8.49 $8.99 $1.09 $8.49 $2.39 $8.99 $1.69 $8.99 $1.09 $8.75 $8.75 $4.45 $2.95 $11.75 $2.15 $4.45 $11.25 $11.75 $8.75 $10.98 $8.99 $3.39 $8.99 $3.99 $8.99 $2.18 $2.18 $10.98 $1.09 $8.99 $2.39 $9.25 $11.25 $11.75 $2.15 $4.45 $9.25 $11.25 $8.75 $8.99 $8.99 $3.39 $8.99 $10.98 $8.99 $1.69 $8.99 $3.99 $8.75 $4.45 $8.75 $8.75 $2.15 $8.75 $11.25 $2.15 $9.25 $8.75 $8.75 $9.25 $8.49 $8.99 $1.09 $9.25 $2.95 $11.75 $11.75 $9.25 $11.75 $4.45 $9.25 $4.45 $11.75 $8.75 $8.75 $4.45 $8.99 $8.99 $3.99 $8.49 $3.39 $8.99 $1.09 $9.25 $4.45 $8.75 $2.95 $4.45 $2.39 $8.49 $8.99 $8.49 $1.09 $8.99 $3.99 $8.75 $9.25 $4.45 $11.25 $4.45 $8.99 $1.09 $9.25 $2.95 $4.45 $11.75 $4.45 $8.49 $2.39 $10.98 $22.50 $22.50 $11.75 $4.45 $11.25 $4.45 $11.25 $4.45 $11.25 $11.25 $11.75 $9.25 $4.45 $11.48 $17.98 $17.98 $1.69 $17.50 $17.50 $4.45 $8.49 $2.39 $17.50 $17.50 $4.45 $4.45 $11.25 $11.75 $10.98 $8.49 $10.98 $2.18 $2.18 $11.48 $8.49 $2.39 $4.45 $11.25 $11.75 $8.75 $8.49 $2.18 $2.18 $8.49 $3.39 $8.49 $8.99 $10.98 $11.48 $8.49 $1.09 $1.09 $9.25 $8.75 $2.95 $9.25 $4.45 $11.25 $11.48 $8.49 $8.49 $8.99 $2.39 $11.25 $8.75 $2.95 $1.09 $8.99 $8.49 $2.39 $10.98 $1.09 $3.99 $11.25 $8.75 $8.49 $3.39 $8.75 $9.25 $2.15 $11.25 $11.25 $11.25 $4.45 $22.50 $22.50 $4.45 $11.75 $8.75 $17.50 $17.50 $8.75 $9.25 $8.75 $2.15 $9.25 $4.30 $4.30 $8.75 $11.25 $2.15 $8.99 $1.09 $8.99 $3.99 $8.75 $2.95 $2.95 $11.75 $5.90 $5.90 $9.25 $9.25 $11.75 $9.25 $2.95 $17.50 $17.50 $8.75 $9.25 $10.98 $8.99 $1.09 $1.09 $1.09 $8.99 $10.98 $1.09 $8.75 $8.75 $9.25 $9.25 $8.75 $8.75 $8.99 $8.99 $8.99 $1.09 $11.75 $1.25 $8.99 $2.39 $9.25 $2.95 $8.99 $3.99 $8.49 $2.39 $8.49 $8.49 $8.49 $1.69 $8.49 $3.99 $8.99 $1.69 $1.09 $23.78 $23.78 $17.50 $17.50 $2.15 $8.75 $9.25 $9.25 $8.75 $4.45 $8.75 $11.25 $11.25 $1.25 $9.25 $4.45 $11.25 $11.75 $11.75 $6.49 $8.99 $2.39 $8.49 $2.39 $11.25 $8.75 $2.15 $8.99 $1.69 $8.75 $11.25 $2.15 $4.45 $8.75 $8.49 $8.99 $17.50 $17.50 $8.49 $1.09 $1.09 $8.75 $1.25 $2.15 $11.08 $8.49 $8.49 $8.99 $2.39 $8.75 $2.15 $1.50 $11.25 $2.15 $8.49 $8.49 $11.75 $9.25 $11.75 $1.25 $11.25 $8.75 $4.45 $6.49 $9.25 $2.95 $11.25 $4.45 $1.25 $1.25 $8.49 $2.39 $2.18 $2.18 $8.49 $2.18 $2.18 $22.16 $22.16 $17.50 $17.50 $8.75 $2.95 $6.49 $8.99 $3.39 $3.39 $8.99 $8.49 $11.25 $2.15 $11.25 $2.95 $11.25 $1.25 $8.99 $1.09 $8.75 $8.75 $9.25 $2.95 $11.75 $1.50 $8.99 $1.09 $11.25 $1.25 $1.25 $11.25 $11.75 $2.15 $8.99 $1.69 $11.75 $6.49 $8.75 $9.25 $11.25 $4.45 $1.25 $11.25 $4.45 $8.49 $8.99 $8.49 $8.99 $11.25 $1.25 $11.75 $1.25 $11.75 $9.25 $4.45 $11.25 $2.15 $32.94 $32.94 $32.94 $1.25 $11.25 $11.48 $1.69 $1.09 $17.50 $17.50 $4.45 $6.49 $9.25 $8.75 $9.25 $9.25 $8.75 $8.75 $2.15 $2.95 $17.50 $17.50 $10.98 $11.48 $11.48 $3.39 $8.99 $1.69 $8.99 $1.09 $10.98 $3.39 $8.99 $1.09 $9.25 $8.75 $11.25 $4.45 $2.95 $9.25 $22.20 $22.20 $22.20 $8.49 $8.99 $8.75 $8.75 $11.75 $8.75 $11.25 $9.25 $11.25 $11.25 $8.75 $11.25 $2.95 $1.25 $8.49 $1.69 $11.75 $11.25 $8.75 $8.75 $4.45 $8.49 $3.99 $8.49 $3.99 $11.48 $1.69 $1.09 $11.25 $1.50 $10.58 $1.69 $9.25 $11.25 $8.75 $9.25 $11.25 $11.25 $8.75 $11.75 $8.75 $8.75 $8.75 $2.15 $11.25 $11.75 $2.50 $2.50 $4.45 $9.25 $4.45 $11.25 $8.49 $3.99 $9.25 $9.25 $11.25 $9.25 $11.75 $11.25 $1.25 $23.50 $23.50 $1.25 $8.99 $8.49 $7.40 $7.40 $8.75 $1.25 $4.45 $8.75 $2.15 $8.75 $4.45 $7.40 $7.40 $7.40 $8.99 $3.99 $8.99 $1.69 $8.75 $8.75 $8.75 $8.75 $11.25 $11.25 $2.95 $8.75 $18.50 $18.50 $8.49 $3.99 $2.95 $9.25 $9.25 $3.00 $3.00 $1.25 $8.75 $9.25 $4.45 $8.75 $11.25 $4.45 $10.98 $22.16 $22.16 $4.45 $8.75 $9.25 $6.49 $9.25 $11.25 $8.75 $9.25 $2.15 $9.25 $4.45 $9.25 $2.95 $9.25 $8.75 $9.25 $1.25 $1.25 $8.75 $8.75 $9.25 $4.45 $11.75 $11.75 $11.75 $9.25 $9.25 $16.98 $16.98 $2.39 $3.39 $3.39 $9.25 $11.75 $11.25 $2.15 $8.75 $9.25 $4.45 $10.98 $11.25 $9.25 $22.50 $22.50 $9.25 $2.95 $1.50 $11.48 $8.49 $1.69 $8.49 $8.49 $8.49 $6.78 $6.78 $11.75 $4.45 $8.75 $4.45 $11.89 $9.39 $8.75 $2.95 $1.25 $9.25 $8.75 $23.78 $23.78 $8.75 $9.25 $2.15 $2.15 $1.25 $8.49 $3.99 $10.98 $1.09 $8.75 $4.45 $8.75 $11.75 $2.95 $4.45 $9.25 $8.75 $8.49 $3.99 $22.50 $22.50 $11.25 $1.25 $8.75 $8.75 $18.50 $18.50 $6.49 $8.75 $8.75 $4.45 $8.49 $3.99 $8.99 $1.09 $8.49 $2.39 $11.48 $1.69 $2.50 $2.50 $9.25 $1.50 $17.50 $17.50 $2.95 $8.75 $4.45 $11.75 $8.75 $8.49 $1.69 $8.49 $3.99 $8.99 $8.99 $3.99 $8.99 $11.25 $4.45 $1.25 $3.99 $10.98 $7.40 $3.00 $7.40 $4.00 $8.49 $3.99 $9.25 $4.45 $11.25 $1.25 $11.75 $1.25 $11.25 $2.15 $11.25 $4.45 $3.75 $3.75 $3.75 $11.75 $8.99 $2.39 $8.75 $4.45 $1.25 $8.99 $8.49 $2.18 $2.18 $8.49 $2.18 $2.18 $1.09 $8.75 $2.95 $1.25 $1.50 $11.25 $9.25 $2.95 $1.25 $8.49 $3.99 $11.48 $3.99 $8.49 $11.25 $1.25 $8.99 $1.69 $11.25 $1.25 $6.49 $8.75 $9.25 $8.75 $2.95 $8.75 $11.75 $8.69 $8.69 $2.29 $3.99 $8.49 $8.75 $8.75 $1.25 $11.75 $11.25 $11.25 $11.25 $1.25 $9.25 $11.75 $6.49 $3.99 $8.49 $11.25 $2.15 $11.25 $11.89 $8.99 $1.69 $8.99 $8.99 $3.99 $8.99 $9.25 $9.25 $2.15 $7.40 $7.40 $8.75 $8.75 $9.25 $4.45 $11.25 $1.25 $11.75 $11.25 $1.25 $3.99 $8.49 $8.49 $8.49 $8.99 $8.75 $2.15 $1.25 $8.49 $1.09 $1.09 $8.75 $2.95 $1.25 $9.25 $1.25 $2.15 $11.25 $1.25 $4.45 $8.75 $2.50 $2.50 $8.90 $8.90 $8.75 $8.75 $8.75 $11.25 $11.25 $10.98 $3.99 $10.98 $3.99 $1.69 $8.99 $9.25 $8.75 $8.99 $1.09 $9.25 $2.95 $8.75 $9.25 $3.99 $8.49 $8.75 $8.75 $22.50 $22.50 $10.98 $3.27 $3.27 $3.27 $3.99 $8.99 $1.09 $11.08 $8.75 $4.45 $11.08 $3.99 $8.49 $4.30 $4.30 $9.25 $8.75 $11.25 $11.25 $9.25 $8.49 $8.99 $8.49 $8.75 $2.95 $4.45 $9.25 $2.95 $9.25 $8.75 $11.25 $4.45 $16.98 $16.98 $8.49 $2.39 $11.25 $3.75 $3.75 $3.75 $9.25 $4.45 $9.25 $9.25 $4.45 $8.75 $9.25 $8.75 $9.25 $9.25 $9.25 $11.48 $8.99 $22.50 $22.50 $11.75 $11.25 $1.25 $8.75 $2.15 $1.25 $11.25 $8.75 $1.25 $11.25 $1.50 $11.25 $11.25 $9.25 $6.49 $8.90 $8.90 $8.75 $4.45 $11.25 $1.25 $17.50 $17.50 $9.25 $8.75 $11.75 $3.00 $3.00 $8.49 $8.49 $10.98 $8.99 $3.99 $8.75 $4.45 $8.99 $1.69 $11.75 $8.75 $11.25 $4.45 $11.75 $1.25 $11.75 $2.95 $8.99 $8.99 $2.18 $2.18 $17.98 $17.98 $8.99 $8.49 $1.69 $11.75 $11.25 $2.95 $3.75 $3.75 $3.75 $9.25 $11.75 $8.75 $2.15 $1.50 $8.49 $8.49 $3.39 $8.69 $3.89 $8.75 $4.45 $8.75 $11.25 $2.15 $8.75 $8.49 $1.69 $8.49 $8.49 $1.25 $8.75 $11.75 $11.75 $8.99 $1.09 $8.75 $4.45 $8.75 $2.95 $8.75 $2.15 $3.99 $8.49 $8.99 $3.99 $8.49 $1.69 $1.09 $8.99 $1.09 $9.25 $8.75 $8.99 $2.39 $1.25 $1.25 $11.25 $11.25 $9.25 $9.25 $11.25 $1.50 $3.99 $8.49 $11.25 $9.25 $11.25 $17.50 $17.50 $8.75 $8.90 $8.90 $8.75 $8.75 $8.99 $2.39 $11.25 $9.25 $2.15 $11.25 $1.25 $11.75 $1.25 $11.25 $11.75 $1.25 $11.25 $11.25 $8.49 $10.98 $8.75 $1.25 $8.75 $8.49 $8.49 $1.50 $1.50 $8.75 $4.45 $11.25 $1.25 $11.75 $8.49 $2.39 $9.25 $4.45 $9.25 $8.75 $8.99 $1.69 $17.50 $17.50 $2.39 $8.99 $8.99 $11.25 $4.45 $8.75 $4.45 $9.25 $6.49 $10.98 $8.49 $8.49 $1.09 $1.69 $9.25 $4.45 $8.75 $1.25 $2.95 $3.99 $8.49 $11.75 $11.75 $2.15 $11.48 $8.75 $2.15 $1.25 $11.25 $2.15 $1.25 $8.75 $8.75 $6.49 $1.69 $8.99 $8.75 $11.75 $10.98 $1.09 $8.49 $3.39 $8.75 $2.15 $1.25 $11.48 $10.98 $10.98 $8.49 $2.95 $9.25 $9.25 $11.75 $4.45 $11.48 $11.25 $8.75 $4.45 $1.69 $8.99 $8.75 $4.45 $1.50 $11.75 $2.15 $8.99 $2.39 $8.75 $2.95 $1.25 $8.75 $2.15 $1.25 $2.18 $2.18 $2.18 $2.18 $11.48 $8.75 $2.95 $11.75 $11.75 $1.25 $10.58 $8.99 $2.39 $11.75 $4.45 $11.25 $11.25 $17.50 $17.50 $8.75 $8.75 $8.75 $22.50 $22.50 $9.25 $8.75 $4.45 $11.75 $1.25 $11.25 $11.25 $2.95 $8.99 $1.69 $11.25 $4.45 $8.75 $6.49 $8.75 $4.45 $9.25 $4.45 $11.75 $11.75 $4.45 $11.89 $11.75 $11.25 $2.95 $1.50 $4.45 $8.75 $8.99 $1.09 $8.99 $1.09 $3.99 $11.48 $8.49 $9.25 $4.45 $11.48 $9.25 $2.95 $9.25 $8.49 $8.99 $8.99 $8.49 $8.75 $2.95 $4.45 $11.89 $10.58 $8.19 $1.69 $8.75 $2.15 $1.25 $17.50 $17.50 $6.49 $9.25 $2.15 $8.75 $4.45 $8.75 $1.25 $11.48 $11.48 $8.99 $2.18 $2.18 $8.49 $8.99 $2.39 $2.39 $2.18 $2.18 $8.75 $4.45 $11.25 $9.25 $9.25 $11.25 $11.25 $4.45 $2.95 $11.75 $8.49 $8.49 $8.99 $1.69 $9.25 $11.25 $11.75 $9.25 $8.75 $11.75 $8.75 $8.75 $11.25 $11.25 $10.98 $11.25 $4.45 $10.98 $8.49 $8.99 $3.39 $3.99 $8.99 $1.09 $1.09 $2.39 $17.50 $17.50 $4.45 $11.25 $11.25 $4.45 $9.25 $4.45 $8.75 $2.15 $1.25 $11.89 $2.95 $11.75 $1.25 $11.25 $4.45 $11.48 $11.48 $2.95 $9.25 $8.75 $9.25 $2.95 $11.25 $1.25 $11.75 $1.25 $8.99 $2.39 $1.25 $11.25 $1.25 $11.25 $8.49 $3.99 $35.00 $35.00 $35.00 $35.00 $27.75 $27.75 $27.75 $8.75 $11.80 $11.80 $11.80 $11.80 $8.90 $8.90 $5.90 $5.90 $6.49 $10.98 $17.98 $17.98 $2.39 $9.25 $8.75 $2.15 $8.75 $4.45 $8.49 $1.69 $8.19 $8.69 $10.98 $3.99 $11.48 $11.48 $4.45 $8.75 $6.49 $8.75 $8.75 $9.25 $1.25 $4.45 $8.49 $1.69 $9.25 $4.45 $8.99 $1.09 $11.25 $2.95 $11.08 $11.08 $3.89 $10.98 $11.25 $8.75 $11.25 $9.25 $4.30 $4.30 $8.75 $8.49 $3.99 $1.69 $8.99 $8.49 $1.69 $11.75 $11.25 $11.89 $9.25 $2.95 $9.25 $2.95 $8.75 $4.45 $4.45 $8.75 $10.98 $11.48 $8.49 $9.25 $4.45 $11.75 $11.89 $8.99 $8.49 $8.75 $9.25 $8.75 $8.75 $11.75 $11.75 $4.45 $11.25 $11.75 $2.50 $2.50 $8.99 $1.69 $11.75 $2.15 $1.25 $9.25 $8.75 $8.90 $8.90 $9.25 $2.95 $8.75 $11.25 $8.90 $8.90 $11.25 $11.75 $11.48 $1.69 $3.39 $9.25 $2.95 $8.99 $1.69 $8.49 $10.98 $11.25 $2.95 $8.99 $1.69 $8.75 $2.15 $1.25 $8.75 $2.95 $9.25 $2.50 $2.50 $11.25 $1.25 $11.75 $2.50 $2.50 $11.25 $1.50 $8.75 $1.25 $2.95 $11.48 $11.48 $8.75 $8.75 $2.15 $11.75 $1.25 $9.25 $9.25 $6.49 $11.75 $8.49 $8.49 $1.09 $10.98 $8.75 $1.25 $2.15 $11.25 $1.50 $11.25 $11.25 $8.49 $8.49 $8.75 $1.50 $1.25 $1.50 $8.75 $2.50 $2.50 $2.15 $7.40 $7.40 $4.00 $9.25 $9.39 $9.25 $9.25 $9.39 $11.25 $8.90 $8.90 $11.25 $6.00 $6.00 $6.00 $6.00 $11.25 $11.25 $11.25 $22.50 $22.50 $11.48 $1.09 $8.49 $8.49 $17.50 $17.50 $11.25 $1.50 $9.25 $8.75 $3.99 $8.49 $8.75 $8.75 $8.75 $8.75 $8.75 $11.75 $1.50 $11.25 $11.25 $2.95 $8.99 $10.98 $9.25 $8.75 $4.45 $8.49 $1.09 $2.39 $8.75 $8.75 $11.48 $8.99 $8.49 $8.49 $2.39 $10.98 $8.49 $3.99 $11.75 $4.45 $8.75 $2.15 $1.25 $10.98 $8.99 $11.25 $1.50 $8.75 $2.15 $1.25 $8.75 $9.25 $8.75 $11.25 $1.50 $8.75 $1.25 $4.45 $10.98 $8.75 $2.95 $1.25 $8.75 $2.95 $1.25 $8.49 $8.49 $2.39 $11.25 $1.25 $8.75 $8.75 $9.25 $8.75 $11.89 $1.25 $8.75 $2.15 $1.25 $8.99 $1.09 $8.75 $4.45 $26.25 $26.25 $26.25 $8.75 $4.45 $11.75 $2.95 $8.75 $8.75 $11.75 $8.75 $11.25 $11.25 $11.25 $4.45 $1.25 $8.49 $8.49 $8.49 $8.99 $8.99 $2.39 $2.39 $3.99 $8.75 $4.45 $2.15 $9.25 $1.25 $11.25 $11.75 $8.75 $4.45 $11.25 $2.15 $8.75 $4.45 $8.75 $8.75 $1.25 $11.25 $2.15 $8.75 $5.90 $5.90 $11.75 $1.25 $9.25 $3.75 $3.75 $3.75 $8.75 $1.25 $4.45 $11.75 $4.45 $8.75 $23.50 $23.50 $8.75 $2.95 $8.75 $8.75 $11.89 $4.45 $2.95 $1.25 $8.75 $4.45 $2.95 $1.25 $8.75 $2.15 $1.25 $11.75 $2.95 $8.99 $3.39 $9.25 $9.25 $17.50 $17.50 $2.95 $11.89 $1.50 $11.25 $2.95 $9.25 $11.25 $11.25 $2.95 $8.75 $9.25 $4.30 $4.30 $8.75 $8.75 $11.25 $8.75 $4.30 $4.30 $8.75 $1.25 $2.15 $8.49 $8.49 $3.39 $3.39 $10.98 $10.98 $2.39 $11.25 $11.75 $11.75 $1.25 $5.90 $5.90 $8.75 $11.25 $9.25 $4.45 $1.50 $3.39 $8.99 $2.39 $11.25 $2.15 $11.25 $11.75 $11.75 $4.45 $11.75 $4.45 $9.25 $8.75 $8.49 $8.99 $8.49 $8.99 $11.75 $8.75 $8.49 $3.99 $3.89 $11.08 $8.49 $8.99 $8.49 $8.49 $8.49 $11.25 $2.15 $17.50 $17.50 $8.75 $2.95 $8.49 $8.49 $10.98 $1.09 $11.25 $2.15 $2.95 $1.25 $8.75 $9.25 $9.25 $9.25 $2.95 $8.75 $2.15 $1.25 $8.99 $3.99 $11.75 $2.15 $8.99 $3.39 $9.25 $8.75 $11.25 $11.25 $4.45 $8.75 $2.15 $1.25 $11.75 $4.45 $9.25 $2.95 $8.49 $8.49 $11.25 $8.75 $4.45 $11.25 $11.25 $11.25 $11.25 $4.45 $8.49 $1.69 $8.49 $3.39 $8.75 $11.25 $9.25 $8.75 $11.25 $11.25 $11.75 $11.25 $11.75 $11.25 $11.75 $21.96 $21.96 $10.98 $1.69 $11.48 $8.99 $8.49 $1.69 $9.25 $2.15 $1.50 $11.25 $1.50 $8.75 $8.75 $2.95 $8.49 $1.69 $8.75 $2.95 $1.25 $11.25 $2.15 $11.08 $8.49 $8.49 $8.49 $11.75 $1.25 $11.75 $8.75 $8.75 $8.75 $4.45 $11.25 $1.50 $23.50 $23.50 $11.75 $6.49 $8.75 $4.45 $6.49 $8.75 $2.50 $2.50 $2.15 $8.49 $2.39 $8.75 $11.75 $4.45 $8.99 $10.98 $9.25 $2.95 $9.25 $9.25 $11.75 $8.75 $8.75 $8.75 $10.98 $11.25 $9.25 $8.75 $8.75 $2.15 $11.25 $2.15 $4.45 $11.75 $8.49 $2.39 $9.25 $1.25 $1.25 $1.25 $1.25 $8.75 $2.15 $8.49 $1.69 $11.25 $1.50 $8.75 $8.75 $8.49 $3.99 $8.99 $1.09 $11.25 $1.25 $8.49 $2.39 $8.49 $8.75 $9.25 $11.25 $4.45 $11.25 $11.89 $8.99 $8.49 $8.75 $4.45 $8.75 $11.75 $11.75 $8.90 $8.90 $9.39 $2.95 $8.49 $3.99 $8.75 $2.15 $1.25 $21.96 $21.96 $8.49 $1.69 $8.75 $4.45 $8.49 $8.99 $8.49 $3.99 $8.75 $8.75 $2.95 $8.75 $17.50 $17.50 $9.25 $2.95 $8.75 $6.49 $4.30 $4.30 $8.75 $8.75 $2.15 $1.50 $8.49 $8.49 $2.39 $9.25 $4.45 $6.49 $11.75 $4.45 $10.98 $1.69 $9.39 $9.25 $9.25 $2.95 $8.75 $2.15 $1.25 $11.25 $9.25 $8.75 $11.25 $8.75 $11.25 $2.50 $2.50 $2.50 $2.50 $6.00 $6.00 $6.00 $6.00 $8.90 $8.90 $5.90 $5.90 $11.25 $11.25 $8.49 $10.98 $8.75 $2.15 $1.50 $9.25 $1.25 $1.50 $2.15 $1.25 $8.75 $2.95 $8.49 $3.99 $11.25 $4.30 $4.30 $11.75 $2.15 $18.50 $18.50 $8.49 $2.39 $8.75 $4.45 $11.75 $8.99 $3.99 $9.25 $9.25 $1.50 $8.75 $2.95 $6.49 $11.75 $8.49 $8.99 $8.75 $4.45 $6.49 $22.50 $22.50 $9.25 $2.95 $8.49 $1.69 $10.98 $8.75 $4.45 $11.25 $2.95 $8.99 $8.49 $2.39 $11.75 $6.49 $11.25 $11.75 $2.95 $8.99 $1.69 $8.99 $2.18 $2.18 $1.09 $8.99 $8.99 $1.09 $8.99 $8.99 $8.49 $10.98 $1.09 $11.75 $9.25 $11.25 $11.25 $2.15 $11.25 $8.75 $4.45 $2.95 $11.75 $1.50 $8.99 $10.98 $2.39 $8.75 $2.15 $9.25 $1.50 $8.75 $2.15 $3.99 $8.99 $6.49 $8.75 $8.90 $8.90 $8.99 $3.99 $17.50 $17.50 $11.25 $1.25 $10.98 $9.25 $4.45 $1.25 $3.00 $3.00 $11.25 $4.45 $4.45 $2.95 $9.25 $11.25 $2.15 $11.25 $11.25 $4.45 $2.95 $9.25 $11.25 $1.25 $8.75 $2.95 $1.25 $8.75 $4.45 $11.48 $11.48 $8.49 $2.39 $11.25 $11.75 $2.15 $1.50 $2.15 $8.75 $11.25 $8.90 $8.90 $11.25 $11.25 $1.25 $4.45 $9.25 $9.25 $8.75 $9.25 $8.75 $8.75 $9.25 $8.75 $11.75 $11.75 $8.75 $8.75 $8.90 $8.90 $2.95 $10.98 $8.49 $8.49 $10.98 $8.99 $8.99 $11.75 $17.50 $17.50 $11.75 $3.99 $8.49 $10.98 $1.69 $17.50 $17.50 $8.99 $2.39 $8.99 $2.39 $1.25 $8.75 $2.95 $11.75 $11.25 $17.50 $17.50 $8.49 $8.49 $2.39 $11.25 $1.50 $8.75 $3.00 $3.00 $1.25 $8.75 $4.45 $11.75 $11.75 $4.45 $21.96 $21.96 $8.75 $4.45 $8.75 $11.25 $9.25 $8.99 $2.39 $9.25 $8.75 $10.98 $8.49 $3.99 $3.39 $11.75 $1.50 $4.45 $9.25 $8.75 $1.25 $11.75 $8.75 $1.50 $8.75 $8.75 $2.15 $1.50 $8.75 $2.95 $8.75 $8.75 $17.50 $17.50 $8.75 $6.49 $4.45 $11.25 $11.25 $4.30 $4.30 $8.75 $11.25 $4.45 $8.99 $2.39 $9.25 $9.25 $9.25 $4.45 $11.75 $11.25 $2.95 $2.15 $11.25 $11.25 $8.75 $2.15 $1.50 $9.25 $4.45 $10.98 $8.99 $2.18 $2.18 $8.75 $4.45 $1.25 $8.99 $2.39 $4.45 $8.75 $10.98 $11.75 $1.50 $10.98 $8.99 $8.49 $3.99 $8.99 $8.49 $3.99 $8.49 $8.49 $8.99 $11.25 $11.25 $10.98 $10.98 $10.98 $2.39 $3.39 $8.75 $1.25 $2.95 $11.75 $1.50 $10.98 $1.69 $4.45 $8.75 $8.75 $8.75 $8.75 $4.45 $9.25 $8.75 $11.25 $8.75 $3.99 $8.99 $8.49 $11.25 $11.25 $8.75 $4.45 $8.75 $4.45 $1.25 $8.75 $8.75 $1.50 $2.15 $11.75 $11.75 $11.75 $11.75 $11.75 $1.50 $8.75 $9.25 $1.25 $8.75 $2.15 $8.99 $1.09 $4.45 $11.25 $11.75 $2.15 $8.75 $8.75 $1.25 $9.25 $2.15 $11.75 $11.25 $8.75 $11.25 $4.45 $8.49 $1.69 $8.75 $8.75 $8.99 $8.49 $9.25 $11.25 $2.95 $4.45 $11.75 $6.49 $11.48 $8.99 $4.36 $4.36 $4.36 $4.36 $11.48 $8.99 $8.49 $11.48 $8.75 $2.15 $1.50 $8.99 $1.69 $11.25 $1.25 $9.25 $9.25 $8.75 $9.25 $8.90 $8.90 $2.15 $9.25 $10.98 $8.49 $8.75 $9.25 $4.30 $4.30 $9.25 $8.75 $8.75 $2.15 $1.25 $8.75 $1.25 $8.75 $5.90 $5.90 $9.25 $8.75 $9.25 $4.45 $9.25 $11.75 $2.50 $2.50 $9.25 $2.15 $9.25 $1.50 $1.25 $11.25 $2.95 $8.75 $8.75 $8.75 $10.98 $8.75 $8.75 $8.75 $2.15 $1.25 $10.98 $8.75 $2.15 $1.50 $8.75 $2.95 $1.25 $9.25 $9.25 $8.49 $2.39 $8.75 $4.45 $9.25 $8.75 $8.75 $8.75 $9.25 $8.75 $9.25 $8.75 $8.75 $8.75 $8.75 $9.25 $8.75 $9.25 $8.75 $9.25 $8.75 $8.75 $8.75 $9.25 $8.75 $9.25 $8.75 $8.99 $8.99 $8.75 $2.95 $1.25 $11.75 $1.50 $11.25 $11.25 $8.75 $1.50 $2.15 $16.98 $16.98 $11.75 $1.50 $8.75 $4.30 $4.30 $1.50 $8.75 $2.95 $1.25 $1.25 $9.25 $4.45 $11.25 $8.75 $4.45 $8.75 $2.15 $1.25 $10.98 $1.69 $8.75 $1.25 $8.75 $1.25 $11.25 $8.75 $8.75 $8.49 $1.69 $9.25 $11.75 $8.49 $2.39 $9.25 $2.95 $6.49 $8.75 $8.75 $9.25 $8.75 $6.78 $6.78 $17.98 $17.98 $3.39 $11.75 $11.25 $8.75 $4.45 $11.75 $9.25 $8.75 $6.49 $8.99 $2.39 $8.75 $11.25 $11.75 $4.45 $8.75 $2.15 $9.25 $9.25 $9.25 $11.89 $11.75 $11.25 $9.25 $9.25 $8.75 $8.75 $8.49 $1.69 $1.09 $11.25 $1.50 $11.25 $11.25 $11.75 $1.50 $8.49 $8.99 $22.50 $22.50 $8.75 $4.30 $4.30 $8.75 $11.25 $2.15 $11.25 $2.95 $4.45 $11.25 $8.49 $3.39 $2.39 $11.75 $2.15 $11.75 $8.99 $2.39 $8.75 $11.75 $11.89 $1.25 $7.50 $7.50 $7.50 $7.50 $7.50 $11.89 $1.09 $8.49 $2.39 $8.75 $8.75 $8.75 $8.75 $9.25 $11.25 $8.75 $8.90 $8.90 $9.25 $8.75 $8.75 $11.75 $3.00 $3.00 $1.50 $11.25 $11.75 $8.99 $10.98 $4.45 $8.75 $2.15 $9.25 $11.25 $4.45 $1.69 $10.98 $9.25 $11.75 $9.25 $4.45 $10.98 $3.99 $8.49 $1.25 $9.25 $4.45 $10.98 $8.75 $8.75 $11.75 $11.25 $8.49 $11.48 $4.45 $1.25 $11.25 $8.99 $1.09 $2.39 $11.25 $2.15 $8.75 $4.45 $8.49 $1.69 $10.98 $1.69 $9.25 $4.45 $11.25 $8.75 $11.25 $11.75 $11.25 $22.50 $22.50 $8.49 $2.39 $2.50 $2.50 $8.75 $8.75 $9.25 $9.25 $11.25 $8.99 $1.09 $8.99 $1.69 $11.75 $1.25 $21.96 $21.96 $8.75 $2.15 $1.25 $8.75 $11.25 $9.25 $11.25 $8.75 $8.75 $11.25 $2.15 $8.99 $1.09 $1.69 $8.75 $2.15 $1.25 $8.49 $1.09 $1.09 $1.69 $11.48 $8.49 $8.49 $4.78 $4.78 $9.25 $1.25 $1.25 $1.25 $11.25 $11.25 $11.75 $4.45 $11.25 $4.45 $8.99 $1.09 $11.25 $2.15 $11.25 $9.25 $11.75 $11.25 $11.25 $9.25 $2.95 $11.25 $4.45 $8.75 $2.95 $2.95 $11.25 $1.50 $10.98 $16.98 $16.98 $18.50 $18.50 $10.98 $3.99 $1.09 $9.25 $9.25 $8.75 $4.45 $17.50 $17.50 $8.75 $4.45 $8.75 $1.25 $4.45 $9.25 $8.75 $8.75 $17.50 $17.50 $4.45 $9.39 $1.25 $2.95 $11.25 $8.75 $8.75 $11.25 $2.15 $8.90 $8.90 $11.25 $11.89 $10.98 $11.25 $4.45 $11.25 $11.25 $8.49 $10.98 $8.49 $3.39 $9.25 $8.75 $2.95 $3.00 $3.00 $9.39 $11.75 $2.95 $1.50 $11.25 $11.75 $8.75 $2.15 $1.50 $8.49 $3.39 $11.75 $1.25 $17.50 $17.50 $11.25 $1.25 $8.75 $2.95 $1.25 $11.25 $11.75 $13.35 $13.35 $13.35 $11.25 $11.75 $11.25 $11.25 $4.45 $11.25 $8.49 $3.39 $9.25 $2.95 $4.78 $4.78 $2.39 $3.99 $8.99 $8.99 $11.25 $11.25 $8.75 $11.25 $2.95 $4.45 $9.25 $8.75 $4.45 $8.49 $8.49 $10.98 $10.98 $3.99 $11.75 $8.75 $11.75 $4.45 $1.50 $1.25 $8.49 $8.49 $8.75 $8.75 $8.75 $9.25 $8.75 $2.95 $1.25 $11.25 $1.50 $11.25 $4.45 $9.25 $8.75 $8.75 $9.25 $8.75 $4.45 $1.50 $8.75 $8.75 $8.49 $1.69 $8.75 $2.15 $9.25 $2.15 $1.50 $11.25 $11.75 $2.15 $6.49 $9.25 $9.25 $11.25 $11.25 $11.75 $11.75 $11.75 $11.25 $8.75 $2.15 $1.25 $11.75 $9.25 $11.25 $8.75 $5.90 $5.90 $8.75 $4.45 $9.25 $9.25 $4.45 $11.25 $4.45 $11.25 $8.75 $2.15 $11.89 $11.25 $8.75 $2.95 $1.50 $8.75 $4.30 $4.30 $8.75 $11.25 $11.75 $11.75 $2.15 $11.25 $8.99 $1.09 $8.49 $8.49 $8.49 $3.39 $8.99 $10.98 $3.99 $11.75 $2.15 $8.75 $4.45 $2.50 $2.50 $11.48 $1.09 $8.49 $8.49 $16.98 $16.98 $3.99 $10.98 $1.09 $8.75 $2.95 $8.75 $8.75 $2.95 $9.25 $11.25 $2.15 $9.25 $4.45 $4.45 $9.25 $11.75 $11.75 $2.15 $9.25 $8.75 $11.25 $6.49 $8.75 $11.25 $2.95 $10.98 $3.99 $1.50 $9.25 $2.15 $8.75 $11.25 $11.89 $4.45 $1.50 $1.25 $8.75 $8.75 $4.45 $11.25 $11.75 $8.49 $1.09 $1.09 $1.69 $8.99 $3.39 $8.99 $1.69 $8.49 $8.99 $3.27 $3.27 $3.27 $8.99 $8.99 $1.09 $10.98 $1.69 $3.99 $8.49 $1.09 $8.75 $8.75 $11.75 $8.75 $9.25 $8.75 $3.39 $8.99 $8.99 $2.39 $9.25 $8.75 $9.25 $9.25 $8.99 $3.99 $2.39 $8.49 $1.09 $8.49 $8.99 $3.39 $11.25 $1.25 $8.99 $3.99 $8.75 $8.90 $8.90 $6.49 $8.75 $9.25 $11.25 $11.25 $11.25 $1.25 $8.75 $9.25 $4.45 $1.25 $8.75 $1.25 $2.15 $17.98 $17.98 $8.99 $8.75 $2.95 $1.25 $11.75 $1.50 $1.50 $8.75 $8.75 $11.08 $8.99 $1.69 $8.99 $1.69 $10.98 $3.99 $3.39 $11.75 $2.15 $11.75 $2.95 $8.75 $8.75 $11.75 $11.25 $11.75 $11.25 $4.45 $11.25 $1.25 $2.18 $2.18 $2.18 $2.18 $2.39 $8.49 $8.99 $2.39 $11.25 $8.75 $11.75 $11.75 $11.25 $4.45 $2.15 $8.19 $10.58 $4.45 $9.25 $1.09 $8.99 $11.25 $1.50 $8.99 $3.99 $4.45 $11.75 $2.15 $11.25 $8.75 $4.45 $8.75 $9.25 $6.45 $6.45 $6.45 $8.75 $11.25 $11.25 $8.75 $11.75 $21.96 $21.96 $8.99 $5.07 $5.07 $5.07 $8.49 $9.25 $11.25 $4.45 $3.39 $8.49 $8.99 $8.49 $17.50 $17.50 $22.96 $22.96 $8.75 $11.25 $11.89 $11.25 $8.49 $1.69 $1.09 $8.99 $8.99 $9.25 $8.75 $9.25 $2.95 $8.49 $3.99 $8.99 $8.49 $7.17 $7.17 $7.17 $8.49 $8.99 $17.50 $17.50 $9.25 $9.25 $11.25 $1.25 $8.99 $1.09 $8.75 $4.45 $11.25 $2.15 $11.75 $11.25 $11.25 $8.75 $8.75 $4.45 $1.25 $11.75 $11.75 $2.50 $2.50 $8.49 $8.99 $2.18 $2.18 $11.25 $4.45 $11.25 $11.75 $8.49 $8.99 $1.69 $1.09 $8.99 $8.99 $11.25 $6.49 $11.25 $8.75 $4.45 $8.99 $1.69 $11.48 $11.75 $2.50 $2.50 $8.49 $1.09 $1.09 $1.69 $8.49 $2.39 $11.75 $1.25 $8.49 $1.69 $8.49 $1.69 $11.75 $4.45 $8.75 $8.75 $4.45 $8.75 $11.25 $11.25 $8.75 $7.98 $7.98 $8.49 $1.09 $8.49 $3.99 $8.49 $3.99 $8.99 $3.99 $11.25 $4.45 $8.49 $2.39 $8.49 $2.39 $3.99 $8.49 $1.25 $11.25 $4.45 $9.25 $4.45 $1.09 $8.99 $3.99 $11.25 $8.90 $8.90 $9.25 $11.25 $8.75 $11.25 $11.25 $11.25 $11.25 $11.25 $8.99 $8.49 $8.75 $8.75 $4.45 $16.98 $16.98 $11.75 $11.25 $9.25 $4.45 $9.25 $2.95 $8.49 $1.69 $3.75 $3.75 $3.75 $4.45 $9.25 $1.50 $11.25 $11.48 $11.25 $2.15 $8.75 $9.39 $8.49 $3.99 $8.19 $2.29 $11.48 $1.69 $11.48 $3.99 $8.49 $1.69 $9.25 $2.95 $8.49 $1.69 $11.25 $4.45 $9.39 $9.25 $8.75 $8.75 $4.45 $11.89 $4.45 $4.45 $8.75 $8.75 $8.75 $2.15 $8.75 $3.75 $3.75 $3.75 $9.25 $11.25 $4.45 $6.49 $16.98 $16.98 $18.50 $18.50 $2.50 $2.50 $2.95 $3.99 $8.49 $8.19 $11.08 $6.49 $11.75 $2.39 $8.99 $1.09 $11.25 $4.45 $11.25 $8.99 $1.69 $21.96 $21.96 $2.18 $2.18 $8.99 $8.99 $2.39 $8.69 $1.69 $8.90 $8.90 $2.50 $2.50 $8.75 $8.99 $1.09 $8.49 $8.49 $8.75 $4.45 $17.50 $17.50 $8.75 $9.25 $8.49 $2.39 $8.75 $4.45 $11.25 $11.25 $11.75 $8.75 $8.49 $8.49 $8.49 $8.99 $8.75 $4.45 $11.48 $8.75 $1.25 $2.15 $9.25 $4.45 $11.75 $2.15 $11.25 $8.99 $2.39 $8.69 $8.69 $11.75 $2.95 $11.75 $1.50 $9.25 $4.45 $1.50 $11.48 $8.99 $2.39 $11.25 $11.89 $2.15 $1.25 $11.75 $4.45 $8.75 $8.75 $11.25 $4.45 $11.25 $2.15 $4.45 $8.49 $1.09 $3.99 $11.25 $11.25 $8.49 $2.39 $8.99 $2.39 $11.25 $2.15 $8.75 $2.95 $1.25 $8.75 $11.25 $17.50 $17.50 $11.75 $11.75 $11.25 $11.25 $4.45 $2.50 $2.50 $8.75 $8.99 $8.99 $1.69 $8.99 $1.69 $11.25 $1.25 $11.08 $8.69 $8.99 $1.09 $11.25 $11.25 $2.95 $1.25 $8.75 $1.25 $8.75 $8.75 $2.15 $1.25 $8.49 $3.99 $8.49 $2.39 $8.49 $7.17 $7.17 $7.17 $8.75 $4.45 $11.48 $8.75 $8.75 $11.48 $8.75 $9.25 $8.49 $3.99 $1.50 $11.25 $11.25 $8.75 $8.75 $4.45 $9.25 $4.45 $8.75 $8.75 $4.30 $4.30 $2.95 $8.75 $4.50 $4.50 $4.50 $9.25 $11.25 $4.45 $11.25 $11.25 $8.75 $9.25 $8.75 $2.15 $1.25 $8.75 $2.15 $1.25 $8.99 $8.49 $8.75 $8.75 $1.25 $11.75 $4.50 $4.50 $4.50 $8.75 $8.75 $3.99 $3.39 $8.49 $2.39 $8.99 $1.50 $11.25 $11.25 $8.75 $8.75 $2.15 $8.75 $2.15 $1.25 $21.96 $21.96 $8.49 $1.69 $26.07 $26.07 $26.07 $11.75 $1.50 $8.99 $8.99 $11.48 $9.25 $9.25 $8.75 $8.75 $8.75 $9.25 $8.75 $8.75 $2.15 $1.25 $11.89 $8.75 $11.75 $4.45 $18.50 $18.50 $9.25 $9.39 $8.49 $2.39 $8.49 $1.69 $1.09 $8.99 $8.49 $2.18 $2.18 $11.25 $1.25 $8.49 $3.39 $8.49 $3.99 $11.25 $8.75 $8.49 $1.69 $16.98 $16.98 $9.25 $9.25 $11.75 $1.25 $11.25 $8.75 $8.49 $8.49 $8.75 $1.25 $1.25 $1.25 $11.25 $12.98 $12.98 $11.75 $11.75 $4.45 $11.25 $11.75 $10.98 $8.49 $8.49 $2.39 $9.25 $11.25 $8.75 $2.95 $1.50 $11.25 $2.95 $9.25 $2.95 $9.25 $8.75 $11.25 $8.75 $8.75 $4.45 $11.25 $1.25 $8.75 $2.95 $2.50 $2.50 $9.25 $9.25 $9.25 $6.49 $17.50 $17.50 $8.49 $1.69 $8.49 $3.99 $8.75 $2.95 $2.95 $8.49 $8.99 $8.99 $1.09 $8.75 $4.45 $1.25 $11.25 $11.75 $11.25 $9.25 $11.25 $1.50 $11.25 $2.15 $10.98 $8.75 $11.75 $2.95 $11.25 $11.25 $9.25 $8.75 $9.25 $8.75 $8.75 $8.75 $1.25 $11.25 $1.50 $2.15 $8.75 $8.75 $11.25 $8.75 $8.75 $11.25 $4.45 $8.49 $8.49 $8.49 $8.49 $8.99 $1.69 $2.39 $1.09 $1.09 $11.25 $2.95 $35.25 $35.25 $35.25 $8.75 $2.95 $1.25 $11.25 $2.15 $9.25 $4.45 $8.75 $8.75 $8.75 $4.45 $1.25 $11.89 $8.75 $2.15 $1.25 $8.49 $1.09 $1.09 $1.69 $8.69 $8.69 $9.25 $2.95 $10.98 $2.39 $22.50 $22.50 $21.96 $21.96 $10.98 $8.49 $1.69 $11.75 $2.95 $8.75 $11.25 $8.75 $9.25 $8.75 $8.19 $10.58 $8.75 $2.95 $1.50 $2.50 $2.50 $9.25 $4.45 $9.25 $11.25 $8.69 $8.69 $3.89 $8.69 $1.69 $4.45 $9.25 $11.25 $4.45 $2.15 $8.19 $8.69 $4.45 $11.25 $1.25 $11.25 $8.75 $11.89 $11.75 $11.75 $9.25 $8.75 $2.15 $1.50 $11.75 $4.45 $3.99 $8.99 $10.98 $2.39 $1.25 $2.95 $8.75 $2.95 $9.25 $9.25 $9.25 $8.75 $2.15 $9.25 $9.25 $3.39 $8.49 $8.49 $2.39 $10.98 $1.09 $1.09 $8.49 $11.25 $1.25 $8.99 $1.09 $8.75 $2.15 $1.50 $3.99 $8.49 $8.75 $2.15 $1.50 $8.49 $10.98 $2.18 $2.18 $8.75 $2.15 $1.50 $4.45 $9.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $10.50 $10.50 $10.50 $10.50 $10.50 $10.50 $10.50 $6.49 $33.75 $33.75 $33.75 $35.00 $35.00 $35.00 $35.00 $27.75 $27.75 $27.75 $3.00 $3.00 $11.25 $11.75 $10.98 $2.39 $2.50 $2.50 $8.75 $4.45 $16.98 $16.98 $8.75 $6.49 $16.98 $16.98 $17.98 $17.98 $16.98 $16.98 $16.98 $16.98 $8.99 $8.99 $8.49 $9.25 $8.75 $2.95 $11.48 $2.39 $8.99 $2.39 $11.25 $4.45 $8.75 $9.25 $6.49 $26.25 $26.25 $26.25 $8.75 $26.25 $26.25 $26.25 $8.75 $8.75 $11.25 $11.25 $2.15 $1.25 $11.75 $8.75 $2.15 $1.50 $11.25 $2.15 $8.99 $2.39 $11.25 $11.25 $2.15 $11.25 $11.25 $8.75 $4.78 $4.78 $21.96 $21.96 $8.49 $2.39 $9.25 $2.95 $16.98 $16.98 $8.19 $3.89 $8.99 $1.09 $8.99 $3.39 $9.25 $4.45 $10.98 $10.98 $17.50 $17.50 $11.25 $1.25 $8.75 $11.75 $4.45 $11.25 $11.25 $8.99 $1.09 $10.98 $8.49 $1.69 $11.25 $9.25 $16.98 $16.98 $8.75 $4.45 $11.25 $6.49 $11.75 $9.25 $9.25 $8.75 $4.45 $2.50 $2.50 $8.75 $11.75 $8.75 $9.25 $11.75 $9.25 $8.75 $9.25 $8.75 $11.25 $11.75 $9.25 $8.75 $11.75 $8.49 $1.09 $1.09 $8.49 $1.09 $1.69 $11.25 $1.25 $8.75 $2.15 $1.50 $8.49 $1.69 $1.25 $8.75 $2.95 $8.49 $3.99 $8.49 $8.49 $8.75 $11.25 $2.15 $1.50 $11.75 $8.99 $1.09 $10.98 $10.98 $11.25 $1.25 $8.75 $4.45 $4.45 $1.25 $11.89 $8.99 $8.99 $11.25 $4.45 $23.50 $23.50 $8.49 $3.99 $9.25 $4.45 $4.45 $9.25 $8.75 $4.45 $8.75 $8.75 $11.75 $6.49 $17.50 $17.50 $4.45 $8.75 $2.95 $1.50 $8.75 $8.75 $8.75 $4.45 $11.25 $11.25 $11.75 $11.25 $2.95 $11.25 $4.45 $3.00 $3.00 $1.25 $2.95 $9.25 $8.99 $2.39 $6.49 $8.75 $8.90 $8.90 $11.48 $1.09 $10.98 $9.25 $9.25 $11.25 $8.75 $11.75 $11.25 $11.25 $1.25 $9.25 $4.45 $9.25 $6.49 $11.75 $11.75 $8.99 $2.39 $8.49 $8.49 $9.25 $9.25 $1.25 $8.75 $2.95 $11.75 $2.15 $8.49 $8.49 $8.69 $16.38 $16.38 $8.19 $3.89 $2.29 $11.75 $8.75 $8.75 $8.75 $4.45 $8.49 $8.49 $9.25 $8.75 $6.49 $2.95 $11.25 $11.25 $2.15 $9.25 $11.75 $21.96 $21.96 $8.49 $3.39 $1.69 $8.49 $8.75 $4.45 $8.49 $3.99 $11.25 $8.75 $11.25 $2.15 $11.75 $4.45 $11.25 $9.25 $8.75 $18.50 $18.50 $1.50 $8.75 $2.15 $11.48 $2.18 $2.18 $3.99 $11.25 $1.50 $8.99 $2.39 $11.75 $1.50 $11.25 $6.49 $4.45 $11.25 $8.49 $3.99 $2.50 $2.50 $8.75 $9.25 $3.99 $8.99 $8.75 $6.49 $13.52 $13.52 $13.52 $13.52 $13.52 $13.52 $13.52 $13.52 $16.98 $16.98 $16.98 $16.98 $17.98 $17.98 $16.98 $16.98 $8.75 $8.75 $11.25 $11.25 $8.49 $1.09 $1.69 $1.25 $9.25 $2.95 $8.69 $8.19 $8.49 $2.39 $8.49 $2.39 $10.98 $8.99 $8.99 $1.69 $8.49 $8.75 $8.75 $11.25 $4.45 $4.45 $17.50 $17.50 $8.75 $4.45 $8.75 $2.15 $1.50 $1.50 $8.99 $1.09 $8.75 $4.45 $8.75 $8.75 $11.25 $4.30 $4.30 $8.49 $1.69 $1.09 $1.09 $8.75 $2.15 $1.50 $8.99 $8.49 $3.99 $8.75 $2.15 $1.25 $9.25 $2.95 $11.25 $4.45 $9.25 $2.95 $3.99 $8.99 $8.49 $8.75 $8.75 $11.25 $9.25 $8.75 $4.45 $11.25 $1.25 $9.25 $11.25 $4.45 $2.95 $10.98 $8.75 $8.75 $18.50 $18.50 $9.25 $9.25 $5.00 $5.00 $5.00 $5.00 $8.75 $2.95 $16.98 $16.98 $11.25 $2.95 $8.75 $2.15 $1.50 $8.49 $2.39 $9.25 $2.15 $1.25 $8.19 $8.69 $8.19 $8.19 $8.75 $2.95 $1.25 $9.25 $2.95 $11.25 $8.75 $11.25 $11.25 $8.99 $1.09 $9.25 $9.25 $4.45 $8.49 $3.99 $2.39 $1.09 $8.99 $8.49 $8.75 $8.75 $11.25 $11.75 $4.45 $2.50 $2.50 $8.75 $8.49 $3.39 $8.75 $9.25 $4.45 $1.25 $11.25 $2.15 $4.45 $2.50 $2.50 $8.99 $3.99 $8.75 $2.15 $11.75 $11.75 $1.25 $8.75 $9.39 $11.25 $9.25 $9.25 $2.95 $9.25 $4.45 $1.25 $9.25 $8.75 $11.75 $1.50 $8.75 $4.45 $8.99 $1.09 $9.25 $2.95 $8.99 $1.69 $8.69 $1.69 $11.25 $4.45 $8.75 $8.75 $4.45 $11.25 $8.75 $2.95 $1.50 $8.19 $8.69 $1.09 $1.69 $8.49 $8.75 $2.95 $1.25 $8.49 $3.99 $10.98 $3.39 $11.25 $11.25 $2.15 $18.50 $18.50 $8.49 $8.49 $11.25 $1.50 $8.49 $2.39 $8.99 $2.39 $11.75 $4.45 $17.50 $17.50 $9.25 $9.25 $8.75 $4.45 $3.75 $3.75 $3.75 $8.75 $4.45 $11.75 $2.95 $1.25 $4.45 $8.75 $1.25 $1.50 $9.25 $11.25 $11.25 $11.25 $11.25 $9.25 $11.25 $4.45 $8.75 $9.25 $8.75 $8.75 $4.45 $8.75 $1.25 $2.15 $8.75 $8.75 $4.30 $4.30 $8.75 $1.25 $2.95 $9.25 $2.95 $4.45 $11.25 $11.25 $9.25 $9.25 $4.50 $4.50 $4.50 $11.75 $1.25 $11.75 $11.75 $1.25 $1.25 $9.25 $4.45 $11.25 $11.75 $17.50 $17.50 $2.15 $1.25 $11.25 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $11.25 $4.45 $4.45 $2.95 $11.25 $2.15 $1.25 $1.50 $8.75 $11.25 $2.95 $11.25 $1.25 $2.15 $11.25 $9.25 $6.49 $1.25 $8.75 $2.15 $8.75 $6.49 $11.25 $1.50 $8.75 $4.45 $8.75 $4.45 $9.25 $9.25 $1.25 $1.25 $8.75 $4.50 $4.50 $4.50 $11.25 $1.25 $1.50 $9.25 $2.15 $11.25 $4.45 $11.25 $4.45 $8.75 $4.45 $9.25 $4.45 $1.25 $11.25 $4.45 $8.75 $4.45 $8.75 $2.15 $8.75 $4.45 $8.75 $11.75 $1.50 $11.25 $4.45 $8.75 $2.15 $1.50 $8.75 $4.45 $8.75 $11.75 $8.75 $8.75 $11.25 $11.25 $1.50 $8.75 $2.15 $11.75 $2.15 $9.25 $2.95 $18.50 $18.50 $1.25 $4.45 $8.50 $8.50 $8.50 $8.50 $11.25 $11.89 $1.25 $9.39 $4.45 $8.75 $2.15 $1.50 $11.75 $8.75 $9.25 $9.25 $4.45 $1.25 $11.25 $9.25 $2.95 $8.75 $8.75 $2.15 $8.75 $11.25 $11.25 $11.25 $11.75 $11.25 $2.15 $11.25 $2.15 $8.99 $8.99 $8.75 $9.25 $9.25 $11.25 $8.75 $4.45 $8.75 $1.25 $4.45 $11.25 $1.25 $8.75 $8.75 $11.25 $8.75 $1.25 $1.25 $1.25 $9.25 $11.75 $2.15 $8.75 $4.45 $8.75 $4.45 $11.25 $9.25 $8.75 $9.25 $4.45 $11.75 $4.45 $1.25 $4.45 $11.75 $9.25 $11.25 $2.15 $23.50 $23.50 $9.25 $2.15 $18.50 $18.50 $8.75 $5.90 $5.90 $11.89 $4.45 $8.75 $4.45 $9.25 $2.95 $11.25 $2.95 $11.25 $11.25 $11.25 $2.95 $11.75 $9.25 $9.25 $2.95 $11.25 $2.15 $9.25 $8.90 $8.90 $8.75 $11.25 $11.25 $11.25 $8.75 $2.15 $1.25 $11.25 $1.25 $8.75 $4.45 $1.25 $11.25 $11.25 $11.75 $1.25 $11.25 $11.25 $11.25 $8.75 $8.75 $18.50 $18.50 $11.75 $1.25 $4.45 $9.25 $6.49 $4.45 $8.75 $11.25 $6.49 $11.75 $8.75 $9.25 $11.25 $2.15 $8.75 $4.45 $11.25 $4.45 $8.75 $9.25 $4.45 $1.25 $1.25 $8.75 $11.25 $11.75 $2.15 $11.75 $4.45 $11.75 $1.25 $11.75 $11.75 $11.25 $4.30 $4.30 $9.39 $9.39 $8.75 $1.25 $9.25 $4.45 $11.25 $1.25 $8.75 $1.25 $2.95 $11.25 $4.45 $8.75 $1.50 $4.45 $4.45 $9.25 $8.75 $2.95 $1.25 $11.25 $2.95 $8.75 $8.75 $8.75 $8.75 $4.30 $4.30 $9.25 $9.39 $4.45 $9.25 $1.25 $22.50 $22.50 $4.45 $2.95 $2.15 $23.50 $23.50 $11.75 $2.15 $1.25 $9.25 $4.45 $11.25 $11.75 $17.50 $17.50 $8.75 $11.75 $11.25 $8.75 $4.45 $11.75 $1.50 $8.75 $8.75 $11.75 $9.25 $11.25 $4.45 $11.75 $9.25 $4.45 $11.25 $8.75 $8.75 $2.15 $1.50 $8.75 $4.45 $9.25 $8.75 $1.50 $1.25 $1.25 $1.25 $8.75 $2.95 $11.25 $11.25 $1.50 $11.75 $11.25 $2.15 $9.25 $8.75 $11.75 $2.95 $1.50 $8.75 $1.50 $1.25 $8.75 $11.75 $11.25 $11.25 $11.75 $11.25 $11.75 $8.75 $17.80 $17.80 $17.80 $17.80 $5.00 $5.00 $5.00 $5.00 $5.00 $5.00 $5.00 $5.00 $8.75 $2.95 $1.25 $11.25 $1.25 $2.15 $11.25 $2.50 $2.50 $9.25 $1.25 $11.25 $2.95 $11.75 $2.15 $11.25 $1.50 $8.99 $1.99 $11.49 $8.75 $4.45 $1.25 $8.75 $4.45 $1.25 $1.50 $11.75 $8.75 $8.75 $11.25 $6.49 $11.75 $8.75 $2.15 $1.25 $6.49 $8.75 $4.45 $8.75 $4.45 $8.75 $11.25 $4.45 $6.49 $9.25 $8.75 $1.25 $4.45 $11.25 $8.75 $1.50 $8.75 $1.50 $1.25 $9.25 $9.39 $4.45 $9.25 $8.75 $4.45 $1.25 $11.25 $11.75 $8.75 $11.25 $9.25 $8.75 $11.25 $2.50 $2.50 $17.50 $17.50 $9.25 $4.45 $11.25 $1.25 $8.75 $4.45 $1.50 $8.75 $1.50 $1.25 $9.39 $8.75 $8.75 $4.45 $11.25 $1.25 $9.25 $4.45 $11.25 $8.75 $3.00 $3.00 $8.75 $2.15 $1.25 $11.25 $11.25 $4.45 $11.25 $11.25 $8.75 $11.75 $11.75 $11.75 $8.75 $4.45 $1.25 $1.50 $8.75 $4.45 $1.25 $9.25 $9.25 $8.75 $4.45 $1.25 $11.75 $11.25 $1.25 $11.75 $11.25 $9.25 $2.15 $1.50 $8.75 $4.45 $11.75 $11.75 $11.25 $8.75 $8.75 ' to numeric\n\n\n\n\n\nStep 17. How many different items are sold?\n\nchipo.item_name.value_counts().count()\n\n50\n\n\n\nSource: Ex2 - Getting and Knowing your Data\n\n01Occupation-Exercises-with-solutions\n\n\nEx3 - Getting and Knowing your Data\nThis time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\nimport numpy as np\n\n\n\nStep 2. Import the dataset from this address.\n\n\nStep 3. Assign it to a variable called users and use the ‘user_id’ as index\n\nusers = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT8/master/data/u.user', \n                      sep='|', index_col='user_id')\n\n\n\nStep 4. See the first 25 entries\n\nusers.head(25)\n\n\n\n\n\n\n\n\nage\ngender\noccupation\nzip_code\n\n\nuser_id\n\n\n\n\n\n\n\n\n1\n24\nM\ntechnician\n85711\n\n\n2\n53\nF\nother\n94043\n\n\n3\n23\nM\nwriter\n32067\n\n\n4\n24\nM\ntechnician\n43537\n\n\n5\n33\nF\nother\n15213\n\n\n6\n42\nM\nexecutive\n98101\n\n\n7\n57\nM\nadministrator\n91344\n\n\n8\n36\nM\nadministrator\n05201\n\n\n9\n29\nM\nstudent\n01002\n\n\n10\n53\nM\nlawyer\n90703\n\n\n11\n39\nF\nother\n30329\n\n\n12\n28\nF\nother\n06405\n\n\n13\n47\nM\neducator\n29206\n\n\n14\n45\nM\nscientist\n55106\n\n\n15\n49\nF\neducator\n97301\n\n\n16\n21\nM\nentertainment\n10309\n\n\n17\n30\nM\nprogrammer\n06355\n\n\n18\n35\nF\nother\n37212\n\n\n19\n40\nM\nlibrarian\n02138\n\n\n20\n42\nF\nhomemaker\n95660\n\n\n21\n26\nM\nwriter\n30068\n\n\n22\n25\nM\nwriter\n40206\n\n\n23\n30\nF\nartist\n48197\n\n\n24\n21\nF\nartist\n94533\n\n\n25\n39\nM\nengineer\n55107\n\n\n\n\n\n\n\n\n\nStep 5. See the last 10 entries\n\nusers.tail(10)\n\n\n\n\n\n\n\n\nage\ngender\noccupation\nzip_code\n\n\nuser_id\n\n\n\n\n\n\n\n\n934\n61\nM\nengineer\n22902\n\n\n935\n42\nM\ndoctor\n66221\n\n\n936\n24\nM\nother\n32789\n\n\n937\n48\nM\neducator\n98072\n\n\n938\n38\nF\ntechnician\n55038\n\n\n939\n26\nF\nstudent\n33319\n\n\n940\n32\nM\nadministrator\n02215\n\n\n941\n20\nM\nstudent\n97229\n\n\n942\n48\nF\nlibrarian\n78209\n\n\n943\n22\nM\nstudent\n77841\n\n\n\n\n\n\n\n\n\nStep 6. What is the number of observations in the dataset?\n\nusers.shape[0]\n\n943\n\n\n\n\nStep 7. What is the number of columns in the dataset?\n\nusers.shape[1]\n\n4\n\n\n\n\nStep 8. Print the name of all the columns.\n\nusers.columns\n\nIndex(['age', 'gender', 'occupation', 'zip_code'], dtype='object')\n\n\n\n\nStep 9. How is the dataset indexed?\n\nusers.index\n\nIndex([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,\n       ...\n       934, 935, 936, 937, 938, 939, 940, 941, 942, 943],\n      dtype='int64', name='user_id', length=943)\n\n\n\n\nStep 10. What is the data type of each column?\n\nusers.dtypes\n\nage            int64\ngender        object\noccupation    object\nzip_code      object\ndtype: object\n\n\n\n\nStep 11. Print only the occupation column\n\nusers.occupation\n\n#or\n\nusers['occupation']\n\nuser_id\n1         technician\n2              other\n3             writer\n4         technician\n5              other\n           ...      \n939          student\n940    administrator\n941          student\n942        librarian\n943          student\nName: occupation, Length: 943, dtype: object\n\n\n\n\nStep 12. How many different occupations are in this dataset?\n\nusers.occupation.nunique()\n\n21\n\n\n\n\nStep 13. What is the most frequent occupation?\n\nusers.occupation.value_counts().head(1).index[0]\n\n'student'\n\n\n\n\nStep 14. Summarize the DataFrame.\n\nusers.describe() \n\n\n\n\n\n\n\n\nage\n\n\n\n\ncount\n943.000000\n\n\nmean\n34.051962\n\n\nstd\n12.192740\n\n\nmin\n7.000000\n\n\n25%\n25.000000\n\n\n50%\n31.000000\n\n\n75%\n43.000000\n\n\nmax\n73.000000\n\n\n\n\n\n\n\n\n\nStep 15. Summarize all the columns\n\nusers.describe(include = \"all\")\n\n\n\n\n\n\n\n\nage\ngender\noccupation\nzip_code\n\n\n\n\ncount\n943.000000\n943\n943\n943\n\n\nunique\nNaN\n2\n21\n795\n\n\ntop\nNaN\nM\nstudent\n55414\n\n\nfreq\nNaN\n670\n196\n9\n\n\nmean\n34.051962\nNaN\nNaN\nNaN\n\n\nstd\n12.192740\nNaN\nNaN\nNaN\n\n\nmin\n7.000000\nNaN\nNaN\nNaN\n\n\n25%\n25.000000\nNaN\nNaN\nNaN\n\n\n50%\n31.000000\nNaN\nNaN\nNaN\n\n\n75%\n43.000000\nNaN\nNaN\nNaN\n\n\nmax\n73.000000\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\nStep 16. Summarize only the occupation column\n\nusers.occupation.describe()\n\ncount         943\nunique         21\ntop       student\nfreq          196\nName: occupation, dtype: object\n\n\n\n\nStep 17. What is the mean age of users?\n\nround(users.age.mean())\n\n34\n\n\n\n\nStep 18. What is the age with least occurrence?\n\nusers.age.value_counts().tail()\n\nage\n7     1\n66    1\n11    1\n10    1\n73    1\nName: count, dtype: int64\n\n\n\nSource: Ex3 - Getting and Knowing your Data\n\n01World-Food-Facts-Exercises-with-solutions\n\n\nExercise 1\n\nStep 1. Go to https://www.kaggle.com/openfoodfacts/world-food-facts/data\n\n\nStep 2. Download the dataset to your computer and unzip it.\n\nimport pandas as pd\nimport numpy as np\n\n\n\nStep 3. Use the tsv file and assign it to a dataframe called food\n\nfood = pd.read_csv('E:\\Yang Fan\\Lab 1\\en.openfoodfacts.org.products.tsv', sep='\\t')\n\n\n\nStep 4. See the first 5 entries\n\nfood.head()\n\n\n\n\n\n\n\n\ncode\nurl\ncreator\ncreated_t\ncreated_datetime\nlast_modified_t\nlast_modified_datetime\nproduct_name\ngeneric_name\nquantity\n...\nfruits-vegetables-nuts_100g\nfruits-vegetables-nuts-estimate_100g\ncollagen-meat-protein-ratio_100g\ncocoa_100g\nchlorophyl_100g\ncarbon-footprint_100g\nnutrition-score-fr_100g\nnutrition-score-uk_100g\nglycemic-index_100g\nwater-hardness_100g\n\n\n\n\n0\n3087\nhttp://world-en.openfoodfacts.org/product/0000...\nopenfoodfacts-contributors\n1474103866\n2016-09-17T09:17:46Z\n1474103893\n2016-09-17T09:18:13Z\nFarine de blé noir\nNaN\n1kg\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n4530\nhttp://world-en.openfoodfacts.org/product/0000...\nusda-ndb-import\n1489069957\n2017-03-09T14:32:37Z\n1489069957\n2017-03-09T14:32:37Z\nBanana Chips Sweetened (Whole)\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n14.0\n14.0\nNaN\nNaN\n\n\n2\n4559\nhttp://world-en.openfoodfacts.org/product/0000...\nusda-ndb-import\n1489069957\n2017-03-09T14:32:37Z\n1489069957\n2017-03-09T14:32:37Z\nPeanuts\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\n\n\n3\n16087\nhttp://world-en.openfoodfacts.org/product/0000...\nusda-ndb-import\n1489055731\n2017-03-09T10:35:31Z\n1489055731\n2017-03-09T10:35:31Z\nOrganic Salted Nut Mix\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n12.0\n12.0\nNaN\nNaN\n\n\n4\n16094\nhttp://world-en.openfoodfacts.org/product/0000...\nusda-ndb-import\n1489055653\n2017-03-09T10:34:13Z\n1489055653\n2017-03-09T10:34:13Z\nOrganic Polenta\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 163 columns\n\n\n\n\n\nStep 5. What is the number of observations in the dataset?\n\nfood.shape\n\n(356027, 163)\n\n\n\nfood.shape[0]\n\n356027\n\n\n\n\nStep 6. What is the number of columns in the dataset?\n\nprint(food.shape) \nprint(food.shape[1]) \n\n#OR\n\nfood.info() \n\n(356027, 163)\n163\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 356027 entries, 0 to 356026\nColumns: 163 entries, code to water-hardness_100g\ndtypes: float64(107), object(56)\nmemory usage: 442.8+ MB\n\n\n\n\nStep 7. Print the name of all the columns.\n\nfood.columns\n\nIndex(['code', 'url', 'creator', 'created_t', 'created_datetime',\n       'last_modified_t', 'last_modified_datetime', 'product_name',\n       'generic_name', 'quantity',\n       ...\n       'fruits-vegetables-nuts_100g', 'fruits-vegetables-nuts-estimate_100g',\n       'collagen-meat-protein-ratio_100g', 'cocoa_100g', 'chlorophyl_100g',\n       'carbon-footprint_100g', 'nutrition-score-fr_100g',\n       'nutrition-score-uk_100g', 'glycemic-index_100g',\n       'water-hardness_100g'],\n      dtype='object', length=163)\n\n\n\n\nStep 8. What is the name of 105th column?\n\nfood.columns[104]\n\n'-glucose_100g'\n\n\n\n\nStep 9. What is the type of the observations of the 105th column?\n\nfood.dtypes['-glucose_100g']\n\ndtype('float64')\n\n\n\n\nStep 10. How is the dataset indexed?\n\nfood.index\n\nRangeIndex(start=0, stop=356027, step=1)\n\n\n\n\nStep 11. What is the product name of the 19th observation?\n\nfood.values[18][7]\n\n'Lotus Organic Brown Jasmine Rice'\n\n\n\nSource: Exercise 1\n\n02Chipotle-Exercises-with-solutions\n\n\nEx1 - Filtering and Sorting Data\nThis time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\n\n\n\nStep 2. Import the dataset from this address.\n\n\nStep 3. Assign it to a variable called chipo.\n\nurl = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv'\n\nchipo = pd.read_csv(url, sep = '\\t')\n\n\n\nStep 4. How many products cost more than $10.00?\n\n\nprices = [float(value[1 : -1]) for value in chipo.item_price]\n\nchipo.item_price = prices\n\nchipo_filtered = chipo.drop_duplicates(['item_name','quantity','choice_description'])\n\nchipo_one_prod = chipo_filtered[chipo_filtered.quantity == 1]\nchipo_one_prod\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nchoice_description\nitem_price\n\n\n\n\n0\n1\n1\nChips and Fresh Tomato Salsa\nNaN\n2.39\n\n\n1\n1\n1\nIzze\n[Clementine]\n3.39\n\n\n2\n1\n1\nNantucket Nectar\n[Apple]\n3.39\n\n\n3\n1\n1\nChips and Tomatillo-Green Chili Salsa\nNaN\n2.39\n\n\n5\n3\n1\nChicken Bowl\n[Fresh Tomato Salsa (Mild), [Rice, Cheese, Sou...\n10.98\n\n\n...\n...\n...\n...\n...\n...\n\n\n4602\n1827\n1\nBarbacoa Burrito\n[Tomatillo Green Chili Salsa]\n9.25\n\n\n4607\n1829\n1\nSteak Burrito\n[Tomatillo Green Chili Salsa, [Rice, Cheese, S...\n11.75\n\n\n4610\n1830\n1\nSteak Burrito\n[Fresh Tomato Salsa, [Rice, Sour Cream, Cheese...\n11.75\n\n\n4611\n1830\n1\nVeggie Burrito\n[Tomatillo Green Chili Salsa, [Rice, Fajita Ve...\n11.25\n\n\n4612\n1831\n1\nCarnitas Bowl\n[Fresh Tomato Salsa, [Fajita Vegetables, Rice,...\n9.25\n\n\n\n\n1806 rows × 5 columns\n\n\n\n\nchipo.query('item_price &gt; 10').item_name.nunique()\n\n31\n\n\n\n\nStep 5. What is the price of each item?\n\nprint a data frame with only two columns item_name and item_price\n\n\nchipo_filtered = chipo.drop_duplicates(['item_name','quantity'])\n\nchipo[(chipo['item_name'] == 'Chicken Bowl') & (chipo['quantity'] == 1)]\n\nchipo_one_prod = chipo_filtered[chipo_filtered.quantity == 1]\n\nprice_per_item = chipo_one_prod[['item_name', 'item_price']]\n\nprice_per_item.sort_values(by = \"item_price\", ascending = False).head(20)\n\n\n\n\n\n\n\n\nitem_name\nitem_price\n\n\n\n\n606\nSteak Salad Bowl\n11.89\n\n\n1229\nBarbacoa Salad Bowl\n11.89\n\n\n1132\nCarnitas Salad Bowl\n11.89\n\n\n7\nSteak Burrito\n11.75\n\n\n168\nBarbacoa Crispy Tacos\n11.75\n\n\n39\nBarbacoa Bowl\n11.75\n\n\n738\nVeggie Soft Tacos\n11.25\n\n\n186\nVeggie Salad Bowl\n11.25\n\n\n62\nVeggie Bowl\n11.25\n\n\n57\nVeggie Burrito\n11.25\n\n\n250\nChicken Salad\n10.98\n\n\n5\nChicken Bowl\n10.98\n\n\n8\nSteak Soft Tacos\n9.25\n\n\n554\nCarnitas Crispy Tacos\n9.25\n\n\n237\nCarnitas Soft Tacos\n9.25\n\n\n56\nBarbacoa Soft Tacos\n9.25\n\n\n92\nSteak Crispy Tacos\n9.25\n\n\n664\nSteak Salad\n8.99\n\n\n54\nSteak Bowl\n8.99\n\n\n3750\nCarnitas Salad\n8.99\n\n\n\n\n\n\n\n\n\n\nStep 6. Sort by the name of the item\n\nchipo.item_name.sort_values()\n\n# OR\n\nchipo.sort_values(by = \"item_name\")\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nchoice_description\nitem_price\n\n\n\n\n3389\n1360\n2\n6 Pack Soft Drink\n[Diet Coke]\n12.98\n\n\n341\n148\n1\n6 Pack Soft Drink\n[Diet Coke]\n6.49\n\n\n1849\n749\n1\n6 Pack Soft Drink\n[Coke]\n6.49\n\n\n1860\n754\n1\n6 Pack Soft Drink\n[Diet Coke]\n6.49\n\n\n2713\n1076\n1\n6 Pack Soft Drink\n[Coke]\n6.49\n\n\n...\n...\n...\n...\n...\n...\n\n\n2384\n948\n1\nVeggie Soft Tacos\n[Roasted Chili Corn Salsa, [Fajita Vegetables,...\n8.75\n\n\n781\n322\n1\nVeggie Soft Tacos\n[Fresh Tomato Salsa, [Black Beans, Cheese, Sou...\n8.75\n\n\n2851\n1132\n1\nVeggie Soft Tacos\n[Roasted Chili Corn Salsa (Medium), [Black Bea...\n8.49\n\n\n1699\n688\n1\nVeggie Soft Tacos\n[Fresh Tomato Salsa, [Fajita Vegetables, Rice,...\n11.25\n\n\n1395\n567\n1\nVeggie Soft Tacos\n[Fresh Tomato Salsa (Mild), [Pinto Beans, Rice...\n8.49\n\n\n\n\n4622 rows × 5 columns\n\n\n\n\n\nStep 7. What was the quantity of the most expensive item ordered?\n\nchipo.sort_values(by = \"item_price\", ascending = False).head(1)\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nchoice_description\nitem_price\n\n\n\n\n3598\n1443\n15\nChips and Fresh Tomato Salsa\nNaN\n44.25\n\n\n\n\n\n\n\n\n\nStep 8. How many times was a Veggie Salad Bowl ordered?\n\nchipo_salad = chipo[chipo.item_name == \"Veggie Salad Bowl\"]\n# chipo_salad = chipo.query('item_name == \"Veggie Salad Bowl\"')\n\nlen(chipo_salad)\n\n18\n\n\n\n\nStep 9. How many times did someone order more than one Canned Soda?\n\nchipo_drink_steak_bowl = chipo[(chipo.item_name == \"Canned Soda\") & (chipo.quantity &gt; 1)]\n# chipo_drink_steak_bowl = chipo.query('item_name == \"Canned Soda\" & quantity &gt; 1')\n\nlen(chipo_drink_steak_bowl)\n\n20\n\n\n\nSource: Ex1 - Filtering and Sorting Data\n\n02Euro12-Exercises-with-solutions\n\n\nEx2 - Filtering and Sorting Data\nThis time we are going to pull data directly from the internet.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\n\n\n\nStep 2. Import the dataset from this address.\n\n\nStep 3. Assign it to a variable called euro12.\n\neuro12 = pd.read_csv('https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/02_Filtering_%26_Sorting/Euro12/Euro_2012_stats_TEAM.csv', sep=',')\neuro12\n\n\n\n\n\n\n\n\nTeam\nGoals\nShots on target\nShots off target\nShooting Accuracy\n% Goals-to-shots\nTotal shots (inc. Blocked)\nHit Woodwork\nPenalty goals\nPenalties not scored\n...\nSaves made\nSaves-to-shots ratio\nFouls Won\nFouls Conceded\nOffsides\nYellow Cards\nRed Cards\nSubs on\nSubs off\nPlayers Used\n\n\n\n\n0\nCroatia\n4\n13\n12\n51.9%\n16.0%\n32\n0\n0\n0\n...\n13\n81.3%\n41\n62\n2\n9\n0\n9\n9\n16\n\n\n1\nCzech Republic\n4\n13\n18\n41.9%\n12.9%\n39\n0\n0\n0\n...\n9\n60.1%\n53\n73\n8\n7\n0\n11\n11\n19\n\n\n2\nDenmark\n4\n10\n10\n50.0%\n20.0%\n27\n1\n0\n0\n...\n10\n66.7%\n25\n38\n8\n4\n0\n7\n7\n15\n\n\n3\nEngland\n5\n11\n18\n50.0%\n17.2%\n40\n0\n0\n0\n...\n22\n88.1%\n43\n45\n6\n5\n0\n11\n11\n16\n\n\n4\nFrance\n3\n22\n24\n37.9%\n6.5%\n65\n1\n0\n0\n...\n6\n54.6%\n36\n51\n5\n6\n0\n11\n11\n19\n\n\n5\nGermany\n10\n32\n32\n47.8%\n15.6%\n80\n2\n1\n0\n...\n10\n62.6%\n63\n49\n12\n4\n0\n15\n15\n17\n\n\n6\nGreece\n5\n8\n18\n30.7%\n19.2%\n32\n1\n1\n1\n...\n13\n65.1%\n67\n48\n12\n9\n1\n12\n12\n20\n\n\n7\nItaly\n6\n34\n45\n43.0%\n7.5%\n110\n2\n0\n0\n...\n20\n74.1%\n101\n89\n16\n16\n0\n18\n18\n19\n\n\n8\nNetherlands\n2\n12\n36\n25.0%\n4.1%\n60\n2\n0\n0\n...\n12\n70.6%\n35\n30\n3\n5\n0\n7\n7\n15\n\n\n9\nPoland\n2\n15\n23\n39.4%\n5.2%\n48\n0\n0\n0\n...\n6\n66.7%\n48\n56\n3\n7\n1\n7\n7\n17\n\n\n10\nPortugal\n6\n22\n42\n34.3%\n9.3%\n82\n6\n0\n0\n...\n10\n71.5%\n73\n90\n10\n12\n0\n14\n14\n16\n\n\n11\nRepublic of Ireland\n1\n7\n12\n36.8%\n5.2%\n28\n0\n0\n0\n...\n17\n65.4%\n43\n51\n11\n6\n1\n10\n10\n17\n\n\n12\nRussia\n5\n9\n31\n22.5%\n12.5%\n59\n2\n0\n0\n...\n10\n77.0%\n34\n43\n4\n6\n0\n7\n7\n16\n\n\n13\nSpain\n12\n42\n33\n55.9%\n16.0%\n100\n0\n1\n0\n...\n15\n93.8%\n102\n83\n19\n11\n0\n17\n17\n18\n\n\n14\nSweden\n5\n17\n19\n47.2%\n13.8%\n39\n3\n0\n0\n...\n8\n61.6%\n35\n51\n7\n7\n0\n9\n9\n18\n\n\n15\nUkraine\n2\n7\n26\n21.2%\n6.0%\n38\n0\n0\n0\n...\n13\n76.5%\n48\n31\n4\n5\n0\n9\n9\n18\n\n\n\n\n16 rows × 35 columns\n\n\n\n\n\nStep 4. Select only the Goal column.\n\neuro12.Goals\n\n0      4\n1      4\n2      4\n3      5\n4      3\n5     10\n6      5\n7      6\n8      2\n9      2\n10     6\n11     1\n12     5\n13    12\n14     5\n15     2\nName: Goals, dtype: int64\n\n\n\n\nStep 5. How many team participated in the Euro2012?\n\neuro12.shape[0]\n\n16\n\n\n\n\nStep 6. What is the number of columns in the dataset?\n\neuro12.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 16 entries, 0 to 15\nData columns (total 35 columns):\n #   Column                      Non-Null Count  Dtype  \n---  ------                      --------------  -----  \n 0   Team                        16 non-null     object \n 1   Goals                       16 non-null     int64  \n 2   Shots on target             16 non-null     int64  \n 3   Shots off target            16 non-null     int64  \n 4   Shooting Accuracy           16 non-null     object \n 5   % Goals-to-shots            16 non-null     object \n 6   Total shots (inc. Blocked)  16 non-null     int64  \n 7   Hit Woodwork                16 non-null     int64  \n 8   Penalty goals               16 non-null     int64  \n 9   Penalties not scored        16 non-null     int64  \n 10  Headed goals                16 non-null     int64  \n 11  Passes                      16 non-null     int64  \n 12  Passes completed            16 non-null     int64  \n 13  Passing Accuracy            16 non-null     object \n 14  Touches                     16 non-null     int64  \n 15  Crosses                     16 non-null     int64  \n 16  Dribbles                    16 non-null     int64  \n 17  Corners Taken               16 non-null     int64  \n 18  Tackles                     16 non-null     int64  \n 19  Clearances                  16 non-null     int64  \n 20  Interceptions               16 non-null     int64  \n 21  Clearances off line         15 non-null     float64\n 22  Clean Sheets                16 non-null     int64  \n 23  Blocks                      16 non-null     int64  \n 24  Goals conceded              16 non-null     int64  \n 25  Saves made                  16 non-null     int64  \n 26  Saves-to-shots ratio        16 non-null     object \n 27  Fouls Won                   16 non-null     int64  \n 28  Fouls Conceded              16 non-null     int64  \n 29  Offsides                    16 non-null     int64  \n 30  Yellow Cards                16 non-null     int64  \n 31  Red Cards                   16 non-null     int64  \n 32  Subs on                     16 non-null     int64  \n 33  Subs off                    16 non-null     int64  \n 34  Players Used                16 non-null     int64  \ndtypes: float64(1), int64(29), object(5)\nmemory usage: 4.5+ KB\n\n\n\n\nStep 7. View only the columns Team, Yellow Cards and Red Cards and assign them to a dataframe called discipline\n\ndiscipline = euro12[['Team', 'Yellow Cards', 'Red Cards']]\ndiscipline\n\n\n\n\n\n\n\n\nTeam\nYellow Cards\nRed Cards\n\n\n\n\n0\nCroatia\n9\n0\n\n\n1\nCzech Republic\n7\n0\n\n\n2\nDenmark\n4\n0\n\n\n3\nEngland\n5\n0\n\n\n4\nFrance\n6\n0\n\n\n5\nGermany\n4\n0\n\n\n6\nGreece\n9\n1\n\n\n7\nItaly\n16\n0\n\n\n8\nNetherlands\n5\n0\n\n\n9\nPoland\n7\n1\n\n\n10\nPortugal\n12\n0\n\n\n11\nRepublic of Ireland\n6\n1\n\n\n12\nRussia\n6\n0\n\n\n13\nSpain\n11\n0\n\n\n14\nSweden\n7\n0\n\n\n15\nUkraine\n5\n0\n\n\n\n\n\n\n\n\n\nStep 8. Sort the teams by Red Cards, then to Yellow Cards\n\ndiscipline.sort_values(['Red Cards', 'Yellow Cards'], ascending = False)\n\n\n\n\n\n\n\n\nTeam\nYellow Cards\nRed Cards\n\n\n\n\n6\nGreece\n9\n1\n\n\n9\nPoland\n7\n1\n\n\n11\nRepublic of Ireland\n6\n1\n\n\n7\nItaly\n16\n0\n\n\n10\nPortugal\n12\n0\n\n\n13\nSpain\n11\n0\n\n\n0\nCroatia\n9\n0\n\n\n1\nCzech Republic\n7\n0\n\n\n14\nSweden\n7\n0\n\n\n4\nFrance\n6\n0\n\n\n12\nRussia\n6\n0\n\n\n3\nEngland\n5\n0\n\n\n8\nNetherlands\n5\n0\n\n\n15\nUkraine\n5\n0\n\n\n2\nDenmark\n4\n0\n\n\n5\nGermany\n4\n0\n\n\n\n\n\n\n\n\n\nStep 9. Calculate the mean Yellow Cards given per Team\n\n\nround(discipline['Yellow Cards'].mean())\n\n7\n\n\n\n\nStep 10. Filter teams that scored more than 6 goals\n\neuro12[euro12.Goals &gt; 6]\n\n\n\n\n\n\n\n\nTeam\nGoals\nShots on target\nShots off target\nShooting Accuracy\n% Goals-to-shots\nTotal shots (inc. Blocked)\nHit Woodwork\nPenalty goals\nPenalties not scored\n...\nSaves made\nSaves-to-shots ratio\nFouls Won\nFouls Conceded\nOffsides\nYellow Cards\nRed Cards\nSubs on\nSubs off\nPlayers Used\n\n\n\n\n5\nGermany\n10\n32\n32\n47.8%\n15.6%\n80\n2\n1\n0\n...\n10\n62.6%\n63\n49\n12\n4\n0\n15\n15\n17\n\n\n13\nSpain\n12\n42\n33\n55.9%\n16.0%\n100\n0\n1\n0\n...\n15\n93.8%\n102\n83\n19\n11\n0\n17\n17\n18\n\n\n\n\n2 rows × 35 columns\n\n\n\n\n\nStep 11. Select the teams that start with G\n\neuro12[euro12.Team.str.startswith('G')]\n\n\n\n\n\n\n\n\nTeam\nGoals\nShots on target\nShots off target\nShooting Accuracy\n% Goals-to-shots\nTotal shots (inc. Blocked)\nHit Woodwork\nPenalty goals\nPenalties not scored\n...\nSaves made\nSaves-to-shots ratio\nFouls Won\nFouls Conceded\nOffsides\nYellow Cards\nRed Cards\nSubs on\nSubs off\nPlayers Used\n\n\n\n\n5\nGermany\n10\n32\n32\n47.8%\n15.6%\n80\n2\n1\n0\n...\n10\n62.6%\n63\n49\n12\n4\n0\n15\n15\n17\n\n\n6\nGreece\n5\n8\n18\n30.7%\n19.2%\n32\n1\n1\n1\n...\n13\n65.1%\n67\n48\n12\n9\n1\n12\n12\n20\n\n\n\n\n2 rows × 35 columns\n\n\n\n\n\nStep 12. Select the first 7 columns\n\neuro12.iloc[: , 0:7]\n\n\n\n\n\n\n\n\nTeam\nGoals\nShots on target\nShots off target\nShooting Accuracy\n% Goals-to-shots\nTotal shots (inc. Blocked)\n\n\n\n\n0\nCroatia\n4\n13\n12\n51.9%\n16.0%\n32\n\n\n1\nCzech Republic\n4\n13\n18\n41.9%\n12.9%\n39\n\n\n2\nDenmark\n4\n10\n10\n50.0%\n20.0%\n27\n\n\n3\nEngland\n5\n11\n18\n50.0%\n17.2%\n40\n\n\n4\nFrance\n3\n22\n24\n37.9%\n6.5%\n65\n\n\n5\nGermany\n10\n32\n32\n47.8%\n15.6%\n80\n\n\n6\nGreece\n5\n8\n18\n30.7%\n19.2%\n32\n\n\n7\nItaly\n6\n34\n45\n43.0%\n7.5%\n110\n\n\n8\nNetherlands\n2\n12\n36\n25.0%\n4.1%\n60\n\n\n9\nPoland\n2\n15\n23\n39.4%\n5.2%\n48\n\n\n10\nPortugal\n6\n22\n42\n34.3%\n9.3%\n82\n\n\n11\nRepublic of Ireland\n1\n7\n12\n36.8%\n5.2%\n28\n\n\n12\nRussia\n5\n9\n31\n22.5%\n12.5%\n59\n\n\n13\nSpain\n12\n42\n33\n55.9%\n16.0%\n100\n\n\n14\nSweden\n5\n17\n19\n47.2%\n13.8%\n39\n\n\n15\nUkraine\n2\n7\n26\n21.2%\n6.0%\n38\n\n\n\n\n\n\n\n\n\nStep 13. Select all columns except the last 3.\n\neuro12.iloc[: , :-3]\n\n\n\n\n\n\n\n\nTeam\nGoals\nShots on target\nShots off target\nShooting Accuracy\n% Goals-to-shots\nTotal shots (inc. Blocked)\nHit Woodwork\nPenalty goals\nPenalties not scored\n...\nClean Sheets\nBlocks\nGoals conceded\nSaves made\nSaves-to-shots ratio\nFouls Won\nFouls Conceded\nOffsides\nYellow Cards\nRed Cards\n\n\n\n\n0\nCroatia\n4\n13\n12\n51.9%\n16.0%\n32\n0\n0\n0\n...\n0\n10\n3\n13\n81.3%\n41\n62\n2\n9\n0\n\n\n1\nCzech Republic\n4\n13\n18\n41.9%\n12.9%\n39\n0\n0\n0\n...\n1\n10\n6\n9\n60.1%\n53\n73\n8\n7\n0\n\n\n2\nDenmark\n4\n10\n10\n50.0%\n20.0%\n27\n1\n0\n0\n...\n1\n10\n5\n10\n66.7%\n25\n38\n8\n4\n0\n\n\n3\nEngland\n5\n11\n18\n50.0%\n17.2%\n40\n0\n0\n0\n...\n2\n29\n3\n22\n88.1%\n43\n45\n6\n5\n0\n\n\n4\nFrance\n3\n22\n24\n37.9%\n6.5%\n65\n1\n0\n0\n...\n1\n7\n5\n6\n54.6%\n36\n51\n5\n6\n0\n\n\n5\nGermany\n10\n32\n32\n47.8%\n15.6%\n80\n2\n1\n0\n...\n1\n11\n6\n10\n62.6%\n63\n49\n12\n4\n0\n\n\n6\nGreece\n5\n8\n18\n30.7%\n19.2%\n32\n1\n1\n1\n...\n1\n23\n7\n13\n65.1%\n67\n48\n12\n9\n1\n\n\n7\nItaly\n6\n34\n45\n43.0%\n7.5%\n110\n2\n0\n0\n...\n2\n18\n7\n20\n74.1%\n101\n89\n16\n16\n0\n\n\n8\nNetherlands\n2\n12\n36\n25.0%\n4.1%\n60\n2\n0\n0\n...\n0\n9\n5\n12\n70.6%\n35\n30\n3\n5\n0\n\n\n9\nPoland\n2\n15\n23\n39.4%\n5.2%\n48\n0\n0\n0\n...\n0\n8\n3\n6\n66.7%\n48\n56\n3\n7\n1\n\n\n10\nPortugal\n6\n22\n42\n34.3%\n9.3%\n82\n6\n0\n0\n...\n2\n11\n4\n10\n71.5%\n73\n90\n10\n12\n0\n\n\n11\nRepublic of Ireland\n1\n7\n12\n36.8%\n5.2%\n28\n0\n0\n0\n...\n0\n23\n9\n17\n65.4%\n43\n51\n11\n6\n1\n\n\n12\nRussia\n5\n9\n31\n22.5%\n12.5%\n59\n2\n0\n0\n...\n0\n8\n3\n10\n77.0%\n34\n43\n4\n6\n0\n\n\n13\nSpain\n12\n42\n33\n55.9%\n16.0%\n100\n0\n1\n0\n...\n5\n8\n1\n15\n93.8%\n102\n83\n19\n11\n0\n\n\n14\nSweden\n5\n17\n19\n47.2%\n13.8%\n39\n3\n0\n0\n...\n1\n12\n5\n8\n61.6%\n35\n51\n7\n7\n0\n\n\n15\nUkraine\n2\n7\n26\n21.2%\n6.0%\n38\n0\n0\n0\n...\n0\n4\n4\n13\n76.5%\n48\n31\n4\n5\n0\n\n\n\n\n16 rows × 32 columns\n\n\n\n\n\nStep 14. Present only the Shooting Accuracy from England, Italy and Russia\n\neuro12.loc[euro12.Team.isin(['England', 'Italy', 'Russia']), ['Team','Shooting Accuracy']]\n\n\n\n\n\n\n\n\nTeam\nShooting Accuracy\n\n\n\n\n3\nEngland\n50.0%\n\n\n7\nItaly\n43.0%\n\n\n12\nRussia\n22.5%\n\n\n\n\n\n\n\n\nSource: Ex2 - Filtering and Sorting Data\n\n03Chipotle-Exercises-with-solutions(1)\n\n\nVisualizing Chipotle’s Data\nThis time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# set this so the graphs open internally\n%matplotlib inline\n\n\n\nStep 2. Import the dataset from this address.\n\n\nStep 3. Assign it to a variable called chipo.\n\nurl = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv'\n    \nchipo = pd.read_csv(url, sep = '\\t')\n\n\n\nStep 4. See the first 10 entries\n\nchipo.head(10)\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nchoice_description\nitem_price\n\n\n\n\n0\n1\n1\nChips and Fresh Tomato Salsa\nNaN\n$2.39\n\n\n1\n1\n1\nIzze\n[Clementine]\n$3.39\n\n\n2\n1\n1\nNantucket Nectar\n[Apple]\n$3.39\n\n\n3\n1\n1\nChips and Tomatillo-Green Chili Salsa\nNaN\n$2.39\n\n\n4\n2\n2\nChicken Bowl\n[Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n$16.98\n\n\n5\n3\n1\nChicken Bowl\n[Fresh Tomato Salsa (Mild), [Rice, Cheese, Sou...\n$10.98\n\n\n6\n3\n1\nSide of Chips\nNaN\n$1.69\n\n\n7\n4\n1\nSteak Burrito\n[Tomatillo Red Chili Salsa, [Fajita Vegetables...\n$11.75\n\n\n8\n4\n1\nSteak Soft Tacos\n[Tomatillo Green Chili Salsa, [Pinto Beans, Ch...\n$9.25\n\n\n9\n5\n1\nSteak Burrito\n[Fresh Tomato Salsa, [Rice, Black Beans, Pinto...\n$9.25\n\n\n\n\n\n\n\n\n\nStep 5. Create a histogram of the top 5 items bought\n\n# get the Series of the names\nx = chipo.item_name\n\n# use the Counter class from collections to create a dictionary with keys(text) and frequency\nletter_counts = Counter(x)\n\n# convert the dictionary to a DataFrame\ndf = pd.DataFrame.from_dict(letter_counts, orient='index')\n\n# sort the values from the top to the least value and slice the first 5 items\ndf = df[0].sort_values(ascending = True)[45:50]\n\n# create the plot\ndf.plot(kind='bar')\n\n# Set the title and labels\nplt.xlabel('Items')\nplt.ylabel('Number of Times Ordered')\nplt.title('Most ordered Chipotle\\'s Items')\n\n# show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nStep 6. Create a scatterplot with the number of items orderered per order price\n\nHint: Price should be in the X-axis and Items ordered in the Y-axis\n\n# create a list of prices\nchipo.item_price = [float(value[1:-1]) for value in chipo.item_price] # strip the dollar sign and trailing space\n\n# then groupby the orders and sum\norders = chipo.groupby('order_id').sum()\n\n# creates the scatterplot\n# plt.scatter(orders.quantity, orders.item_price, s = 50, c = 'green')\nplt.scatter(x = orders.item_price, y = orders.quantity, s = 50, c = 'green')\n\n# Set the title and labels\nplt.xlabel('Order Price')\nplt.ylabel('Items ordered')\nplt.title('Number of items ordered per order price')\nplt.ylim(0)\n\n(0.0, 36.7)\n\n\n\n\n\nStep 7. BONUS: Create a question and a graph to answer your own question.\n\nSource: Visualizing Chipotle's Data\n\n03Scores-Exercises-with-solutions\n\n\nScores\n\nIntroduction:\nThis time you will create the data.\nExercise based on Chris Albon work, the credits belong to him.\n\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n%matplotlib inline\n\n\n\nStep 2. Create the DataFrame that should look like the one below.\n\nraw_data = {'first_name': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'], \n            'last_name': ['Miller', 'Jacobson', 'Ali', 'Milner', 'Cooze'], \n            'female': [0, 1, 1, 0, 1],\n            'age': [42, 52, 36, 24, 73], \n            'preTestScore': [4, 24, 31, 2, 3],\n            'postTestScore': [25, 94, 57, 62, 70]}\n\ndf = pd.DataFrame(raw_data, columns = ['first_name', 'last_name', 'age', 'female', 'preTestScore', 'postTestScore'])\n\ndf\n\n\n\n\n\n\n\n\nfirst_name\nlast_name\nage\nfemale\npreTestScore\npostTestScore\n\n\n\n\n0\nJason\nMiller\n42\n0\n4\n25\n\n\n1\nMolly\nJacobson\n52\n1\n24\n94\n\n\n2\nTina\nAli\n36\n1\n31\n57\n\n\n3\nJake\nMilner\n24\n0\n2\n62\n\n\n4\nAmy\nCooze\n73\n1\n3\n70\n\n\n\n\n\n\n\n\n\nStep 3. Create a Scatterplot of preTestScore and postTestScore, with the size of each point determined by age\n\nHint: Don’t forget to place the labels\n\nplt.scatter(df.preTestScore, df.postTestScore, s=df.age)\n\n#set labels and titles\nplt.title(\"preTestScore x postTestScore\")\nplt.xlabel('preTestScore')\nplt.ylabel('preTestScore')\n\nText(0, 0.5, 'preTestScore')\n\n\n\n\n\nStep 4. Create a Scatterplot of preTestScore and postTestScore.\n\n\nThis time the size should be 4.5 times the postTestScore and the color determined by sex\n\nplt.scatter(df.preTestScore, df.postTestScore, s= df.postTestScore * 4.5, c = df.female)\n\n#set labels and titles\nplt.title(\"preTestScore x postTestScore\")\nplt.xlabel('preTestScore')\nplt.ylabel('preTestScore')\n\nText(46.972222222222214, 0.5, 'preTestScore')\n\n\n\n\nBONUS: Create your own question and answer it.\n\nSource: Scores"
  }
]